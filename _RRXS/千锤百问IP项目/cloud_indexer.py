# cloud_indexer.py ä¼ªä»£ç  - åœ¨ VSCode ä¸­è¿è¡Œä¸€æ¬¡æ€§è„šæœ¬

import os
from openai import OpenAI
from supabase import create_client, Client
from langchain.text_splitter import MarkdownTextSplitter

# é…ç½®ä¿¡æ¯# æ–‡ä»¶å: cloud_indexer.py - DeepSeek Embedding å®æ–½æ–¹æ¡ˆ

import os
import time
from openai import OpenAI # ä»ç„¶ä½¿ç”¨ OpenAI åº“ï¼Œä½†æŒ‡å‘ DeepSeek API
from supabase import create_client, Client
from langchain.text_splitter import MarkdownTextSplitter
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„é…ç½®
load_dotenv()

# ----------------- 1. é…ç½®ä¿¡æ¯ -----------------
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
# âš ï¸ å…³é”®ï¼šä½¿ç”¨ DeepSeek å¯†é’¥
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY") 
OBSIDIAN_VAULT_PATH = os.getenv("OBSIDIAN_VAULT_PATH")

# âš ï¸ å…³é”®ï¼šéœ€è¦åŒæ­¥çš„æ ¸å¿ƒç›®å½•ï¼ˆå·²æ ¹æ®æ‚¨çš„è·¯å¾„æ¨æ–­ä¿®æ­£ï¼‰
TARGET_FOLDERS = [
    "_RRXS/åƒé”¤ç™¾é—®IPå­¦é•¿RRXS",  
    "_RRXS/åƒé”¤ç™¾é—®IPé¡¹ç›®",       
    "_RRXS/_RRXS.XYZç½‘ç«™"      
]
# å¼ºåˆ¶æ£€æŸ¥ï¼šè¡¨åä½¿ç”¨ rrx_notes è§„èŒƒ
SUPABASE_TABLE_NAME = 'rrxs_notes' 
EMBEDDING_MODEL = "deepseek-ai/deepseek-content-embedding"
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"

# ----------------- 2. åˆå§‹åŒ–å®¢æˆ·ç«¯ -----------------
print("===============================================")
print("ğŸ“¢ æ­£åœ¨å¯åŠ¨ Obsidian Vault äº‘ç«¯ç´¢å¼•åŒæ­¥...")

if not all([SUPABASE_URL, SUPABASE_KEY, DEEPSEEK_API_KEY, OBSIDIAN_VAULT_PATH]):
    print("âŒ é”™è¯¯ï¼šå…³é”®ç¯å¢ƒå˜é‡ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")
    exit()

try:
    # âš ï¸ åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯ï¼šä¿®æ”¹ base_url
    deepseek_client = OpenAI(
        api_key=DEEPSEEK_API_KEY,
        base_url=DEEPSEEK_BASE_URL
    )
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    print("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼šDeepSeek API & Supabase è¿æ¥å°±ç»ªã€‚")
except Exception as e:
    print(f"âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    exit()

# ----------------- 3. åˆå§‹åŒ– Markdown åˆ‡å—å™¨ -----------------
markdown_splitter = MarkdownTextSplitter(
    chunk_size=1000,
    chunk_overlap=200, 
    length_function=len
)

def is_target_file(file_path):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä½äºç›®æ ‡åŒæ­¥ç›®å½•å†…"""
    relative_path = os.path.relpath(file_path, OBSIDIAN_VAULT_PATH)
    
    # æ£€æŸ¥ç›¸å¯¹è·¯å¾„æ˜¯å¦ä»¥ä»»ä¸€ç›®æ ‡æ–‡ä»¶å¤¹å¼€å¤´
    for folder in TARGET_FOLDERS:
        if relative_path.startswith(folder.replace('/', os.sep)):
            return True
    return False

def index_file(file_path):
    """å¤„ç†å•ä¸ª Markdown æ–‡ä»¶ï¼šåˆ‡å— -> å‘é‡åŒ– -> æ’å…¥ Supabase"""
    try:
        if not is_target_file(file_path):
            return # è·³è¿‡éç›®æ ‡æ–‡ä»¶
            
        print(f"\n--- æ­£åœ¨å¤„ç†: {file_path} ---")
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        chunks = markdown_splitter.create_documents([content])
        print(f"æ–‡ä»¶åˆ‡åˆ†å®Œæˆï¼Œå…± {len(chunks)} ä¸ªç‰‡æ®µã€‚")

        for i, chunk in enumerate(chunks):
            text = chunk.page_content
            
            # 3.1 ç”Ÿæˆ Embedding (è°ƒç”¨ DeepSeek API)
            response = deepseek_client.embeddings.create(
                input=text,
                model=EMBEDDING_MODEL
            )
            embedding = response.data[0].embedding
            
            # 3.2 æ’å…¥ Supabase
            data, count = supabase.table(SUPABASE_TABLE_NAME).insert({
                "content": text,
                "embedding": embedding,
                "source": os.path.basename(file_path),
                "metadata": {"path": file_path, "index": i}
            }).execute()

            print(f"  - ç‰‡æ®µ {i+1}/{len(chunks)} ç´¢å¼•æˆåŠŸã€‚")
            time.sleep(0.1) # å¢åŠ å»¶è¿Ÿï¼Œé¿å…æ½œåœ¨çš„é€Ÿç‡é™åˆ¶

    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")


# ----------------- 4. éå† Vault å¹¶ç´¢å¼• -----------------
file_count = 0
for root, _, files in os.walk(OBSIDIAN_VAULT_PATH):
    for file in files:
        if file.endswith(".md"):
            index_file(os.path.join(root, file))
            file_count += 1

print("\n===============================================")
print(f"ğŸ‰ æ‰€æœ‰ Obsidian Vault æ ¸å¿ƒç¬”è®°å·²åŒæ­¥è‡³ Supabase äº‘ç«¯çŸ¥è¯†åº“ ({SUPABASE_TABLE_NAME})ã€‚")
print(f"æ€»è®¡æ‰«æå¹¶å¤„ç†äº† {file_count} ä¸ª .md æ–‡ä»¶ã€‚")
print("ä¸‹ä¸€æ­¥ï¼šè¯·åœ¨ Supabase éƒ¨ç½² Edge Function ä½œä¸º Agent çš„æ£€ç´¢æ¥å£ã€‚")
print("===============================================")w
SUPABASE_URL = "YOUR_SUPABASE_URL"
SUPABASE_KEY = "YOUR_SUPABASE_ANON_KEY"
OPENAI_API_KEY = "YOUR_OPENAI_KEY"
OBSIDIAN_VAULT_PATH = "/path/to/your/Obsidian/Vault"

# 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
openai_client = OpenAI(api_key=OPENAI_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 2. æ–‡æ¡£åˆ‡å—ï¼ˆä½¿ç”¨ LangChain ä¿æŒä¸“ä¸šæ€§ï¼‰
markdown_splitter = MarkdownTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

def index_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # 2.1 åˆ‡å—
    chunks = markdown_splitter.create_documents([content])

    for chunk in chunks:
        text = chunk.page_content
        
        # 2.2 ç”Ÿæˆ Embedding (CALL CLOUD API)
        response = openai_client.embeddings.create(
            input=text,
            model="text-embedding-ada-002" 
        )
        embedding = response.data[0].embedding
        
        # 2.3 æ’å…¥ Supabase
        # æ³¨æ„ï¼šæ­¤å¤„éœ€å¤„ç† API é”™è¯¯å’Œé€Ÿç‡é™åˆ¶
        data, count = supabase.table('rrxs_notes').insert({
            "content": text,
            "embedding": embedding,
            "source": file_path,
            "metadata": {"tags": "Ref-Prep"}
        }).execute()
        
        print(f"Indexed chunk from {file_path}")

# 3. éå† Vault å¹¶ç´¢å¼•ï¼ˆä»…é’ˆå¯¹ .md æ–‡ä»¶ï¼‰
for root, _, files in os.walk(OBSIDIAN_VAULT_PATH):
    for file in files:
        if file.endswith(".md"):
            index_file(os.path.join(root, file))

print("Obsidian Vault å‘é‡åŒ–ç´¢å¼•å®Œæˆï¼Œå·²åŒæ­¥è‡³ Supabase äº‘ç«¯ã€‚")