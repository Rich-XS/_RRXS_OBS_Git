
"smart_sources:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md": {"path":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md","last_embed":{"hash":null},"embeddings":{},"last_read":{"hash":"66e80940878101e5fc07d0a1e16f55c3778d5e6be4d767e342ec66c9b4794b7c","at":1757413828613},"class_name":"SmartSource","last_import":{"mtime":1475552427000,"size":10713,"at":1757413828614,"hash":"66e80940878101e5fc07d0a1e16f55c3778d5e6be4d767e342ec66c9b4794b7c"},"blocks":{"#---frontmatter---":[1,3],"#":[4,13],"##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化":[14,31],"##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#{1}":[16,19],"##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#[归一化的目标](http://blog.csdn.net/pipisorry/article/details/null)":[20,31],"##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#[归一化的目标](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[22,31],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)":[32,123],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[min-max标准化(Min-max normalization)](http://blog.csdn.net/pipisorry/article/details/null)":[34,57],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[min-max标准化(Min-max normalization)](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[36,57],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null>[log函数转换](http://blog.csdn.net/pipisorry/article/details/null)":[58,65],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null>[log函数转换](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[60,65],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[atan函数转换](http://blog.csdn.net/pipisorry/article/details/null)":[66,73],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[atan函数转换](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[68,73],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)":[74,107],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[76,97],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null><http://blog.csdn.net/pipisorry/article/details/null>":[98,107],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null><http://blog.csdn.net/pipisorry/article/details/null>#{1}":[100,107],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[Decimal scaling小数定标标准化](http://blog.csdn.net/pipisorry/article/details/null)":[108,113],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[Decimal scaling小数定标标准化](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[110,113],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[对数Logistic模式](http://blog.csdn.net/pipisorry/article/details/null)":[114,117],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[对数Logistic模式](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[116,117],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[模糊量化模式](http://blog.csdn.net/pipisorry/article/details/null)":[118,123],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[模糊量化模式](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[120,123],"#[数据标准化/归一化的编程实现](http://blog.csdn.net/pipisorry/article/details/null)":[124,133],"#[数据标准化/归一化的编程实现](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[126,133]},"outlinks":[{"title":"blog.csdn.net/pipisorry/article/details/52247379","target":"http://blog.csdn.net/pipisorry/article/details/52247379","line":6},{"title":"[均值、方差与协方差矩阵","target":"http://blog.csdn.net/pipisorry/article/details/48788671","line":10},{"title":"[矩阵论：向量范数和矩阵范数","target":"http://blog.csdn.net/pipisorry/article/details/51030563","line":12},{"title":"归一化的目标","target":"http://blog.csdn.net/pipisorry/article/details/null","line":20},{"title":"1 把数变为（0，1）之间的小数        主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。2 把有量纲表达式变为无量纲表达式        归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 比如，复数阻抗可以归一化书写：Z = R + jωL = R(1 + jωL/R) ，复数部分变成了纯数量了，没有量纲。 另外，微波之中也就是电路分析、信号系统、电磁波传输等，有很多运算都可以如此处理，既保证了运算的便捷，又能凸现出物理量的本质含义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":22},{"title":"    在多指标评价体系中，由于各评价指标的性质不同，通常具有不同的量纲和数量级。当各指标间的水平相差很大时，如果直接用原始指标值进行分析，就会突出数值较高的指标在综合分析中的作用，相对削弱数值水平较低指标的作用。因此，为了保证结果的可靠性，需要对原始指标数据进行标准化处理。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":24},{"title":"    在数据分析之前，我们通常需要先将数据标准化（normalization），利用标准化后的数据进行数据分析。数据标准化也就是统计数据的指数化。数据标准化处理主要包括数据同趋化处理和无量纲化处理两个方面。数据同趋化处理主要解决不同性质数据问题，对不同性质指标直接加总不能正确反映不同作用力的综合结果，须先考虑改变逆指标数据性质，使所有指标对测评方案的作用力同趋化，再加总才能得出正确结果。数据无量纲化处理主要解决数据的可比性。数据标准化的方法有很多种，常用的有“最小—最大标准化”、“Z-score标准化”和“按小数定标标准化”等。经过上述标准化处理，原始数据均转换为无量纲化指标测评值，即各指标值都处于同一个数量级别上，可以进行综合测评分析。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":26},{"title":"从经验上说，归一化是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":28},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","line":30},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":30},{"title":"常见的数据归一化方法","target":"http://blog.csdn.net/pipisorry/article/details/null","line":32},{"title":"min-max标准化(Min-max normalization)","target":"http://blog.csdn.net/pipisorry/article/details/null","line":34},{"title":"也叫离差标准化，是对原始数据的线性变换。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":36},{"title":"使结果落到0,1区间，转换函数如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":38},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","line":40},{"title":"其中max为样本数据的最大值，min为样本数据的最小值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":42},{"title":"def Normalization(x):    return (float(i)-min(x))/float(max(x)-min(x)) for i in x","target":"http://blog.csdn.net/pipisorry/article/details/null","line":44},{"title":"如果想要将数据映射到-1,1，则将公式换成：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":46},{"title":"_x_∗=_x_−_x__m__e__a__n__x__m__a__x_−_x__m__i__n_\n\nx_mean表示数据的均值。\n\ndef Normalization2(x):    return (float(i)-np.mean(x))/(max(x)-min(x)) for i in x\n\n这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":50},{"title":"log函数转换","target":"http://blog.csdn.net/pipisorry/article/details/null","line":58},{"title":"通过以10为底的log函数转换的方法同样可以实现归一下，具体方法如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":60},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","line":62},{"title":"看了下网上很多介绍都是x*=log10(x)，其实是有问题的，这个结果并非一定落到0,1区间上，应该还要除以log10(max)，max为样本数据最大值，并且所有的数据都要大于等于1。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":64},{"title":"atan函数转换","target":"http://blog.csdn.net/pipisorry/article/details/null","line":66},{"title":"用反正切函数也可以实现数据的归一化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":68},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","line":70},{"title":"使用这个方法需要注意的是如果想映射的区间为0,1，则数据都应该大于等于0，小于0的数据将被映射到-1,0区间上，而并非所有数据标准化的结果都映射到0,1区间上。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":72},{"title":"z-score 标准化(zero-mean normalization)","target":"http://blog.csdn.net/pipisorry/article/details/null","line":74},{"title":"最常见的标准化方法就是Z标准化，也是SPSS中最为常用的标准化方法，spss默认的标准化方法就是z-score标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":76},{"title":"也叫标准差标准化，这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":78},{"title":"经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":80},{"title":"_x_∗=_x_−_μ__σ_\n\n其中μ为所有样本数据的均值，σ为所有样本数据的标准差。\n\nz-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。\n\n**标准化的公式很简单，步骤如下**\n\n　　1.求出各变量（指标）的算术平均值（数学期望）xi和标准差si ；　　2.进行标准化处理：　　zij=（xij－xi）/si　　其中：zij为标准化后的变量值；xij为实际变量值。　　3.将逆指标前的正负号对调。　　标准化后的变量值围绕0上下波动，大于0说明高于平均水平，小于0说明低于平均水平。\n\ndef z_score(x):    x_mean=np.mean(x)    s2=sum((i-np.mean(x))*(i-np.mean(x)) for i in x)/len(x)    return (i-x_mean)/s2 for i in x\n\n**为什么z-score 标准化后的数据标准差为1?**","target":"http://blog.csdn.net/pipisorry/article/details/null","line":84},{"title":"x-μ只改变均值，标准差不变，所以均值变为0","target":"http://blog.csdn.net/pipisorry/article/details/null","line":100},{"title":"(x-μ)/σ只会使标准差除以σ倍，所以标准差变为1","target":"http://blog.csdn.net/pipisorry/article/details/null","line":102},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","line":104},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":106},{"title":"Decimal scaling小数定标标准化","target":"http://blog.csdn.net/pipisorry/article/details/null","line":108},{"title":"这种方法通过移动数据的小数点位置来进行标准化。小数点移动多少位取决于属性A的取值中的最大绝对值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":110},{"title":"将属性A的原始值x使用decimal scaling标准化到x'的计算方法是：x'=x/(10^j)其中，j是满足条件的最小整数。例如 假定A的值由-986到917，A的最大绝对值为986，为使用小数定标标准化，我们用每个值除以1000（即，j=3），这样，-986被规范化为-0.986。注意，标准化会对原始数据做出改变，因此需要保存所使用的标准化方法的参数，以便对后续的数据进行统一的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":112},{"title":"对数Logistic模式","target":"http://blog.csdn.net/pipisorry/article/details/null","line":114},{"title":"新数据=1/（1+e^(-原数据)）","target":"http://blog.csdn.net/pipisorry/article/details/null","line":116},{"title":"模糊量化模式","target":"http://blog.csdn.net/pipisorry/article/details/null","line":118},{"title":"新数据=1/2+1/2sin派3.1415/（极大值-极小值）*（X-（极大值-极小值）/2） X为原数据","target":"http://blog.csdn.net/pipisorry/article/details/null","line":120},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":122},{"title":"数据标准化/归一化的编程实现","target":"http://blog.csdn.net/pipisorry/article/details/null","line":124},{"title":"Python","target":"http://lib.csdn.net/base/python","line":126},{"title":"[Scikit-learn：数据预处理Preprocessing data","target":"http://blog.csdn.net/pipisorry/article/details/52247679","line":128}],"metadata":{"source":"http://blog.csdn.net/pipisorry/article/details/52247379"}},
"smart_sources:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md": {"path":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md","embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04666915,-0.04119189,0.01216358,-0.0139048,0.04300664,-0.00886551,-0.03172278,-0.01571642,0.01889702,-0.00375415,0.00324587,-0.0448191,0.0795861,0.03052354,0.05812147,-0.00485904,0.01315652,0.0118969,-0.07278498,-0.00716259,0.14205864,-0.03587357,0.05334286,-0.06470496,0.10369773,-0.00000552,-0.03026487,-0.05065734,-0.01364209,-0.21854974,0.01715307,0.02157584,0.07704198,-0.02871941,0.07585544,-0.06881148,-0.03065319,0.07665736,0.0206734,0.03193403,0.05678312,0.00807985,-0.02380645,0.0217063,0.01482506,-0.03603513,-0.03699573,-0.04461493,-0.0145616,-0.02655839,-0.01182032,-0.03770553,-0.02214806,0.03251524,0.01845634,0.02202787,0.08146567,0.01613862,0.06997074,0.0479034,0.00637312,0.09146363,-0.21909776,0.04653322,0.00390496,-0.01181272,-0.01809388,-0.03494592,0.0504652,0.0589852,-0.00897438,-0.02163707,0.00945589,0.05192303,0.02593854,-0.05363144,0.02229758,0.02035051,-0.02286417,-0.00271728,-0.03054111,0.07435402,0.06193785,-0.02434328,0.04963001,-0.00552317,0.04780825,-0.05405252,-0.00518898,-0.06223971,-0.04477422,-0.00641588,-0.00852024,0.0098396,-0.03135889,-0.04625022,0.04544181,0.02793499,-0.04149904,0.11209006,-0.03710051,0.05232209,0.01278327,-0.00451423,0.06317785,0.0603732,-0.04993373,-0.0191768,-0.02130189,0.00035109,-0.03471062,-0.01093625,0.05339749,-0.04626316,0.00453616,-0.00758511,0.0606356,0.05621507,0.00089909,0.03185599,0.02446403,-0.0000974,0.0398696,-0.03405604,0.01141579,-0.02992241,0.00148294,0.03242514,0.03695096,0.03806321,0.07994141,0.04204395,-0.08876428,-0.01468291,0.05742678,0.00893905,-0.01468402,-0.03110924,0.01135296,0.01170815,-0.04309994,-0.0405551,-0.01873885,-0.0540057,-0.09110636,0.10266019,-0.09501873,-0.02472191,-0.0084838,-0.03796752,-0.00855718,0.018708,-0.02204576,-0.05059015,0.0768887,-0.021466,0.0691028,0.07465515,-0.05806825,-0.06633981,0.0140372,-0.07047526,-0.05852699,0.12492236,0.05897787,-0.03929852,-0.02236411,0.03733587,0.01860031,-0.0527578,0.05947364,0.00122909,-0.0263626,0.01735822,0.0229105,-0.02100405,-0.05044055,-0.05234755,0.0062198,0.05656547,0.04225549,-0.03479717,-0.03541519,0.05027986,0.01417271,-0.10420226,0.00664615,-0.05045553,-0.01993312,0.00823359,-0.04528075,0.01247536,-0.04597955,-0.04227973,-0.01648744,0.01417617,0.01300185,-0.04705554,0.01664238,-0.06556286,0.04801634,0.03663504,0.04504058,-0.04447419,-0.05343768,0.01786033,0.048972,-0.0713572,0.00557732,0.04458189,-0.04823789,-0.01615631,0.0034245,-0.03355127,-0.01533722,0.00519547,0.00476677,0.06893216,0.01140456,0.06073,-0.00456305,0.0155042,-0.10423063,-0.21492502,-0.03268627,0.04732914,-0.07084586,0.06702323,-0.01005936,-0.00063892,0.00326788,0.04735835,0.08074739,0.05412503,0.00437259,-0.0009663,0.03020021,0.00527458,0.01732924,0.07008251,0.03975348,-0.01817042,0.0264086,-0.00728287,0.02881886,-0.03947996,-0.04723179,-0.01621449,0.01614468,0.10455342,-0.01622566,0.07697812,-0.01034852,0.01993089,0.01848028,0.02198149,-0.09125722,0.04795343,0.00353191,-0.03330002,-0.0307013,-0.02954498,-0.05665523,0.0083182,0.00709171,-0.01793498,-0.1056362,-0.0481513,-0.00088074,-0.01551846,-0.00557541,0.00319506,0.02538054,-0.03421091,-0.02510124,-0.0164907,0.05363698,0.0095978,-0.03697453,-0.08921909,-0.03238884,-0.06208223,0.02601944,-0.04087253,-0.06181042,0.03894433,-0.0858919,-0.04231558,-0.00550518,-0.01002626,0.00916049,0.04214807,0.02560865,-0.10398044,0.12982233,0.02730003,0.04083332,0.01443034,0.06477661,-0.05360404,0.00219324,0.01918897,0.01114853,0.11318918,-0.01460153,0.06606694,0.07367141,0.00007765,0.00885068,0.03372013,-0.04126064,0.03351084,-0.01795555,-0.04431957,-0.01532016,-0.06563554,-0.03298974,0.05924031,0.00943648,-0.26270354,0.00883487,0.02816285,-0.02645496,0.05763352,0.05104499,0.01096812,-0.0020409,0.00090327,0.0192367,-0.04057559,0.02897658,0.01416948,-0.031726,-0.05607781,-0.02546757,0.07227828,0.03893266,0.03278498,-0.0213865,-0.02203138,0.03383881,0.21366602,0.01501578,-0.01615197,-0.02090724,-0.02757205,0.02189127,0.01070834,-0.00122223,-0.05268973,-0.00215202,0.09108385,0.06232798,0.02879226,0.11499415,0.00028591,-0.01302445,0.03081702,0.03268929,-0.05282965,-0.01325355,-0.05405512,-0.03385957,0.09960859,-0.0136078,-0.04955103,-0.04965144,0.01277862,-0.01430905,-0.02150549,0.02957095,-0.00295901,-0.0096592,0.01556545,0.04337278,0.03952176,-0.02338373,-0.01836093,-0.06692824,0.01978132,0.00986163,0.0114009,0.04410933,0.00550513],"last_embed":{"hash":"66e80940878101e5fc07d0a1e16f55c3778d5e6be4d767e342ec66c9b4794b7c","tokens":350}}},"last_read":{"hash":"66e80940878101e5fc07d0a1e16f55c3778d5e6be4d767e342ec66c9b4794b7c","at":1757414184031},"class_name":"SmartSource","last_import":{"mtime":1475552427000,"size":10713,"at":1757413828614,"hash":"66e80940878101e5fc07d0a1e16f55c3778d5e6be4d767e342ec66c9b4794b7c"},"blocks":{"#---frontmatter---":[1,3],"#":[4,13],"##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化":[14,31],"##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#{1}":[16,19],"##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#[归一化的目标](http://blog.csdn.net/pipisorry/article/details/null)":[20,31],"##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#[归一化的目标](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[22,31],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)":[32,123],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[min-max标准化(Min-max normalization)](http://blog.csdn.net/pipisorry/article/details/null)":[34,57],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[min-max标准化(Min-max normalization)](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[36,57],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null>[log函数转换](http://blog.csdn.net/pipisorry/article/details/null)":[58,65],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null>[log函数转换](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[60,65],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[atan函数转换](http://blog.csdn.net/pipisorry/article/details/null)":[66,73],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[atan函数转换](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[68,73],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)":[74,107],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[76,97],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null><http://blog.csdn.net/pipisorry/article/details/null>":[98,107],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null><http://blog.csdn.net/pipisorry/article/details/null>#{1}":[100,107],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[Decimal scaling小数定标标准化](http://blog.csdn.net/pipisorry/article/details/null)":[108,113],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[Decimal scaling小数定标标准化](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[110,113],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[对数Logistic模式](http://blog.csdn.net/pipisorry/article/details/null)":[114,117],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[对数Logistic模式](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[116,117],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[模糊量化模式](http://blog.csdn.net/pipisorry/article/details/null)":[118,123],"#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[模糊量化模式](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[120,123],"#[数据标准化/归一化的编程实现](http://blog.csdn.net/pipisorry/article/details/null)":[124,133],"#[数据标准化/归一化的编程实现](http://blog.csdn.net/pipisorry/article/details/null)#{1}":[126,133]},"outlinks":[{"title":"blog.csdn.net/pipisorry/article/details/52247379","target":"http://blog.csdn.net/pipisorry/article/details/52247379","line":6},{"title":"[均值、方差与协方差矩阵","target":"http://blog.csdn.net/pipisorry/article/details/48788671","line":10},{"title":"[矩阵论：向量范数和矩阵范数","target":"http://blog.csdn.net/pipisorry/article/details/51030563","line":12},{"title":"归一化的目标","target":"http://blog.csdn.net/pipisorry/article/details/null","line":20},{"title":"1 把数变为（0，1）之间的小数        主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。2 把有量纲表达式变为无量纲表达式        归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 比如，复数阻抗可以归一化书写：Z = R + jωL = R(1 + jωL/R) ，复数部分变成了纯数量了，没有量纲。 另外，微波之中也就是电路分析、信号系统、电磁波传输等，有很多运算都可以如此处理，既保证了运算的便捷，又能凸现出物理量的本质含义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":22},{"title":"    在多指标评价体系中，由于各评价指标的性质不同，通常具有不同的量纲和数量级。当各指标间的水平相差很大时，如果直接用原始指标值进行分析，就会突出数值较高的指标在综合分析中的作用，相对削弱数值水平较低指标的作用。因此，为了保证结果的可靠性，需要对原始指标数据进行标准化处理。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":24},{"title":"    在数据分析之前，我们通常需要先将数据标准化（normalization），利用标准化后的数据进行数据分析。数据标准化也就是统计数据的指数化。数据标准化处理主要包括数据同趋化处理和无量纲化处理两个方面。数据同趋化处理主要解决不同性质数据问题，对不同性质指标直接加总不能正确反映不同作用力的综合结果，须先考虑改变逆指标数据性质，使所有指标对测评方案的作用力同趋化，再加总才能得出正确结果。数据无量纲化处理主要解决数据的可比性。数据标准化的方法有很多种，常用的有“最小—最大标准化”、“Z-score标准化”和“按小数定标标准化”等。经过上述标准化处理，原始数据均转换为无量纲化指标测评值，即各指标值都处于同一个数量级别上，可以进行综合测评分析。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":26},{"title":"从经验上说，归一化是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":28},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","line":30},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":30},{"title":"常见的数据归一化方法","target":"http://blog.csdn.net/pipisorry/article/details/null","line":32},{"title":"min-max标准化(Min-max normalization)","target":"http://blog.csdn.net/pipisorry/article/details/null","line":34},{"title":"也叫离差标准化，是对原始数据的线性变换。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":36},{"title":"使结果落到0,1区间，转换函数如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":38},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","line":40},{"title":"其中max为样本数据的最大值，min为样本数据的最小值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":42},{"title":"def Normalization(x):    return (float(i)-min(x))/float(max(x)-min(x)) for i in x","target":"http://blog.csdn.net/pipisorry/article/details/null","line":44},{"title":"如果想要将数据映射到-1,1，则将公式换成：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":46},{"title":"_x_∗=_x_−_x__m__e__a__n__x__m__a__x_−_x__m__i__n_\n\nx_mean表示数据的均值。\n\ndef Normalization2(x):    return (float(i)-np.mean(x))/(max(x)-min(x)) for i in x\n\n这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":50},{"title":"log函数转换","target":"http://blog.csdn.net/pipisorry/article/details/null","line":58},{"title":"通过以10为底的log函数转换的方法同样可以实现归一下，具体方法如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":60},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","line":62},{"title":"看了下网上很多介绍都是x*=log10(x)，其实是有问题的，这个结果并非一定落到0,1区间上，应该还要除以log10(max)，max为样本数据最大值，并且所有的数据都要大于等于1。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":64},{"title":"atan函数转换","target":"http://blog.csdn.net/pipisorry/article/details/null","line":66},{"title":"用反正切函数也可以实现数据的归一化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":68},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","line":70},{"title":"使用这个方法需要注意的是如果想映射的区间为0,1，则数据都应该大于等于0，小于0的数据将被映射到-1,0区间上，而并非所有数据标准化的结果都映射到0,1区间上。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":72},{"title":"z-score 标准化(zero-mean normalization)","target":"http://blog.csdn.net/pipisorry/article/details/null","line":74},{"title":"最常见的标准化方法就是Z标准化，也是SPSS中最为常用的标准化方法，spss默认的标准化方法就是z-score标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":76},{"title":"也叫标准差标准化，这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":78},{"title":"经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":80},{"title":"_x_∗=_x_−_μ__σ_\n\n其中μ为所有样本数据的均值，σ为所有样本数据的标准差。\n\nz-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。\n\n**标准化的公式很简单，步骤如下**\n\n　　1.求出各变量（指标）的算术平均值（数学期望）xi和标准差si ；　　2.进行标准化处理：　　zij=（xij－xi）/si　　其中：zij为标准化后的变量值；xij为实际变量值。　　3.将逆指标前的正负号对调。　　标准化后的变量值围绕0上下波动，大于0说明高于平均水平，小于0说明低于平均水平。\n\ndef z_score(x):    x_mean=np.mean(x)    s2=sum((i-np.mean(x))*(i-np.mean(x)) for i in x)/len(x)    return (i-x_mean)/s2 for i in x\n\n**为什么z-score 标准化后的数据标准差为1?**","target":"http://blog.csdn.net/pipisorry/article/details/null","line":84},{"title":"x-μ只改变均值，标准差不变，所以均值变为0","target":"http://blog.csdn.net/pipisorry/article/details/null","line":100},{"title":"(x-μ)/σ只会使标准差除以σ倍，所以标准差变为1","target":"http://blog.csdn.net/pipisorry/article/details/null","line":102},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","line":104},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":106},{"title":"Decimal scaling小数定标标准化","target":"http://blog.csdn.net/pipisorry/article/details/null","line":108},{"title":"这种方法通过移动数据的小数点位置来进行标准化。小数点移动多少位取决于属性A的取值中的最大绝对值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":110},{"title":"将属性A的原始值x使用decimal scaling标准化到x'的计算方法是：x'=x/(10^j)其中，j是满足条件的最小整数。例如 假定A的值由-986到917，A的最大绝对值为986，为使用小数定标标准化，我们用每个值除以1000（即，j=3），这样，-986被规范化为-0.986。注意，标准化会对原始数据做出改变，因此需要保存所使用的标准化方法的参数，以便对后续的数据进行统一的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":112},{"title":"对数Logistic模式","target":"http://blog.csdn.net/pipisorry/article/details/null","line":114},{"title":"新数据=1/（1+e^(-原数据)）","target":"http://blog.csdn.net/pipisorry/article/details/null","line":116},{"title":"模糊量化模式","target":"http://blog.csdn.net/pipisorry/article/details/null","line":118},{"title":"新数据=1/2+1/2sin派3.1415/（极大值-极小值）*（X-（极大值-极小值）/2） X为原数据","target":"http://blog.csdn.net/pipisorry/article/details/null","line":120},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":122},{"title":"数据标准化/归一化的编程实现","target":"http://blog.csdn.net/pipisorry/article/details/null","line":124},{"title":"Python","target":"http://lib.csdn.net/base/python","line":126},{"title":"[Scikit-learn：数据预处理Preprocessing data","target":"http://blog.csdn.net/pipisorry/article/details/52247679","line":128}],"metadata":{"source":"http://blog.csdn.net/pipisorry/article/details/52247379"}},"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#---frontmatter---": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.05328934,-0.04357738,0.00451712,-0.0223708,0.03737271,-0.00559888,-0.03187986,-0.00642308,0.0155775,0.00096246,0.00714458,-0.045253,0.07159444,0.04545999,0.05922094,0.00987851,0.00620349,0.01142294,-0.07419683,-0.00580703,0.1420884,-0.02109711,0.06203109,-0.0838455,0.08499382,-0.01444373,-0.02926162,-0.05165501,-0.00815842,-0.20549545,0.0087275,0.02317024,0.07870683,-0.02693759,0.07975184,-0.06015217,-0.03591482,0.05862405,0.01967945,0.05774399,0.05425388,0.00986512,-0.03104844,0.01586457,0.02406344,-0.0328874,-0.01931046,-0.03049603,-0.00347463,-0.02631828,-0.00715234,-0.02558231,-0.01929924,0.03263061,0.01395634,0.03889864,0.08713336,0.00700605,0.06237874,0.06127123,0.02531306,0.0794692,-0.21545883,0.03701746,0.00194205,0.00178224,-0.02027655,-0.03338025,0.02794645,0.0763363,-0.00645238,-0.01709134,0.00044742,0.06075168,0.02279257,-0.03677877,0.02436696,0.00289207,-0.01926276,-0.01921843,-0.00187475,0.0660813,0.05369652,-0.02935652,0.05080344,-0.00114392,0.04451446,-0.04605854,0.00490462,-0.0677254,-0.04078212,-0.02027213,0.00461005,0.00978242,-0.0329792,-0.04339904,0.04527476,0.03202589,-0.03114468,0.12207771,-0.04024395,0.04922147,0.02017522,-0.00010865,0.06173273,0.05574068,-0.03406764,-0.0124434,-0.02786853,-0.00853601,-0.03142759,-0.00921718,0.05011675,-0.04455027,0.01233791,-0.01867417,0.06591007,0.05615876,0.00436568,0.03113459,0.0124621,-0.0162694,0.04930389,-0.03995189,0.01283239,-0.03896059,0.00572695,0.03364356,0.03380226,0.03973353,0.06652954,0.03001421,-0.09367808,-0.01395883,0.06368995,0.00483961,-0.01387948,-0.03589053,0.00801727,0.02778444,-0.03746898,-0.04078428,-0.02138814,-0.05368596,-0.07699721,0.09806219,-0.07093506,-0.02217388,-0.0046326,-0.05102091,-0.01185492,0.02195702,-0.02243181,-0.04857628,0.07218786,-0.01668172,0.07003463,0.05500802,-0.06257033,-0.07156097,0.01691912,-0.07318785,-0.06389546,0.12773126,0.06280656,-0.04564559,-0.02199273,0.04252809,0.00710004,-0.05866968,0.05545632,0.00330196,-0.04175964,0.02160614,0.03549021,-0.025295,-0.05568432,-0.03340078,0.00183493,0.04275293,0.05503852,-0.03089213,-0.04400051,0.04746681,0.00575857,-0.09181654,0.01980977,-0.05461293,-0.00782907,0.00339394,-0.03601063,0.01993542,-0.04773532,-0.04995207,-0.01908715,0.01427237,0.01801905,-0.03664785,0.02657145,-0.06715605,0.04266518,0.04046969,0.03276098,-0.03243404,-0.04653909,0.01985988,0.04996232,-0.05731595,0.00948943,0.03798291,-0.05414302,-0.01089135,0.01211241,-0.02817662,-0.02660761,0.00414153,0.00665643,0.07170512,0.01705462,0.05427996,-0.00767568,0.00587239,-0.09464297,-0.23291145,-0.04102521,0.04687806,-0.08082843,0.0648786,0.0008643,-0.0087265,0.01971084,0.04729961,0.0779974,0.05746897,0.00034071,0.00638347,0.00681821,0.00332237,0.02119316,0.06837968,0.02271849,-0.01580171,0.01110208,-0.01114118,0.02992422,-0.0324632,-0.08235287,0.00247257,0.01624053,0.11121351,0.01228693,0.08976609,-0.02993755,0.01955797,0.00507733,0.02478517,-0.11577656,0.03820612,-0.00264051,-0.03167756,-0.02802088,-0.03105081,-0.05608004,0.02062489,0.01490123,-0.02672605,-0.12268696,-0.04113457,-0.00024203,-0.01770786,-0.03036248,0.0052992,0.02744594,-0.04632634,-0.01178603,-0.00686181,0.06798991,-0.0028572,-0.01984682,-0.10357476,-0.02585094,-0.06104657,0.03162386,-0.03957522,-0.06247475,0.02692067,-0.08323915,-0.03572657,0.00041846,-0.02004226,-0.00518175,0.05551905,0.0132374,-0.09263123,0.11948013,0.02148811,0.03335707,0.00842578,0.05585218,-0.0449955,0.00101971,0.01275344,0.01050777,0.11474242,-0.0259783,0.07448824,0.08628728,0.00687747,0.01747154,0.0309389,-0.02359212,0.04388664,-0.0309164,-0.03851626,-0.01062728,-0.07118932,-0.05886056,0.05641854,0.00465704,-0.26402462,0.02537682,0.03163579,-0.00580556,0.04376837,0.05629412,0.01547334,-0.02036016,-0.00841008,0.03110343,-0.0255437,0.02981449,0.00867615,-0.01939077,-0.04833782,-0.02018721,0.07933622,0.03951189,0.03424446,-0.0142478,-0.00504124,0.03136263,0.22101302,0.00994276,-0.00659224,-0.01015686,-0.02983944,0.01953675,0.02615354,-0.00077864,-0.05092477,-0.00321243,0.06101338,0.04854523,0.03281797,0.10847323,0.0072041,-0.00708406,0.03773269,0.02894435,-0.06931187,-0.02003694,-0.06241508,-0.03001389,0.09865524,-0.00178636,-0.04522331,-0.05108212,-0.00008194,-0.01703737,-0.02486091,0.01099175,0.00345025,-0.0079237,0.00463311,0.05354109,0.04407607,-0.04640868,-0.03392315,-0.06738167,0.00914805,-0.00033062,0.03166195,0.03247014,-0.00154973],"last_embed":{"hash":"10e9265ef609395ec309e7a7a51c1a19f674e8a39ca14218d3cba8a29d830d9d","tokens":70}}},"text":null,"length":0,"last_read":{"hash":"10e9265ef609395ec309e7a7a51c1a19f674e8a39ca14218d3cba8a29d830d9d","at":1757414183187},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#---frontmatter---","lines":[1,3],"size":71,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.0531195,-0.04676979,0.01274588,-0.01720903,0.0405555,-0.00931434,-0.03457233,-0.01219998,0.01711187,0.00279859,0.00154979,-0.04563715,0.07497922,0.0353012,0.0565217,-0.00036844,0.01302009,0.00474198,-0.07397211,-0.01448927,0.14930092,-0.03435582,0.05321195,-0.06218709,0.09824151,-0.00612487,-0.02735209,-0.04601208,-0.01091908,-0.2144669,0.01717789,0.03045658,0.07636756,-0.02379022,0.07922462,-0.06690555,-0.03553882,0.07364701,0.02070606,0.04140667,0.04846998,0.00194193,-0.02230721,0.02643892,0.02573275,-0.03508947,-0.02632323,-0.04268967,-0.0126595,-0.03433441,-0.01100609,-0.03239029,-0.02054903,0.02872873,0.0226782,0.02401404,0.08491968,0.01485744,0.06893841,0.04912601,0.01660922,0.08587401,-0.21542737,0.0438672,-0.00529119,0.00061856,-0.0222399,-0.03381177,0.04569009,0.06833697,-0.01415826,-0.01861937,0.00255225,0.05219361,0.02312408,-0.04971616,0.02446144,0.01731833,-0.02215637,-0.01602603,-0.02017643,0.0755127,0.05783348,-0.0278454,0.04712504,-0.00333694,0.04734739,-0.04919466,-0.00498152,-0.06576435,-0.04331718,-0.00954651,-0.00300918,0.00660902,-0.02467475,-0.03905216,0.04203936,0.02081787,-0.04148401,0.11914472,-0.03861726,0.04993404,0.00996926,-0.00766206,0.05971051,0.05666799,-0.0420373,-0.01623212,-0.02566951,0.00263605,-0.03616389,-0.01065921,0.05307533,-0.04814831,0.00819136,-0.00815422,0.06133259,0.06343478,-0.00001195,0.03101956,0.01912848,-0.00636739,0.04763818,-0.03758993,0.00537662,-0.03624312,0.00345731,0.02870177,0.03812688,0.04009328,0.07389501,0.03826172,-0.0857389,-0.01896284,0.04896791,0.0115904,-0.0141007,-0.04223231,0.00780001,0.02676618,-0.03905068,-0.0473382,-0.0229711,-0.05700337,-0.07702755,0.10528323,-0.08383714,-0.0269536,-0.00824761,-0.03280569,-0.01432745,0.02167406,-0.0170888,-0.05375253,0.07335308,-0.02956027,0.06753584,0.0760622,-0.05792346,-0.06893584,0.01848666,-0.07313167,-0.0607988,0.12894312,0.06427373,-0.04374398,-0.02611079,0.04008358,0.02182598,-0.05893251,0.05749565,-0.00235154,-0.03095115,0.02016477,0.02872912,-0.02276682,-0.04634511,-0.04992085,-0.00020008,0.04874561,0.03746565,-0.04067856,-0.04293061,0.04346017,0.01259229,-0.09958201,0.01146434,-0.06011467,-0.01529401,0.00607853,-0.03575187,0.01585468,-0.04602457,-0.03898229,-0.0178052,0.00856423,0.01243375,-0.05222671,0.0179044,-0.07046823,0.04021789,0.0314908,0.04060055,-0.04838069,-0.05379149,0.02138497,0.05175872,-0.06384226,0.00311518,0.04661912,-0.05265995,-0.01833865,0.00337812,-0.02724719,-0.0182164,0.00650374,0.00481428,0.06749738,0.01431325,0.05745606,-0.00556401,0.01935243,-0.10282173,-0.21502641,-0.04200482,0.04204943,-0.06861079,0.0684844,-0.010095,0.0008064,0.01499062,0.05159935,0.08697121,0.06284491,0.00367256,0.00417932,0.02914095,0.00981291,0.02715917,0.07014782,0.0354527,-0.01775141,0.0217251,-0.00819212,0.02969408,-0.04629386,-0.05155217,-0.0113385,0.01081886,0.11317459,0.00125278,0.08340956,-0.01745169,0.02272102,0.00427034,0.02611539,-0.0928275,0.04751422,0.00203671,-0.03240928,-0.02987004,-0.02853429,-0.06230188,0.01670169,0.01024495,-0.03072878,-0.10699425,-0.05258034,0.00096078,-0.02016962,-0.01239465,0.00159661,0.02744107,-0.03376655,-0.02434685,-0.01495148,0.05299839,0.01079893,-0.03336444,-0.09926643,-0.03789777,-0.05761782,0.03205073,-0.04265932,-0.05818146,0.03388302,-0.08176426,-0.03443324,-0.00082695,-0.01023799,0.00243553,0.04810793,0.02331737,-0.09835856,0.12688713,0.02096014,0.04502855,0.02042402,0.06078351,-0.0459961,0.00696863,0.01780857,0.01154718,0.11024342,-0.02117575,0.06796262,0.08089131,0.0051209,0.00625829,0.02603954,-0.03831149,0.04383321,-0.02520861,-0.04049562,-0.01846858,-0.06951778,-0.03485114,0.0551457,0.00467402,-0.25904727,0.01309963,0.02933756,-0.01465495,0.04940299,0.05499387,0.00976153,-0.01103349,-0.00595467,0.02443041,-0.03920757,0.03633981,0.00860752,-0.02482854,-0.05750548,-0.03115786,0.07077158,0.03576012,0.03442372,-0.0199653,-0.01187318,0.02776676,0.2171728,0.0206028,-0.01661498,-0.02102916,-0.03238936,0.02096429,0.00368389,-0.0066413,-0.0519315,-0.00532563,0.07171506,0.0579427,0.03730921,0.10990694,0.00703096,-0.00833749,0.03690657,0.03732051,-0.06310452,-0.00927601,-0.06304912,-0.02967834,0.10142177,0.00028759,-0.05255607,-0.04541893,0.00978261,-0.00868692,-0.02130126,0.0261169,-0.00579299,-0.00650667,0.01370096,0.05409321,0.04269139,-0.0325196,-0.01938917,-0.06411788,0.01974723,0.01690643,0.01548002,0.04685719,-0.00320546],"last_embed":{"hash":"946b991787d52ff5f60b6dd0b5a36c2a84c8025724aa7461dc7f7b1afb2101bb","tokens":213}}},"text":null,"length":0,"last_read":{"hash":"946b991787d52ff5f60b6dd0b5a36c2a84c8025724aa7461dc7f7b1afb2101bb","at":1757414183201},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#","lines":[4,13],"size":337,"outlinks":[{"title":"blog.csdn.net/pipisorry/article/details/52247379","target":"http://blog.csdn.net/pipisorry/article/details/52247379","line":3},{"title":"[均值、方差与协方差矩阵","target":"http://blog.csdn.net/pipisorry/article/details/48788671","line":7},{"title":"[矩阵论：向量范数和矩阵范数","target":"http://blog.csdn.net/pipisorry/article/details/51030563","line":9}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.0464342,-0.03137784,0.01084031,-0.01392579,0.02647017,-0.02281949,-0.02274483,-0.0275844,0.02879855,0.00400164,0.00718788,-0.02578808,0.08330251,0.02387167,0.04913834,-0.00208359,0.03666412,0.02643955,-0.08126406,-0.01302338,0.13371553,-0.03033687,0.04162893,-0.06738812,0.09773877,0.01226675,-0.0394032,-0.06850763,-0.0127078,-0.20122147,0.03054949,0.02018486,0.06068238,-0.01491208,0.05572556,-0.07928373,-0.02338468,0.06791679,0.01261463,0.02870651,0.04462857,0.00919896,-0.01208311,-0.00051638,0.0047106,-0.02960231,-0.03450549,-0.04683977,-0.01227453,-0.03176868,-0.0176903,-0.03461699,-0.02341445,0.0316898,-0.00187076,0.00369852,0.08415674,0.00827879,0.05672261,0.03919032,0.0058752,0.08601784,-0.21260649,0.04717855,0.01791497,-0.02863215,-0.00692131,-0.04727069,0.07248635,0.07918043,-0.01381681,-0.02140739,0.01298006,0.06591775,0.0247646,-0.04339568,0.02305314,0.01252085,-0.0256742,0.01804295,-0.05499663,0.06328923,0.07584672,-0.00860721,0.04372566,0.00354006,0.03006009,-0.05036614,-0.0144641,-0.04522663,-0.0299656,-0.00677516,-0.03219738,-0.01306064,-0.02442177,-0.05489217,0.05441023,0.02414209,-0.0593104,0.10984412,-0.03632119,0.05193771,-0.01687145,0.01156026,0.05954351,0.05741182,-0.05301054,-0.04190941,-0.01644918,-0.0182715,-0.01613442,-0.00937518,0.05825343,-0.04730333,-0.03850224,0.01340717,0.04324393,0.05041376,-0.00166928,0.01755832,0.01751732,0.00802772,0.04005885,-0.04198297,0.03172021,-0.03335336,0.0113164,0.04317789,0.0326852,0.01267297,0.07691152,0.04192619,-0.08699623,0.0010772,0.06562479,-0.00269632,-0.02295184,-0.02477388,0.00930668,-0.01176103,-0.04598796,-0.03878601,-0.00626264,-0.08342072,-0.09540614,0.08472747,-0.08473188,-0.00891237,-0.02935998,-0.05942169,0.00223582,0.01919827,-0.01606417,-0.05592241,0.0664748,-0.01922797,0.07367745,0.09702687,-0.05162064,-0.04125656,0.00467361,-0.06615846,-0.05600441,0.10869328,0.05514937,-0.05202296,-0.00117722,0.03985852,0.0022511,-0.04054417,0.05790951,-0.00161826,-0.0295258,0.01018558,0.01671633,-0.02607806,-0.03452527,-0.04533027,-0.00003835,0.06122304,0.05587942,-0.01617814,-0.03528593,0.04967546,0.00648458,-0.11127372,0.00145773,-0.04591209,-0.02792576,-0.00553451,-0.06178629,0.01050234,-0.04377959,-0.03376053,-0.02682768,0.02282557,0.00191781,-0.02518753,0.01818127,-0.05969772,0.05876329,0.04877941,0.05043605,-0.03712846,-0.04780909,0.01127311,0.04763361,-0.07291195,0.00705508,0.05553677,-0.02895707,-0.00150355,0.00176959,-0.02998569,-0.01078937,0.00614159,-0.01654551,0.07419473,-0.00922017,0.0713082,-0.00095075,0.01798785,-0.10070253,-0.22337191,-0.026283,0.03462752,-0.0569872,0.07227253,0.00459711,-0.01959358,-0.01744957,0.0484871,0.09327946,0.0624981,0.02830756,-0.02776562,0.03291571,0.00032994,-0.00180227,0.06470652,0.0379642,-0.02089249,0.0174647,-0.01662747,0.03026346,-0.01239855,-0.03606704,-0.0106428,0.02325118,0.09689631,-0.03488335,0.06994235,0.00330923,0.0260735,0.02751315,0.02180837,-0.11224358,0.04157105,0.00328727,-0.03514268,-0.03769267,-0.01446158,-0.05903434,0.01218414,0.00756597,-0.00159827,-0.08820622,-0.04017117,-0.00722486,-0.00178784,-0.00438397,-0.010687,0.01672882,-0.03257712,-0.02763045,-0.02753878,0.06033138,0.01089927,-0.03259971,-0.07147564,-0.02979061,-0.05391867,0.00447639,-0.03868115,-0.04932336,0.05957807,-0.05782783,-0.06429757,-0.01812568,-0.00804992,0.01236278,0.03319116,0.01368534,-0.12876055,0.13994938,0.02737893,0.02643987,0.01910481,0.07019612,-0.05660116,0.0002235,0.01286842,0.02380962,0.13065268,0.0041215,0.07306775,0.07385065,-0.01197816,0.02011241,0.0221134,-0.03667901,0.03404316,-0.0255438,-0.03298884,0.00009839,-0.07394538,-0.01962997,0.05327814,0.01383502,-0.26382232,0.00172229,0.00204283,-0.03810527,0.04033279,0.04821609,0.02422155,0.00879683,0.01125681,0.01748702,-0.0171873,0.04289759,0.02698946,-0.03447463,-0.04991757,-0.04681914,0.08265526,0.02301029,0.03935111,-0.02096744,-0.02162467,0.0439061,0.22964114,-0.00064262,0.01695164,-0.01834417,-0.0112503,0.01837101,0.00969795,0.01141888,-0.04855896,-0.02140155,0.09800879,0.0407056,0.0599146,0.1012274,-0.00452779,-0.01667123,0.0242665,0.01210167,-0.0471423,-0.02446502,-0.05894289,-0.02111635,0.08273181,-0.00698932,-0.05324203,-0.06842763,0.00876997,-0.0148165,-0.01943502,0.01963789,0.00635543,-0.00980166,0.01839417,0.04295981,0.0454247,0.00012962,-0.01534592,-0.06789664,0.01824196,0.00748661,0.01390451,0.07864327,0.02752667],"last_embed":{"hash":"61c6fa07afda95e2dc9578ed8cfdab124936f809ad03945bf99c127b324b924a","tokens":460}}},"text":null,"length":0,"last_read":{"hash":"61c6fa07afda95e2dc9578ed8cfdab124936f809ad03945bf99c127b324b924a","at":1757414183224},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化","lines":[14,31],"size":1644,"outlinks":[{"title":"归一化的目标","target":"http://blog.csdn.net/pipisorry/article/details/null","line":7},{"title":"1 把数变为（0，1）之间的小数        主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。2 把有量纲表达式变为无量纲表达式        归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 比如，复数阻抗可以归一化书写：Z = R + jωL = R(1 + jωL/R) ，复数部分变成了纯数量了，没有量纲。 另外，微波之中也就是电路分析、信号系统、电磁波传输等，有很多运算都可以如此处理，既保证了运算的便捷，又能凸现出物理量的本质含义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":9},{"title":"    在多指标评价体系中，由于各评价指标的性质不同，通常具有不同的量纲和数量级。当各指标间的水平相差很大时，如果直接用原始指标值进行分析，就会突出数值较高的指标在综合分析中的作用，相对削弱数值水平较低指标的作用。因此，为了保证结果的可靠性，需要对原始指标数据进行标准化处理。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":11},{"title":"    在数据分析之前，我们通常需要先将数据标准化（normalization），利用标准化后的数据进行数据分析。数据标准化也就是统计数据的指数化。数据标准化处理主要包括数据同趋化处理和无量纲化处理两个方面。数据同趋化处理主要解决不同性质数据问题，对不同性质指标直接加总不能正确反映不同作用力的综合结果，须先考虑改变逆指标数据性质，使所有指标对测评方案的作用力同趋化，再加总才能得出正确结果。数据无量纲化处理主要解决数据的可比性。数据标准化的方法有很多种，常用的有“最小—最大标准化”、“Z-score标准化”和“按小数定标标准化”等。经过上述标准化处理，原始数据均转换为无量纲化指标测评值，即各指标值都处于同一个数量级别上，可以进行综合测评分析。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":13},{"title":"从经验上说，归一化是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":15},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","line":17},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":17}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04611126,-0.02212378,0.01232251,-0.00943421,0.02909872,-0.01948029,-0.01529958,-0.03144559,0.0279235,-0.00163867,0.00399801,-0.02384993,0.08691806,0.03045432,0.04501967,0.00207678,0.04348417,0.02626534,-0.07656963,-0.01105786,0.1297552,-0.0302363,0.03365644,-0.07277479,0.10238207,0.00896384,-0.03371524,-0.07054077,-0.01443913,-0.19917876,0.02024577,0.01333333,0.06611466,-0.01116582,0.06359638,-0.08533812,-0.0187028,0.06057597,0.01292152,0.02753164,0.04610344,0.01655696,-0.00725062,-0.00401849,0.0072876,-0.02345218,-0.02250848,-0.04626105,-0.01860072,-0.02634985,-0.0175552,-0.03421773,-0.0246107,0.03293498,-0.00921574,0.00652761,0.08925661,0.00680941,0.05615555,0.03751124,0.00339958,0.08050574,-0.20693022,0.04207003,0.01486709,-0.0280282,-0.00565265,-0.05001392,0.07241725,0.08674683,-0.00670907,-0.0186629,0.01214448,0.07820576,0.02519272,-0.04093006,0.02293548,0.01265473,-0.0255463,0.02168529,-0.05394613,0.0607792,0.06741143,-0.00706668,0.04668114,0.01891792,0.03816689,-0.04708697,-0.01587648,-0.04317956,-0.02897089,-0.0083787,-0.02704397,-0.01327421,-0.02984301,-0.05406035,0.05533361,0.01911619,-0.05861148,0.10699178,-0.03425559,0.05780813,-0.02034354,0.0099793,0.05767575,0.06044545,-0.0526773,-0.03856232,-0.02218873,-0.01633064,-0.0188199,-0.00611234,0.0550027,-0.04431376,-0.03771258,0.01195507,0.04175653,0.05697053,-0.00946973,0.01332348,0.00726952,0.00905859,0.04219794,-0.04371865,0.03885457,-0.03530152,0.00443563,0.04503867,0.0285978,0.00823455,0.07242727,0.0361768,-0.09399041,-0.00348334,0.06568617,-0.00739822,-0.02191258,-0.02306215,0.00396161,-0.02457818,-0.04912129,-0.03654125,-0.0054954,-0.08457477,-0.09055648,0.0775857,-0.07800262,-0.00675918,-0.02510685,-0.06149973,0.00526421,0.01459315,-0.02020646,-0.05506628,0.06244492,-0.02059542,0.08080173,0.10887622,-0.05943695,-0.04147987,0.00308271,-0.06483886,-0.06296714,0.11518349,0.05593833,-0.04909177,-0.00014131,0.03448063,-0.00377326,-0.04498725,0.05428587,-0.00386349,-0.02953337,0.0084828,0.01739434,-0.02228961,-0.03016919,-0.04293802,-0.00383806,0.06429606,0.05518652,-0.00464074,-0.03380734,0.0453134,0.00639412,-0.11354081,-0.00595401,-0.04351057,-0.0286212,-0.00967586,-0.07720821,0.0165484,-0.04657935,-0.03673046,-0.03653125,0.02097791,0.00859209,-0.03215837,0.02493147,-0.05355356,0.05814683,0.04747535,0.04391417,-0.03514713,-0.03565256,0.01272958,0.04802927,-0.07460889,0.00084227,0.0608126,-0.02442181,0.0012088,-0.00253262,-0.02769263,-0.00878282,0.00388238,-0.01358151,0.07185454,-0.02231435,0.0655963,0.00544507,0.02163956,-0.0995551,-0.2179496,-0.02306855,0.02872237,-0.06319502,0.07767354,0.00599091,-0.02039495,-0.00989837,0.04695487,0.08586011,0.06394646,0.04063155,-0.02922387,0.03008648,0.00005899,-0.00038786,0.05952978,0.036633,-0.02522044,0.01614374,-0.02022791,0.0271394,-0.01228416,-0.04031542,-0.00251567,0.02860113,0.09689134,-0.02721299,0.08585428,-0.00492844,0.03390761,0.02140249,0.02584006,-0.11209382,0.0408274,0.00474641,-0.03550963,-0.04311697,-0.01224706,-0.06420808,0.01466896,0.00355086,-0.00593461,-0.08696978,-0.04313644,-0.00665394,-0.00362108,-0.00144375,-0.0123297,0.00679077,-0.0379316,-0.0293936,-0.03376316,0.05733394,0.00689297,-0.02890249,-0.06660404,-0.02727913,-0.05976624,0.00623704,-0.03738159,-0.04601863,0.05774671,-0.05838406,-0.06111041,-0.01912609,-0.00721414,0.01149121,0.03773818,0.01089197,-0.13231872,0.13882799,0.0184236,0.02881091,0.01329598,0.07435364,-0.05626557,-0.0016457,0.01043299,0.02652096,0.1295685,0.0013763,0.07339048,0.06870165,-0.00944514,0.02495279,0.02936126,-0.03905237,0.03457496,-0.02645262,-0.03252358,-0.00412822,-0.07345618,-0.0213128,0.05365579,0.00761899,-0.25729334,-0.00665331,0.00583731,-0.0402332,0.03853937,0.04892492,0.02758981,0.0152084,0.00231841,0.01093714,-0.01163326,0.04873667,0.0300317,-0.03658569,-0.04272212,-0.05298248,0.07620946,0.03364747,0.0375144,-0.01617115,-0.01863011,0.04476904,0.23342583,0.00441528,0.02067134,-0.01559486,-0.00258937,0.02088725,0.00506764,0.01616414,-0.03912855,-0.03115274,0.09489977,0.03929199,0.06441198,0.10097066,-0.00263687,-0.01907696,0.02169129,0.0119711,-0.05241546,-0.02621432,-0.05846198,-0.02301513,0.07851475,-0.01372877,-0.05327932,-0.068236,0.01051774,-0.01858442,-0.02511171,0.01751611,0.01109424,-0.00535672,0.02510745,0.0491704,0.04967367,-0.00199503,-0.02520322,-0.06735293,0.02785304,0.0059262,0.01247064,0.06862831,0.02496696],"last_embed":{"hash":"217037b2b2b47c79dd5f5292011116bf1f488cd728ec345f95f6b9cb1ef463bc","tokens":336}}},"text":null,"length":0,"last_read":{"hash":"217037b2b2b47c79dd5f5292011116bf1f488cd728ec345f95f6b9cb1ef463bc","at":1757414183268},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#{1}","lines":[16,19],"size":282,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#[归一化的目标](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.0315774,-0.02047614,0.00095935,-0.00915751,0.03585546,-0.01177242,-0.02522087,-0.01781086,0.02691606,0.0074278,0.00512754,-0.02584795,0.09596175,0.03714301,0.04954682,-0.01333327,0.04929455,0.01956861,-0.08843523,-0.01509965,0.13315175,-0.0435438,0.05165866,-0.06255767,0.10514644,0.0113347,-0.03372133,-0.04870133,-0.02030785,-0.19793861,0.02386492,0.01408364,0.06665533,-0.00667372,0.06551213,-0.08237432,-0.02060986,0.08201043,0.02988284,0.00849739,0.0385008,-0.00412014,-0.02155861,0.01409778,-0.00135286,-0.04213523,-0.03411486,-0.04462688,0.00367732,-0.04811483,-0.01291995,-0.02452369,-0.02494753,0.02477681,0.00601047,-0.00000177,0.06467742,0.00994152,0.06184271,0.0395021,0.00684324,0.08464655,-0.21276942,0.04986691,0.01334255,-0.02122271,-0.01865762,-0.04240471,0.07050591,0.06386157,-0.01908389,-0.03076945,0.01327128,0.06530623,0.02814976,-0.05554581,0.01402462,0.00907868,-0.03369373,0.01397172,-0.03016245,0.06057683,0.06261759,-0.01114721,0.04868668,0.00374381,0.05585403,-0.04141172,-0.02198535,-0.04411619,-0.0382658,-0.00587938,-0.00905691,-0.00408623,-0.03339811,-0.03781218,0.06019186,0.01987548,-0.05861443,0.10653602,-0.03484919,0.04324043,-0.01295176,-0.00055845,0.06766739,0.04812855,-0.04503791,-0.03296274,-0.00569627,-0.01630445,-0.01933223,-0.02053553,0.04980905,-0.04211236,-0.01732024,-0.01343411,0.04936539,0.03803126,-0.02293305,0.01873602,0.02745287,0.00873216,0.0437996,-0.04189581,0.04089672,-0.03167663,0.01538199,0.0407494,0.0279394,0.01881253,0.07591182,0.03880906,-0.08880857,-0.00508166,0.07310323,0.0039374,-0.01636336,-0.03494115,0.00322194,-0.01463245,-0.05066968,-0.04801568,-0.00943381,-0.08051468,-0.09762596,0.09060133,-0.07969701,-0.00424489,-0.01875692,-0.05378421,-0.01424562,0.03132029,-0.01707055,-0.05567089,0.06367153,-0.01492998,0.07553908,0.09943526,-0.0470922,-0.04599471,-0.00278308,-0.08061672,-0.05765685,0.11658784,0.06026127,-0.05629383,-0.00884837,0.02734832,-0.00240369,-0.04570273,0.05872019,-0.01241528,-0.02283546,-0.00387898,0.018139,-0.02500192,-0.03008366,-0.06097769,0.01514796,0.065688,0.03674608,-0.0239508,-0.04724687,0.0449914,0.01147013,-0.09960353,-0.01447117,-0.04877817,-0.02785487,-0.00524697,-0.06035407,0.01335631,-0.02921248,-0.04235139,-0.03265281,0.01651744,0.02175012,-0.0334517,0.0022709,-0.06977001,0.07810528,0.03953157,0.04984068,-0.03104179,-0.0492445,-0.00384006,0.05610875,-0.06383837,0.00180421,0.05383039,-0.03551233,-0.01434731,-0.00756323,-0.03903912,-0.0164773,0.0022562,-0.01746685,0.05784504,0.0105632,0.07482719,-0.00497723,0.0210874,-0.09953439,-0.22180113,-0.02701956,0.04449228,-0.06510523,0.08404897,0.00116037,-0.0145424,-0.01653315,0.04859948,0.09790859,0.08167007,0.01731755,-0.02790397,0.03069763,0.0039418,0.01192659,0.07943104,0.03247546,-0.01650034,0.01655702,-0.01033068,0.02624651,-0.02577442,-0.02186412,-0.00722773,0.01712836,0.09665944,-0.0135775,0.07572128,-0.00584713,0.02234769,0.02751574,0.0191205,-0.09063768,0.03509812,0.01748956,-0.04684133,-0.04038839,-0.0217045,-0.05663614,-0.0012583,0.00505803,-0.00054543,-0.08845558,-0.04804685,-0.01787342,-0.0148527,0.00581657,-0.01220609,0.03550106,-0.03614841,-0.01663644,-0.01721357,0.06605604,0.02303339,-0.02549524,-0.05790351,-0.03576368,-0.05179272,0.0068186,-0.02808154,-0.04507542,0.05208178,-0.05418324,-0.06313543,-0.02103961,-0.01684093,0.01821635,0.03073568,0.0227613,-0.1301996,0.14263256,0.03564728,0.04475618,0.0201792,0.05898126,-0.05721635,-0.01166185,0.03042841,0.01483881,0.12054159,0.00179837,0.07525203,0.07410862,-0.00506861,0.02844284,0.03319339,-0.04252243,0.03399462,-0.01315297,-0.05010299,-0.02199742,-0.05995999,-0.01451085,0.06068178,0.01106944,-0.26693901,-0.00312445,0.00259715,-0.04115005,0.04955963,0.04156997,0.03255487,0.00378431,0.00063485,0.02795147,-0.02238954,0.03559875,0.0080719,-0.03700399,-0.05505275,-0.03471384,0.06343742,0.03555198,0.03277646,-0.01406528,-0.02037142,0.04792031,0.22617339,0.01013341,0.00817063,-0.01594085,-0.01917309,0.01287248,-0.00724852,0.02633531,-0.05266584,-0.02114058,0.08709168,0.05819115,0.05353275,0.09968127,0.0011398,0.00071464,0.03318327,0.01071057,-0.04972637,-0.0131766,-0.05489931,-0.02236486,0.08477901,-0.02301399,-0.05247007,-0.05490793,0.00547764,-0.01710458,-0.02524894,0.03302731,0.00429189,-0.01520279,0.00994838,0.04238046,0.02587344,-0.01224069,-0.02115155,-0.06925073,0.02471534,0.00732061,0.01764537,0.06352431,0.02153974],"last_embed":{"hash":"ec842c102e8308122d01cbd1581e006875ae6c23650bdd13960fa6611a689289","tokens":451}}},"text":null,"length":0,"last_read":{"hash":"ec842c102e8308122d01cbd1581e006875ae6c23650bdd13960fa6611a689289","at":1757414183302},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#[归一化的目标](http://blog.csdn.net/pipisorry/article/details/null)","lines":[20,31],"size":1278,"outlinks":[{"title":"归一化的目标","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"1 把数变为（0，1）之间的小数        主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。2 把有量纲表达式变为无量纲表达式        归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 比如，复数阻抗可以归一化书写：Z = R + jωL = R(1 + jωL/R) ，复数部分变成了纯数量了，没有量纲。 另外，微波之中也就是电路分析、信号系统、电磁波传输等，有很多运算都可以如此处理，既保证了运算的便捷，又能凸现出物理量的本质含义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"    在多指标评价体系中，由于各评价指标的性质不同，通常具有不同的量纲和数量级。当各指标间的水平相差很大时，如果直接用原始指标值进行分析，就会突出数值较高的指标在综合分析中的作用，相对削弱数值水平较低指标的作用。因此，为了保证结果的可靠性，需要对原始指标数据进行标准化处理。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5},{"title":"    在数据分析之前，我们通常需要先将数据标准化（normalization），利用标准化后的数据进行数据分析。数据标准化也就是统计数据的指数化。数据标准化处理主要包括数据同趋化处理和无量纲化处理两个方面。数据同趋化处理主要解决不同性质数据问题，对不同性质指标直接加总不能正确反映不同作用力的综合结果，须先考虑改变逆指标数据性质，使所有指标对测评方案的作用力同趋化，再加总才能得出正确结果。数据无量纲化处理主要解决数据的可比性。数据标准化的方法有很多种，常用的有“最小—最大标准化”、“Z-score标准化”和“按小数定标标准化”等。经过上述标准化处理，原始数据均转换为无量纲化指标测评值，即各指标值都处于同一个数量级别上，可以进行综合测评分析。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":7},{"title":"从经验上说，归一化是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":9},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","line":11},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":11}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#[归一化的目标](http://blog.csdn.net/pipisorry/article/details/null)#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.03199256,-0.01775795,-0.0010551,-0.00929647,0.03923859,-0.01290666,-0.02198021,-0.01790724,0.02533394,0.01230313,0.01295827,-0.02649732,0.09342076,0.03695215,0.04908379,-0.01235217,0.05228914,0.0181395,-0.08450662,-0.01464828,0.13757358,-0.04390881,0.05341326,-0.06060681,0.1062076,0.00971699,-0.03588054,-0.04459552,-0.02073555,-0.19515891,0.02071052,0.0162034,0.0654434,-0.00903727,0.06520212,-0.08501296,-0.01486906,0.0836806,0.03440578,0.0088408,0.03490291,-0.00525182,-0.02521463,0.01198902,-0.0048894,-0.04313603,-0.03323453,-0.04369123,0.00734396,-0.05117547,-0.01136406,-0.02231528,-0.02918022,0.02355855,0.00636469,-0.00130623,0.0629609,0.01299381,0.05956542,0.03525907,0.00901453,0.08630329,-0.21261358,0.05045485,0.01488951,-0.01888052,-0.01846806,-0.04053255,0.07117166,0.06666775,-0.01857109,-0.02742549,0.01534359,0.0648039,0.02848052,-0.0530421,0.01260808,0.01215999,-0.03287527,0.01334973,-0.02893948,0.05741359,0.06223105,-0.00923417,0.04599803,0.00498566,0.0553361,-0.03897951,-0.02372312,-0.04184329,-0.03767965,-0.00727636,-0.01209401,-0.00854253,-0.03902121,-0.03588327,0.05974395,0.01993448,-0.06171649,0.10464206,-0.03215779,0.04161339,-0.01724451,-0.00009731,0.07005939,0.04944314,-0.04733195,-0.03648032,-0.00138523,-0.0213788,-0.0177362,-0.01893885,0.05039811,-0.03936149,-0.02190592,-0.01248052,0.04849919,0.0355213,-0.02811027,0.01965496,0.02525743,0.0074326,0.04897936,-0.04289435,0.03938132,-0.03124146,0.01733666,0.04126889,0.02768147,0.01604806,0.07597172,0.03465504,-0.08410984,-0.00682743,0.07553898,0.00251254,-0.01641976,-0.032645,0.0020906,-0.01786983,-0.05206908,-0.05373982,-0.00966759,-0.07980057,-0.09930202,0.09033743,-0.07807136,-0.00119277,-0.01759241,-0.05630021,-0.01907859,0.03166835,-0.01759411,-0.05764305,0.06124529,-0.01045535,0.07382581,0.10275055,-0.04760037,-0.04042757,-0.00574093,-0.08252966,-0.05819069,0.11271269,0.05828505,-0.05349096,-0.0112685,0.0303228,-0.00802284,-0.04373552,0.06141511,-0.01781986,-0.02491904,-0.0089624,0.01384875,-0.0254302,-0.02480026,-0.0594706,0.01797347,0.06744126,0.03441535,-0.02513064,-0.04902493,0.05022431,0.00826392,-0.09958027,-0.02093204,-0.04792235,-0.03371032,-0.00721503,-0.06309251,0.01176888,-0.02861642,-0.04210159,-0.03481335,0.01561437,0.02230151,-0.03058207,0.00078641,-0.07103626,0.08366594,0.03906784,0.05299664,-0.03105554,-0.04697055,-0.00656965,0.05413769,-0.06267688,0.00401043,0.05166275,-0.03320315,-0.0134465,-0.00890873,-0.04116281,-0.01676399,0.00292643,-0.01885941,0.05630051,0.0103234,0.078027,-0.00722772,0.02189015,-0.09802263,-0.22422981,-0.02408621,0.04231805,-0.06740812,0.08746631,0.00002776,-0.01132165,-0.01922371,0.04718072,0.09413601,0.08214781,0.01931048,-0.02642674,0.03244433,0.00274198,0.01150781,0.08085909,0.02747091,-0.01421269,0.01537329,-0.01011318,0.02098897,-0.0241956,-0.02399776,-0.00816566,0.01660941,0.09717983,-0.01370007,0.07861735,-0.00793601,0.01867579,0.034189,0.01946136,-0.08665185,0.02966186,0.01868161,-0.04215217,-0.04167674,-0.02221258,-0.05645953,-0.00648769,0.00802543,0.00566713,-0.08603204,-0.04578297,-0.01609558,-0.01548886,0.00864,-0.01254576,0.03963901,-0.03649055,-0.01456401,-0.01480102,0.06827031,0.0221756,-0.02123191,-0.05591637,-0.03446703,-0.04917019,0.0089612,-0.0258717,-0.03951688,0.05151031,-0.0498088,-0.0663056,-0.02083353,-0.01778013,0.01657697,0.03001162,0.02735013,-0.12835844,0.14251377,0.03942985,0.04243993,0.0141021,0.05915603,-0.05506299,-0.01133851,0.02777258,0.01531577,0.11738983,-0.00293892,0.07672625,0.07348308,-0.00433182,0.03572974,0.03073088,-0.04242976,0.03588489,-0.01380768,-0.05116891,-0.02038631,-0.06178952,-0.01233079,0.06302415,0.00895308,-0.2649776,-0.00173164,-0.00222086,-0.04415958,0.05529007,0.04132204,0.03914458,0.0029851,-0.00062114,0.02660552,-0.01921392,0.03191417,0.00611035,-0.03684652,-0.05574781,-0.03497122,0.06350257,0.03897955,0.03374824,-0.01967034,-0.01847507,0.04989446,0.22593778,0.00807977,0.00813918,-0.01354153,-0.01828747,0.01220381,-0.00903163,0.02773564,-0.0549765,-0.02512222,0.08957855,0.05654844,0.05506477,0.09640557,-0.00270006,0.00587751,0.03419193,0.00405301,-0.05115627,-0.01540412,-0.0522849,-0.01738001,0.08320849,-0.02424414,-0.0530149,-0.05692163,0.00591878,-0.0169154,-0.02705548,0.03852804,0.00934331,-0.01161169,0.00916605,0.03610129,0.01918869,-0.01500928,-0.02519244,-0.07235319,0.02763388,0.00604313,0.01919824,0.06235494,0.02589883],"last_embed":{"hash":"64825841e06a702853c2b6d5f2ff53779150ab10fe1e9cf22ce955f8ceb2b070","tokens":445}}},"text":null,"length":0,"last_read":{"hash":"64825841e06a702853c2b6d5f2ff53779150ab10fe1e9cf22ce955f8ceb2b070","at":1757414183348},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md##<http://blog.csdn.net/pipisorry/article/details/null>数据的标准化（normalization）和归一化#[归一化的目标](http://blog.csdn.net/pipisorry/article/details/null)#{1}","lines":[22,31],"size":1211,"outlinks":[{"title":"1 把数变为（0，1）之间的小数        主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。2 把有量纲表达式变为无量纲表达式        归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 比如，复数阻抗可以归一化书写：Z = R + jωL = R(1 + jωL/R) ，复数部分变成了纯数量了，没有量纲。 另外，微波之中也就是电路分析、信号系统、电磁波传输等，有很多运算都可以如此处理，既保证了运算的便捷，又能凸现出物理量的本质含义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"    在多指标评价体系中，由于各评价指标的性质不同，通常具有不同的量纲和数量级。当各指标间的水平相差很大时，如果直接用原始指标值进行分析，就会突出数值较高的指标在综合分析中的作用，相对削弱数值水平较低指标的作用。因此，为了保证结果的可靠性，需要对原始指标数据进行标准化处理。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"    在数据分析之前，我们通常需要先将数据标准化（normalization），利用标准化后的数据进行数据分析。数据标准化也就是统计数据的指数化。数据标准化处理主要包括数据同趋化处理和无量纲化处理两个方面。数据同趋化处理主要解决不同性质数据问题，对不同性质指标直接加总不能正确反映不同作用力的综合结果，须先考虑改变逆指标数据性质，使所有指标对测评方案的作用力同趋化，再加总才能得出正确结果。数据无量纲化处理主要解决数据的可比性。数据标准化的方法有很多种，常用的有“最小—最大标准化”、“Z-score标准化”和“按小数定标标准化”等。经过上述标准化处理，原始数据均转换为无量纲化指标测评值，即各指标值都处于同一个数量级别上，可以进行综合测评分析。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5},{"title":"从经验上说，归一化是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":7},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.3.png","line":9},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":9}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04276387,-0.039997,0.01267835,-0.02206113,0.05999396,0.00962336,-0.03049381,-0.00832436,0.03171458,0.00341739,0.02577253,-0.04374725,0.07770035,0.04429531,0.05280045,0.01858333,0.02954835,0.03634377,-0.08643925,-0.00863963,0.13497075,-0.04588329,0.03732563,-0.07836674,0.10845187,-0.00601294,-0.03078243,-0.05066201,-0.01603542,-0.21545696,0.02592093,0.00105306,0.05938904,-0.02556241,0.06661583,-0.05946061,-0.02914221,0.06739616,0.0316275,0.05021583,0.04496018,0.02755956,-0.0143131,0.01502246,0.01413704,-0.05472358,-0.04784219,-0.05490276,-0.02418957,0.00015376,-0.01512361,-0.01457006,-0.00262843,0.01673488,0.03267066,0.02259309,0.06758646,0.02526236,0.05869266,0.04840497,0.00503268,0.08397736,-0.18921266,0.03798689,0.00350018,-0.01375738,-0.01294499,-0.0273757,0.03849306,0.07253737,0.00348424,-0.01760145,0.02002622,0.06855595,0.03477354,-0.04589456,-0.00747015,0.01597447,-0.01530532,-0.01380742,-0.04124843,0.0575659,0.04027416,-0.02356208,0.06699807,0.00868337,0.05446264,-0.05095611,0.0159353,-0.07172002,-0.03838334,-0.01154337,0.00359321,0.0074448,-0.04295629,-0.04942526,0.0580759,0.02501703,-0.04489912,0.11706575,-0.01528677,0.03677342,-0.00384838,-0.01862299,0.05914875,0.05888852,-0.06053757,-0.0119487,-0.01200729,-0.007734,-0.05419738,-0.01348294,0.04338796,-0.03548688,-0.00737668,-0.00485026,0.04503002,0.03498374,-0.02439252,0.03191929,0.0150688,0.00962354,0.03095778,-0.04137328,-0.03004,-0.01163179,-0.00035666,0.03086367,0.03299436,0.04598261,0.07701254,0.0272082,-0.09663939,-0.0292444,0.06478491,-0.00120104,-0.01280596,-0.03670832,-0.00733002,-0.00223906,-0.02357132,-0.06786098,0.00003071,-0.05051,-0.10545321,0.12463465,-0.09411368,-0.04447629,0.00189483,-0.05549536,-0.00226002,0.00018903,-0.0397885,-0.05751655,0.06570148,-0.01957699,0.08716626,0.06735312,-0.04884077,-0.06773382,0.00676835,-0.05833801,-0.04898873,0.1218027,0.05427213,-0.04375845,-0.00487408,0.04982908,-0.00586773,-0.02755384,0.05659325,-0.02093268,-0.03853143,-0.00267175,0.0370171,-0.00270102,-0.04065192,-0.05857854,-0.00606417,0.08740041,0.03547393,-0.04387314,-0.05110587,0.05300876,0.01930706,-0.08465111,0.01316706,-0.03952039,-0.02335322,-0.00583954,-0.07098048,0.00696048,-0.03631224,-0.05442619,-0.03419989,0.0030163,0.02848929,-0.0283708,0.01264264,-0.05659773,0.07959208,0.04650543,0.02245089,-0.04479999,-0.01872657,0.01796364,0.05166664,-0.07962874,-0.00637147,0.06043043,-0.02945011,-0.02159173,0.01476627,-0.02028096,-0.01848215,0.01872179,0.00530194,0.06029024,-0.00860819,0.0626869,-0.00566463,-0.00210866,-0.09982599,-0.22085422,-0.02595443,0.03411594,-0.07347622,0.07764082,-0.03127119,0.01015118,0.00846931,0.04249648,0.09543578,0.06487244,-0.01254164,-0.01933225,0.0271876,0.00766463,-0.00113852,0.05356901,0.0443557,-0.01176536,0.03016176,0.00763838,0.00595367,-0.04901444,-0.04150235,-0.00607651,0.01640603,0.11230439,-0.00645181,0.08875999,0.00043613,0.01890072,-0.00182413,0.01713141,-0.06824586,0.0481042,0.00590848,-0.03994069,-0.02510289,-0.04724802,-0.03604262,0.01892311,0.02147018,-0.00266897,-0.10012073,-0.05214773,-0.0108177,-0.01244542,-0.00921804,-0.00526711,0.03538679,-0.0427958,-0.02225205,-0.03408738,0.0500804,0.01880292,-0.04374359,-0.06999011,-0.02786905,-0.04445257,0.01695332,-0.02164422,-0.06035599,0.04736114,-0.07516234,-0.04958987,-0.02253266,-0.00788298,0.03027482,0.03877105,0.01193216,-0.09293408,0.13783531,0.03219557,0.02943552,0.01306289,0.07540552,-0.0251829,0.0199006,0.02363763,0.01879935,0.08369145,-0.01361313,0.06849509,0.06159101,-0.00573391,0.02262091,0.02054377,-0.04564033,-0.00205615,-0.00247731,-0.04360569,0.00728841,-0.05341439,-0.02599783,0.06099825,-0.00680965,-0.26934475,-0.0014331,0.02345091,-0.01190757,0.04570258,0.05173717,0.01338884,0.01675761,-0.03860528,0.01725532,-0.05685404,0.02425394,0.00307533,-0.01696556,-0.05807864,-0.03349693,0.0627038,0.04543148,0.02488012,-0.01436739,-0.02926871,0.03420508,0.22146228,-0.00570211,-0.0121031,-0.00741399,-0.03133947,0.02149731,0.03193463,0.00343384,-0.05543859,-0.00747515,0.10103363,0.05120108,0.01887872,0.10219741,-0.0222557,-0.01949546,0.03872402,0.04731283,-0.04904621,-0.0352901,-0.03648464,-0.03188605,0.10249301,-0.01721102,-0.05121129,-0.05468076,0.00308284,-0.01074322,-0.04195471,0.05087988,0.00272779,-0.01205523,0.02260626,0.03936192,0.05681077,-0.02066379,-0.00608549,-0.07357454,0.03219186,0.00063288,0.01678085,0.04069436,0.02065444],"last_embed":{"hash":"019e695a1d8739592b78eced54ccfdf7aaee98b51ea8f891b1903e2395db1aea","tokens":411}}},"text":null,"length":0,"last_read":{"hash":"019e695a1d8739592b78eced54ccfdf7aaee98b51ea8f891b1903e2395db1aea","at":1757414183395},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)","lines":[32,123],"size":4268,"outlinks":[{"title":"常见的数据归一化方法","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"min-max标准化(Min-max normalization)","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"也叫离差标准化，是对原始数据的线性变换。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5},{"title":"使结果落到0,1区间，转换函数如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":7},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","line":9},{"title":"其中max为样本数据的最大值，min为样本数据的最小值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":11},{"title":"def Normalization(x):    return (float(i)-min(x))/float(max(x)-min(x)) for i in x","target":"http://blog.csdn.net/pipisorry/article/details/null","line":13},{"title":"如果想要将数据映射到-1,1，则将公式换成：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":15},{"title":"_x_∗=_x_−_x__m__e__a__n__x__m__a__x_−_x__m__i__n_\n\nx_mean表示数据的均值。\n\ndef Normalization2(x):    return (float(i)-np.mean(x))/(max(x)-min(x)) for i in x\n\n这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":19},{"title":"log函数转换","target":"http://blog.csdn.net/pipisorry/article/details/null","line":27},{"title":"通过以10为底的log函数转换的方法同样可以实现归一下，具体方法如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":29},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","line":31},{"title":"看了下网上很多介绍都是x*=log10(x)，其实是有问题的，这个结果并非一定落到0,1区间上，应该还要除以log10(max)，max为样本数据最大值，并且所有的数据都要大于等于1。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":33},{"title":"atan函数转换","target":"http://blog.csdn.net/pipisorry/article/details/null","line":35},{"title":"用反正切函数也可以实现数据的归一化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":37},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","line":39},{"title":"使用这个方法需要注意的是如果想映射的区间为0,1，则数据都应该大于等于0，小于0的数据将被映射到-1,0区间上，而并非所有数据标准化的结果都映射到0,1区间上。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":41},{"title":"z-score 标准化(zero-mean normalization)","target":"http://blog.csdn.net/pipisorry/article/details/null","line":43},{"title":"最常见的标准化方法就是Z标准化，也是SPSS中最为常用的标准化方法，spss默认的标准化方法就是z-score标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":45},{"title":"也叫标准差标准化，这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":47},{"title":"经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":49},{"title":"_x_∗=_x_−_μ__σ_\n\n其中μ为所有样本数据的均值，σ为所有样本数据的标准差。\n\nz-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。\n\n**标准化的公式很简单，步骤如下**\n\n　　1.求出各变量（指标）的算术平均值（数学期望）xi和标准差si ；　　2.进行标准化处理：　　zij=（xij－xi）/si　　其中：zij为标准化后的变量值；xij为实际变量值。　　3.将逆指标前的正负号对调。　　标准化后的变量值围绕0上下波动，大于0说明高于平均水平，小于0说明低于平均水平。\n\ndef z_score(x):    x_mean=np.mean(x)    s2=sum((i-np.mean(x))*(i-np.mean(x)) for i in x)/len(x)    return (i-x_mean)/s2 for i in x\n\n**为什么z-score 标准化后的数据标准差为1?**","target":"http://blog.csdn.net/pipisorry/article/details/null","line":53},{"title":"x-μ只改变均值，标准差不变，所以均值变为0","target":"http://blog.csdn.net/pipisorry/article/details/null","line":69},{"title":"(x-μ)/σ只会使标准差除以σ倍，所以标准差变为1","target":"http://blog.csdn.net/pipisorry/article/details/null","line":71},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","line":73},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":75},{"title":"Decimal scaling小数定标标准化","target":"http://blog.csdn.net/pipisorry/article/details/null","line":77},{"title":"这种方法通过移动数据的小数点位置来进行标准化。小数点移动多少位取决于属性A的取值中的最大绝对值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":79},{"title":"将属性A的原始值x使用decimal scaling标准化到x'的计算方法是：x'=x/(10^j)其中，j是满足条件的最小整数。例如 假定A的值由-986到917，A的最大绝对值为986，为使用小数定标标准化，我们用每个值除以1000（即，j=3），这样，-986被规范化为-0.986。注意，标准化会对原始数据做出改变，因此需要保存所使用的标准化方法的参数，以便对后续的数据进行统一的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":81},{"title":"对数Logistic模式","target":"http://blog.csdn.net/pipisorry/article/details/null","line":83},{"title":"新数据=1/（1+e^(-原数据)）","target":"http://blog.csdn.net/pipisorry/article/details/null","line":85},{"title":"模糊量化模式","target":"http://blog.csdn.net/pipisorry/article/details/null","line":87},{"title":"新数据=1/2+1/2sin派3.1415/（极大值-极小值）*（X-（极大值-极小值）/2） X为原数据","target":"http://blog.csdn.net/pipisorry/article/details/null","line":89},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":91}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[min-max标准化(Min-max normalization)](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.0397272,-0.0403676,0.01217264,-0.02285285,0.06167636,0.01072475,-0.02914313,-0.00812418,0.03076785,0.00572525,0.02877171,-0.04399525,0.07871292,0.0453622,0.05112772,0.01943106,0.03407497,0.03726265,-0.08425995,-0.00696592,0.13765153,-0.04805424,0.03933486,-0.0770767,0.10833705,-0.0046613,-0.03143982,-0.04486091,-0.01601987,-0.21498303,0.0219126,-0.00114139,0.05742356,-0.0259451,0.06537551,-0.06088497,-0.02739068,0.06805325,0.03160887,0.04896108,0.04425577,0.02629528,-0.01518395,0.01367058,0.00846933,-0.05668278,-0.04903457,-0.05586449,-0.02540718,-0.00214957,-0.01497904,-0.01208498,-0.00534758,0.01527181,0.0342774,0.02245927,0.06641833,0.02675063,0.05850143,0.04646742,0.00629306,0.08230983,-0.19039147,0.03704758,0.00438781,-0.01356202,-0.01133824,-0.02677635,0.03850776,0.07050228,0.00426779,-0.01692127,0.02297502,0.07190761,0.036504,-0.04514246,-0.00941542,0.01954303,-0.01580462,-0.01352774,-0.04270186,0.05519064,0.03627844,-0.02223126,0.065333,0.00895552,0.05630634,-0.05007486,0.0151567,-0.07027846,-0.037149,-0.01370845,0.00600593,0.00637648,-0.04513393,-0.04760952,0.05869469,0.02282069,-0.0436733,0.11748196,-0.01456591,0.03684152,-0.00409088,-0.02057762,0.05980454,0.05974979,-0.06166226,-0.01491999,-0.00966953,-0.00665665,-0.05192696,-0.01608711,0.04320182,-0.03405182,-0.00906239,-0.00545845,0.04655101,0.03503766,-0.02867168,0.0297516,0.01408662,0.01291077,0.03358403,-0.04264051,-0.02897687,-0.01133995,-0.00158814,0.03247107,0.03321036,0.0448663,0.07897692,0.02574088,-0.09523904,-0.02997355,0.06566731,-0.00354751,-0.01249451,-0.03613936,-0.00459049,-0.0097442,-0.02295447,-0.07091025,0.00226306,-0.05201653,-0.10848086,0.12458677,-0.09252728,-0.04220978,0.00199776,-0.05616786,-0.00327882,-0.00075533,-0.0420784,-0.05934316,0.06387277,-0.01490742,0.08690456,0.07168902,-0.0495323,-0.06556842,0.00684985,-0.05823641,-0.04698204,0.11961044,0.05380878,-0.0456209,-0.00550912,0.04750621,-0.00821707,-0.02380989,0.05639224,-0.02435744,-0.03915539,-0.0033728,0.03777407,-0.00329233,-0.0368638,-0.05689364,-0.0058369,0.09192406,0.03437288,-0.04298915,-0.05063643,0.05446902,0.01725969,-0.08489636,0.00925775,-0.03898897,-0.02461457,-0.00825747,-0.07372626,0.00660537,-0.03543472,-0.05621582,-0.03551716,0.00167618,0.02777097,-0.02849833,0.00915708,-0.05514675,0.08481424,0.04424763,0.02422939,-0.04100119,-0.01800371,0.01456716,0.051576,-0.08139149,-0.00457822,0.05966773,-0.02986396,-0.02276726,0.01390634,-0.01941769,-0.01854985,0.01732153,0.00287603,0.05745114,-0.00740908,0.06252982,-0.0069098,0.00349202,-0.09925999,-0.22250648,-0.02488449,0.03169233,-0.07405741,0.08017354,-0.03119295,0.01121021,0.00728579,0.04094098,0.09165606,0.0688058,-0.01062202,-0.02159231,0.02964581,0.00605381,-0.00058241,0.05274595,0.04478982,-0.01192178,0.03138338,0.00744843,0.00307933,-0.05032026,-0.03881337,-0.00830572,0.01749488,0.11183503,-0.00668759,0.0889188,0.00139188,0.01869088,0.00106328,0.01688476,-0.06723084,0.04781294,0.0071369,-0.04163384,-0.02882022,-0.04574406,-0.03582688,0.01525409,0.02302132,0.00405976,-0.09871569,-0.05244136,-0.01219696,-0.01404446,-0.00378295,-0.00796533,0.03713623,-0.04078902,-0.02158101,-0.03520535,0.05095877,0.01984391,-0.04324355,-0.06645719,-0.02921662,-0.04211176,0.02124345,-0.0190748,-0.05757025,0.04641495,-0.07360809,-0.05054059,-0.02382341,-0.0089622,0.03205304,0.03686651,0.01264469,-0.08933404,0.13954,0.03112642,0.027912,0.00878002,0.07655527,-0.02229927,0.02022758,0.02442366,0.01849069,0.08092685,-0.01106137,0.06896162,0.05757775,-0.00712634,0.02613644,0.02072422,-0.04822345,-0.00141065,-0.00022346,-0.0477519,0.00711004,-0.05033689,-0.02422675,0.06223458,-0.00477704,-0.26986441,-0.00362237,0.02177979,-0.01576742,0.04762785,0.05053271,0.01511477,0.01896398,-0.03746716,0.01611093,-0.054892,0.02272494,0.00154312,-0.01505715,-0.0580978,-0.03258465,0.060993,0.0491421,0.02233922,-0.01866668,-0.03186634,0.0341255,0.21940078,-0.00707321,-0.01242935,-0.00879485,-0.02981799,0.02350631,0.02926015,0.00332769,-0.05555936,-0.00941644,0.10341749,0.05201856,0.01415229,0.1007043,-0.0253839,-0.01839247,0.03794192,0.04610641,-0.0520066,-0.03227735,-0.03173066,-0.02778922,0.10311411,-0.01988827,-0.05390765,-0.05436005,0.00374676,-0.01258175,-0.03970993,0.05536928,0.0037731,-0.01102195,0.02407523,0.0380142,0.0563146,-0.02273499,-0.00796061,-0.07511358,0.03203262,0.00464818,0.01545944,0.04205539,0.02431331],"last_embed":{"hash":"29cc3cd6c3b79d8226fddb528b920d3a0e338e717845d751aca64eede2fd6eab","tokens":441}}},"text":null,"length":0,"last_read":{"hash":"29cc3cd6c3b79d8226fddb528b920d3a0e338e717845d751aca64eede2fd6eab","at":1757414183443},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[min-max标准化(Min-max normalization)](http://blog.csdn.net/pipisorry/article/details/null)","lines":[34,57],"size":1006,"outlinks":[{"title":"min-max标准化(Min-max normalization)","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"也叫离差标准化，是对原始数据的线性变换。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"使结果落到0,1区间，转换函数如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","line":7},{"title":"其中max为样本数据的最大值，min为样本数据的最小值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":9},{"title":"def Normalization(x):    return (float(i)-min(x))/float(max(x)-min(x)) for i in x","target":"http://blog.csdn.net/pipisorry/article/details/null","line":11},{"title":"如果想要将数据映射到-1,1，则将公式换成：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":13},{"title":"_x_∗=_x_−_x__m__e__a__n__x__m__a__x_−_x__m__i__n_\n\nx_mean表示数据的均值。\n\ndef Normalization2(x):    return (float(i)-np.mean(x))/(max(x)-min(x)) for i in x\n\n这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":17}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[min-max标准化(Min-max normalization)](http://blog.csdn.net/pipisorry/article/details/null)#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04072158,-0.04092282,0.01336795,-0.02138121,0.06197089,0.01268652,-0.0268666,-0.00719068,0.02875779,0.00486038,0.03053814,-0.04274045,0.08057255,0.04344529,0.05117797,0.01905093,0.03313266,0.03734575,-0.08022835,-0.00705623,0.13751665,-0.04948229,0.04150746,-0.0763704,0.10868706,-0.00505444,-0.02818107,-0.04243904,-0.01812279,-0.21446569,0.02146195,0.00059974,0.05647634,-0.02618209,0.06589907,-0.06040133,-0.02716572,0.06868302,0.03223797,0.04886033,0.04374821,0.0237882,-0.01605612,0.01270389,0.00719075,-0.05722257,-0.04971062,-0.05651534,-0.02664021,-0.00264584,-0.01483127,-0.01110883,-0.00541079,0.0137645,0.03544876,0.02170658,0.06623844,0.02862823,0.05757546,0.0494226,0.00661607,0.08443694,-0.19329573,0.03736208,0.00560706,-0.01310844,-0.01287614,-0.02738203,0.03575021,0.06927165,0.00399345,-0.01824009,0.02332846,0.06924696,0.03617711,-0.04793182,-0.01060094,0.01863864,-0.01323912,-0.01667624,-0.04071297,0.05685431,0.03633396,-0.02313498,0.0646278,0.00893012,0.05563524,-0.05155215,0.01342219,-0.07102366,-0.03615885,-0.01406575,0.00749554,0.00474618,-0.04466759,-0.04647109,0.05968973,0.0236193,-0.04415194,0.11723357,-0.01614782,0.03502037,-0.0053305,-0.01947641,0.05919525,0.06027906,-0.06002486,-0.01710184,-0.00786332,-0.00528595,-0.05140814,-0.0158896,0.04088931,-0.0331034,-0.01076185,-0.0046671,0.04618389,0.03430004,-0.0307322,0.0298501,0.01193314,0.01349781,0.03614165,-0.04342884,-0.02901101,-0.01154725,-0.00003305,0.03339246,0.03175194,0.04545535,0.0806058,0.02570147,-0.09313507,-0.03084017,0.06574427,-0.00140916,-0.01233457,-0.03792318,-0.00465053,-0.01003395,-0.02385404,-0.0720586,-0.00048763,-0.05339821,-0.1112035,0.12427145,-0.08996163,-0.04269975,0.00433288,-0.05688823,-0.00557614,-0.00076387,-0.04202914,-0.05949455,0.06356667,-0.01307342,0.08571865,0.07265011,-0.04829981,-0.06502001,0.006132,-0.05758395,-0.04892058,0.12100657,0.0536599,-0.04541139,-0.00610466,0.04765092,-0.00567297,-0.02268508,0.05704842,-0.02308417,-0.03967078,-0.00241745,0.03460536,-0.00618742,-0.03744807,-0.05469115,-0.00711321,0.09089731,0.03620773,-0.04503215,-0.04876112,0.0553628,0.01672803,-0.08550276,0.00798925,-0.03818175,-0.02492522,-0.00731938,-0.07378928,0.00879383,-0.03425235,-0.05646117,-0.03472638,0.00222144,0.02641741,-0.02774953,0.00742793,-0.05719627,0.08536376,0.04230442,0.02602809,-0.04002915,-0.02095694,0.01450729,0.05020736,-0.08288117,-0.00451747,0.05763132,-0.03009346,-0.02070623,0.01447495,-0.02053085,-0.01791312,0.01708072,0.00221936,0.05964735,-0.00616593,0.06342042,-0.00841385,0.00513361,-0.09633139,-0.22113355,-0.02523262,0.03127256,-0.07419495,0.0801378,-0.03251444,0.01232246,0.0054811,0.03976114,0.0931372,0.07008453,-0.01105801,-0.02087888,0.02655099,0.00991569,0.00061699,0.05284857,0.04525351,-0.01158507,0.03129242,0.00805058,0.00118748,-0.04897773,-0.03827338,-0.00860662,0.01911469,0.11096656,-0.00843615,0.08919377,0.00379021,0.0172552,0.00324588,0.01586337,-0.06782855,0.04572232,0.0080251,-0.0379169,-0.02915613,-0.04237756,-0.03881541,0.01248537,0.02405945,0.00436605,-0.09981273,-0.04991004,-0.01367612,-0.01503548,-0.00592997,-0.00839178,0.03689484,-0.03889681,-0.01994612,-0.03661312,0.05274985,0.01684733,-0.04292666,-0.06549872,-0.02794548,-0.0431526,0.02108123,-0.01847887,-0.05791701,0.04696726,-0.07281616,-0.05223928,-0.0262926,-0.00968631,0.03187488,0.03825021,0.0128839,-0.08904162,0.1384248,0.03121554,0.02791394,0.00669215,0.07551327,-0.02416956,0.02003396,0.02428721,0.01935037,0.08269268,-0.01146597,0.07005344,0.06029027,-0.00639204,0.02786148,0.02160415,-0.04997445,0.00103657,-0.00083206,-0.04666555,0.00789008,-0.05152304,-0.02399479,0.06269417,-0.00295517,-0.2703197,-0.00184901,0.02080473,-0.01452397,0.045464,0.04951763,0.01591639,0.01898693,-0.03590728,0.01592287,-0.05328481,0.02063337,0.00256737,-0.01566737,-0.06090296,-0.03391168,0.06278197,0.04909161,0.02155164,-0.0179643,-0.03249392,0.03581876,0.21737911,-0.00855035,-0.00932006,-0.01097173,-0.03149112,0.0241056,0.02580987,0.00410997,-0.05548085,-0.0088242,0.10438297,0.0523277,0.01427032,0.10089488,-0.02444173,-0.01993679,0.03718188,0.04641883,-0.05246979,-0.03286288,-0.03125919,-0.02777326,0.10293742,-0.0193269,-0.05496506,-0.05594505,0.00187251,-0.01277315,-0.03833549,0.05429966,0.00514043,-0.01140532,0.02158814,0.037206,0.05750826,-0.0223144,-0.00722436,-0.07512613,0.03436656,0.00946505,0.01487874,0.04409378,0.02510655],"last_embed":{"hash":"7b478f741df6a3a8d920bad6c6adf4028a5a30d3910f5267e121481b45fef17c","tokens":441}}},"text":null,"length":0,"last_read":{"hash":"7b478f741df6a3a8d920bad6c6adf4028a5a30d3910f5267e121481b45fef17c","at":1757414183488},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[min-max标准化(Min-max normalization)](http://blog.csdn.net/pipisorry/article/details/null)#{1}","lines":[36,57],"size":913,"outlinks":[{"title":"也叫离差标准化，是对原始数据的线性变换。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"使结果落到0,1区间，转换函数如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.4.png","line":5},{"title":"其中max为样本数据的最大值，min为样本数据的最小值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":7},{"title":"def Normalization(x):    return (float(i)-min(x))/float(max(x)-min(x)) for i in x","target":"http://blog.csdn.net/pipisorry/article/details/null","line":9},{"title":"如果想要将数据映射到-1,1，则将公式换成：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":11},{"title":"_x_∗=_x_−_x__m__e__a__n__x__m__a__x_−_x__m__i__n_\n\nx_mean表示数据的均值。\n\ndef Normalization2(x):    return (float(i)-np.mean(x))/(max(x)-min(x)) for i in x\n\n这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":15}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null>[log函数转换](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04891833,-0.02611611,0.00683311,-0.03603444,0.05264125,-0.00638597,-0.03530581,-0.03132311,0.02698594,0.00924513,0.00685693,-0.02767977,0.05785969,0.05003032,0.0638299,0.00251305,0.0270006,0.01170297,-0.0629723,-0.00608014,0.15168579,-0.05694678,0.04842538,-0.08902157,0.09799989,0.00026017,-0.0282218,-0.02776448,-0.02181635,-0.2099684,0.00881099,0.0003815,0.06281371,-0.01804568,0.07298023,-0.0644667,-0.02165988,0.07419978,0.02735326,0.04097293,0.05425567,0.02033596,-0.02257893,0.01775626,-0.00538243,-0.06032979,-0.04672598,-0.04986423,-0.0103131,-0.0377576,-0.02004438,-0.01933826,-0.0110486,-0.00174965,0.00824386,0.01103801,0.07099938,0.01628835,0.06222431,0.02968789,-0.00730097,0.0834057,-0.20316604,0.05583991,0.0251084,-0.01165697,-0.02457024,-0.01925395,0.05679287,0.06918898,-0.00809782,-0.02414072,0.02431211,0.07352789,0.02161058,-0.03586797,0.0047862,0.02364178,-0.02890977,-0.01046596,-0.04842528,0.0584341,0.03988146,-0.01354519,0.05599553,0.0274854,0.05942713,-0.04076153,0.00103696,-0.062829,-0.03042186,-0.00643219,-0.01177737,-0.00059432,-0.03649657,-0.04068454,0.05443336,0.03474702,-0.05153546,0.11946776,-0.02944437,0.0447832,-0.00210783,-0.01520521,0.04765967,0.05514736,-0.04840537,-0.02224557,-0.03004272,-0.0025494,-0.0205274,-0.01107504,0.04633209,-0.01561312,-0.01490773,0.00472266,0.05226297,0.05643896,-0.05022611,0.02751078,0.02081734,-0.00375428,0.03140556,-0.04713339,0.00204331,-0.02485585,-0.01253638,0.0246958,0.03537262,0.03066521,0.07773804,0.03094775,-0.084964,-0.01419532,0.04260827,-0.01199958,-0.02275251,-0.02352185,-0.01236265,-0.00332179,-0.0275665,-0.05823878,0.00100531,-0.06176009,-0.1040727,0.10387783,-0.07300472,-0.02685097,-0.00628984,-0.05789534,-0.00337431,0.01252566,-0.01884956,-0.07277305,0.07100643,-0.01480626,0.07693306,0.09721352,-0.056376,-0.05779039,0.01253203,-0.07479941,-0.03113291,0.12322869,0.05120991,-0.05192906,-0.01880975,0.04456974,-0.0153534,-0.0467373,0.05995642,-0.03032756,-0.02403332,-0.01833819,0.01889543,-0.01609613,-0.01719673,-0.05623211,-0.00352978,0.07703588,0.04316397,-0.0479922,-0.05493164,0.06778799,0.01979853,-0.09467622,-0.01406324,-0.04577937,-0.01956748,-0.00147355,-0.06816664,0.0085803,-0.02276986,-0.05377348,-0.04803682,0.01791551,0.02899265,-0.01781326,0.00176618,-0.08048992,0.08641417,0.05415222,0.03423838,-0.02878545,-0.02881216,0.01665237,0.03156141,-0.06521036,0.00610655,0.0424771,-0.03339045,-0.004121,0.02337811,-0.01616959,-0.02306964,0.0056226,-0.01473979,0.06881344,0.00435637,0.06388019,-0.00868354,0.0068056,-0.10382761,-0.23485939,-0.04498787,0.03931925,-0.0658808,0.07839321,-0.02545354,-0.01316705,0.00612824,0.03239726,0.07761959,0.07892216,0.01900287,-0.0186163,0.02256827,0.0121271,0.02245398,0.06560137,0.03920169,-0.01657302,0.03370522,-0.0020772,-0.00336531,-0.03511517,-0.03587272,-0.02044254,0.01768851,0.10356663,-0.01164072,0.0883214,0.00042512,0.00863225,0.02176247,0.03358949,-0.09851295,0.05245563,0.0136784,-0.05218127,-0.01636994,-0.00891549,-0.05526684,-0.00138549,0.01769849,-0.00496262,-0.09642496,-0.04994548,0.00158269,-0.01785415,-0.00554125,-0.00776893,0.01810225,-0.0090687,-0.01254996,-0.04537354,0.07114583,0.00700443,-0.04256819,-0.06527113,-0.02684956,-0.04974806,0.01192295,-0.02503177,-0.05773596,0.0308521,-0.05035506,-0.04743689,-0.01532806,-0.02162915,0.01506618,0.03658365,0.01179459,-0.08605266,0.12888639,0.03477256,0.02335581,0.01319551,0.06833667,-0.04045727,0.00799949,0.02333709,0.02270408,0.10157687,-0.0155474,0.07306572,0.06265739,0.00956037,0.03393341,0.01492802,-0.04495575,0.02838412,-0.00569181,-0.04419165,-0.00992495,-0.04361563,-0.04923617,0.07109056,0.00807353,-0.27114943,0.0082908,0.00280325,-0.01674019,0.05115635,0.0618435,0.01883367,0.01047646,-0.00288487,0.01708319,-0.02578311,0.02561791,-0.00543305,-0.0178601,-0.06423124,-0.01979705,0.06117116,0.04810733,0.04882731,-0.00497424,-0.0379991,0.0326994,0.20740607,0.01097893,0.01169123,-0.00985091,-0.02601909,0.02655035,0.0334429,-0.00253677,-0.0337771,-0.04196973,0.11507476,0.05845981,0.04747052,0.08376575,0.00749153,-0.02050483,0.04348356,0.01139862,-0.06652144,-0.01650596,-0.03047154,-0.01951488,0.09763726,-0.0163687,-0.04763125,-0.05866156,0.01353064,-0.00966968,-0.02465876,0.05470785,0.0065813,0.00782056,0.00416534,0.03232246,0.04638952,-0.03876328,-0.02825929,-0.08095641,0.03537669,0.02716443,0.01888387,0.04727735,0.01367884],"last_embed":{"hash":"fa92171d8a89cb0827d0de9e63c7f9e44766cb8cb66cc2a1b0adc8cbec84612b","tokens":370}}},"text":null,"length":0,"last_read":{"hash":"fa92171d8a89cb0827d0de9e63c7f9e44766cb8cb66cc2a1b0adc8cbec84612b","at":1757414183536},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null>[log函数转换](http://blog.csdn.net/pipisorry/article/details/null)","lines":[58,65],"size":516,"outlinks":[{"title":"log函数转换","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"通过以10为底的log函数转换的方法同样可以实现归一下，具体方法如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","line":5},{"title":"看了下网上很多介绍都是x*=log10(x)，其实是有问题的，这个结果并非一定落到0,1区间上，应该还要除以log10(max)，max为样本数据最大值，并且所有的数据都要大于等于1。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":7}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null>[log函数转换](http://blog.csdn.net/pipisorry/article/details/null)#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.05030237,-0.02561225,0.00660592,-0.03498336,0.05419473,-0.00682737,-0.03227203,-0.03239916,0.02755226,0.01051742,0.01028304,-0.02572845,0.05689839,0.05219249,0.06388875,0.00624067,0.02864054,0.0117123,-0.05696957,-0.00521117,0.15371875,-0.05653489,0.04920214,-0.08870365,0.09684812,0.00155966,-0.02512574,-0.02222186,-0.02305391,-0.20596807,0.00613408,-0.00024461,0.0629107,-0.01772393,0.07315994,-0.06613033,-0.01777436,0.07486713,0.02755211,0.04044378,0.0552775,0.02312647,-0.02315541,0.01524597,-0.00799848,-0.05978655,-0.04416205,-0.0483592,-0.01273958,-0.03637361,-0.02052478,-0.01811918,-0.01264243,-0.00118735,0.00814505,0.01042645,0.06691323,0.0185517,0.05921079,0.02691127,-0.00799874,0.08202033,-0.20364113,0.05728286,0.023427,-0.00986619,-0.02355299,-0.01838223,0.05608443,0.06898221,-0.0070683,-0.02433223,0.0234764,0.07423279,0.02285479,-0.03527699,0.00230773,0.02690777,-0.02822899,-0.01159308,-0.04906173,0.05692732,0.03680309,-0.01401954,0.05356789,0.02689438,0.0610967,-0.04279974,0.00019978,-0.06072935,-0.02749443,-0.01006645,-0.0098087,-0.00042082,-0.03905617,-0.04017025,0.05398827,0.03448442,-0.05144942,0.11766005,-0.02878252,0.0417645,-0.00426172,-0.01657729,0.04474373,0.0576943,-0.04737593,-0.02402014,-0.03048538,-0.00476447,-0.02155599,-0.00966708,0.04595562,-0.01325638,-0.01667766,0.00558594,0.05245673,0.05419047,-0.05477863,0.02554295,0.01943551,-0.00286091,0.03504414,-0.04952463,0.00156384,-0.02360744,-0.01156832,0.02626825,0.03562628,0.03188051,0.07784623,0.02846115,-0.08380537,-0.01640185,0.04440138,-0.0143499,-0.02260806,-0.02393218,-0.0151691,-0.00832808,-0.02567358,-0.06152564,0.00237488,-0.06414063,-0.1062208,0.10539528,-0.06927969,-0.02520517,-0.00708884,-0.06212382,-0.00355412,0.01255879,-0.01926004,-0.07310287,0.06979819,-0.01350877,0.07661965,0.09888411,-0.05575101,-0.06003168,0.0120108,-0.07575282,-0.03188383,0.12270629,0.05192689,-0.05273846,-0.01907186,0.04319382,-0.01707689,-0.04493229,0.05792521,-0.03236895,-0.0264528,-0.01613127,0.01606227,-0.01713745,-0.0139101,-0.0564485,-0.00438107,0.07918207,0.04388519,-0.04892519,-0.05600202,0.07073681,0.01985228,-0.09489068,-0.01519648,-0.04108899,-0.02053041,-0.0058577,-0.07380132,0.00955168,-0.02093261,-0.05445956,-0.04966368,0.01533675,0.02948846,-0.01633646,0.00234202,-0.08189614,0.09311726,0.05186366,0.03189308,-0.03023376,-0.02703006,0.01509076,0.03255143,-0.06384038,0.00713448,0.04233986,-0.03330112,-0.00276491,0.02401057,-0.01481481,-0.0217839,0.00476123,-0.01630245,0.06958887,0.00368588,0.06593262,-0.00731094,0.01025684,-0.10288154,-0.23362282,-0.04430526,0.04037977,-0.06737965,0.07763445,-0.02676124,-0.00971429,0.00585736,0.03203572,0.07811942,0.07973409,0.02057407,-0.01916613,0.02170973,0.01476973,0.02112735,0.06561448,0.03909228,-0.01600523,0.03309036,-0.00076814,-0.00620689,-0.03117507,-0.03309432,-0.01970409,0.01676689,0.10255562,-0.01150214,0.08914258,-0.00127236,0.00759618,0.02315031,0.03334463,-0.09881595,0.05136694,0.01496065,-0.05028908,-0.01668132,-0.00911066,-0.05627131,-0.00483793,0.02044065,-0.00179279,-0.09715462,-0.04861322,0.00032903,-0.01731704,-0.0043823,-0.00936861,0.02040323,-0.00855181,-0.00959303,-0.0451186,0.07401549,0.00498545,-0.04180682,-0.06178876,-0.02522452,-0.04948849,0.01101783,-0.02436569,-0.05550191,0.0295015,-0.04893413,-0.04786887,-0.01714265,-0.0237007,0.01837514,0.03848688,0.01370297,-0.08623064,0.13021375,0.03314942,0.02138409,0.00958327,0.0661416,-0.04140662,0.00589919,0.02421503,0.02313567,0.10160945,-0.01799162,0.07340896,0.06086564,0.01138969,0.03528014,0.01440634,-0.04274106,0.02735797,-0.00472867,-0.04403029,-0.01099649,-0.04275258,-0.04839099,0.0719,0.00826603,-0.27173474,0.00782912,-0.00097757,-0.01612854,0.05265283,0.06272179,0.02135985,0.01080374,-0.00549239,0.01622671,-0.02581334,0.02494868,-0.00694831,-0.01681192,-0.06294583,-0.01744758,0.05833803,0.04949246,0.04979942,-0.00229027,-0.03963304,0.03416291,0.20404257,0.00948145,0.01742404,-0.00917723,-0.02379571,0.02742679,0.03299772,-0.00134065,-0.0343909,-0.04631886,0.1162798,0.05619394,0.04569648,0.0840458,0.00758427,-0.02154415,0.04392121,0.0080114,-0.06940785,-0.01848153,-0.02832104,-0.01536309,0.0993579,-0.01842577,-0.0504737,-0.06021373,0.01281634,-0.00875375,-0.02699547,0.05696455,0.0074381,0.0116493,0.00330015,0.02891464,0.04567974,-0.04022336,-0.03336641,-0.08254549,0.03407793,0.02807784,0.01660018,0.04625266,0.01408529],"last_embed":{"hash":"f46f255c50bebfcaa8fbc52137a2e7e2babaea57cc99c3cce1c3040fca2597d9","tokens":369}}},"text":null,"length":0,"last_read":{"hash":"f46f255c50bebfcaa8fbc52137a2e7e2babaea57cc99c3cce1c3040fca2597d9","at":1757414183575},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null>[log函数转换](http://blog.csdn.net/pipisorry/article/details/null)#{1}","lines":[60,65],"size":396,"outlinks":[{"title":"通过以10为底的log函数转换的方法同样可以实现归一下，具体方法如下：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.1.png","line":3},{"title":"看了下网上很多介绍都是x*=log10(x)，其实是有问题的，这个结果并非一定落到0,1区间上，应该还要除以log10(max)，max为样本数据最大值，并且所有的数据都要大于等于1。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[atan函数转换](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04513801,-0.02802424,0.00703075,-0.0129251,0.05382563,0.00041362,-0.0180492,-0.02774911,0.0180954,0.00200662,0.00512868,-0.04592578,0.08352148,0.05011559,0.0513898,0.00555171,0.03339194,0.01687697,-0.05387184,-0.0036881,0.14377317,-0.03577017,0.05930255,-0.06794285,0.09100363,-0.00179609,-0.03477531,-0.02850151,-0.02372871,-0.20757669,0.00638245,0.00181488,0.05832504,-0.01556873,0.07846741,-0.07034248,-0.02099882,0.07944793,0.02688386,0.03949273,0.05433182,-0.00123924,-0.02098571,0.01363602,0.01554418,-0.03504852,-0.02775345,-0.04124253,-0.00791331,-0.03817188,-0.01967755,-0.02833373,-0.01896296,0.01382569,0.00359482,0.01435211,0.08010571,0.0284758,0.05759475,0.04657928,0.00409133,0.08453147,-0.21474117,0.04552582,0.00958463,-0.00826933,-0.01802505,-0.02196311,0.04930407,0.06997322,-0.00611321,-0.02579338,0.02197854,0.08402131,0.02827183,-0.04969486,0.0105059,0.01894615,-0.02035894,-0.01048276,-0.0424781,0.06223679,0.04108921,-0.01581896,0.05823807,0.01941663,0.05812201,-0.03631364,-0.00268599,-0.0586934,-0.0378176,-0.018931,-0.00108037,0.0037614,-0.04429268,-0.04582882,0.04816253,0.02559626,-0.04030371,0.11334565,-0.04618137,0.04648334,-0.00088913,-0.00832662,0.06191287,0.05412775,-0.05012611,-0.03323183,-0.0149317,0.00723584,-0.02666114,-0.02335478,0.04825619,-0.03440011,-0.00096879,-0.00964169,0.05403816,0.05083982,-0.0328595,0.01768288,0.01607575,-0.00435193,0.04057125,-0.03125538,0.00209009,-0.03219583,-0.00409789,0.02576165,0.05853646,0.03286333,0.09078265,0.03255627,-0.09827816,-0.02196725,0.06215426,-0.00025474,-0.02069139,-0.0272462,-0.00002953,-0.02045979,-0.03134488,-0.06478333,-0.00372867,-0.04876509,-0.10605176,0.10338264,-0.07845133,-0.02148661,-0.00457905,-0.04749155,-0.01385524,0.01157274,-0.0268789,-0.05724863,0.05669669,-0.01462134,0.07216135,0.08609938,-0.06455623,-0.04629442,0.00984466,-0.07011507,-0.04784072,0.13207974,0.05164613,-0.04497401,-0.02751745,0.04210571,-0.00229872,-0.04864395,0.06116138,-0.02398492,-0.0215023,-0.00477233,0.02161788,-0.01795434,-0.02809408,-0.05655019,0.00237728,0.05956114,0.03602539,-0.03431034,-0.04563095,0.06143164,0.01448228,-0.0877912,-0.00526874,-0.0486322,-0.01932323,-0.00215829,-0.07028928,0.01596365,-0.04251383,-0.05350452,-0.03955564,0.009711,0.01736658,-0.04110395,0.01081043,-0.0587322,0.07232299,0.05594498,0.04418048,-0.02648235,-0.030783,0.01128406,0.03462892,-0.06410524,-0.000553,0.04441414,-0.0469075,-0.00896137,-0.00395052,-0.01937326,-0.02493667,-0.00521556,-0.00760885,0.05143357,0.00934214,0.06714151,-0.01880046,0.00981497,-0.08862934,-0.23141287,-0.02992123,0.02893276,-0.07818291,0.06344991,-0.00948119,-0.00734207,0.00269093,0.03429068,0.07898068,0.06905365,0.01022847,-0.02366671,0.03230929,0.00641574,0.01789569,0.06941467,0.03667514,-0.01239698,0.02685559,-0.01326677,0.01381608,-0.03849635,-0.04991055,-0.01467555,0.03856679,0.10904647,-0.00509819,0.09620176,-0.00715206,0.02407967,0.01870163,0.02388862,-0.09244284,0.05489316,0.00262192,-0.039504,-0.02087528,-0.02894886,-0.05356628,0.01286559,0.01389844,-0.00965363,-0.09551202,-0.05146045,0.00549978,-0.02008598,-0.00546208,0.00210285,0.01352799,-0.03155999,-0.01926734,-0.03002096,0.07882024,-0.00133412,-0.02365649,-0.07048417,-0.02241834,-0.04854974,0.02223179,-0.02852023,-0.04095218,0.04683641,-0.06763216,-0.04289798,-0.00245946,-0.02621341,0.01066539,0.03069611,0.01212008,-0.10871661,0.14068374,0.03817744,0.0281565,0.00525074,0.06832345,-0.05099481,0.00527135,0.01701906,0.01655839,0.09387287,-0.00380486,0.07713623,0.07750776,-0.00154073,0.02987372,0.01946267,-0.059371,0.03838551,-0.01752273,-0.04573523,-0.00763263,-0.06161302,-0.01943883,0.07182529,0.01158137,-0.26679143,0.01044933,0.01959958,-0.03123135,0.05853902,0.04813714,0.02377179,0.00163103,-0.00932115,0.02166384,-0.02569385,0.0239705,0.00654415,-0.03272422,-0.06540748,-0.02239745,0.0721159,0.0480492,0.03157167,-0.03022253,-0.028412,0.03271405,0.21985288,0.00184093,-0.01439962,-0.01339454,-0.00952315,0.02741494,0.02410302,0.02167529,-0.05209079,-0.03386336,0.09771151,0.06369148,0.03971741,0.09130847,-0.00942948,-0.01344203,0.03698516,0.02360203,-0.0712263,-0.02634036,-0.03566517,-0.01980875,0.0914415,-0.01528785,-0.04273625,-0.05483003,0.01186396,-0.01203375,-0.03138716,0.04741436,0.00503314,0.00323098,0.01539944,0.04058722,0.04307593,-0.02497916,-0.03325186,-0.0807616,0.03128015,0.00935946,0.00927971,0.05307237,0.01801893],"last_embed":{"hash":"72ab957777a82ca75934936c6ba52cad29c224da2733bf24d66f4e632049113b","tokens":332}}},"text":null,"length":0,"last_read":{"hash":"72ab957777a82ca75934936c6ba52cad29c224da2733bf24d66f4e632049113b","at":1757414183610},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[atan函数转换](http://blog.csdn.net/pipisorry/article/details/null)","lines":[66,73],"size":435,"outlinks":[{"title":"atan函数转换","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"用反正切函数也可以实现数据的归一化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","line":5},{"title":"使用这个方法需要注意的是如果想映射的区间为0,1，则数据都应该大于等于0，小于0的数据将被映射到-1,0区间上，而并非所有数据标准化的结果都映射到0,1区间上。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":7}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[atan函数转换](http://blog.csdn.net/pipisorry/article/details/null)#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04648217,-0.0277627,0.00749429,-0.01167308,0.05562254,0.00157429,-0.01753105,-0.02876313,0.01964931,0.00124081,0.00599866,-0.04443981,0.08585145,0.05083605,0.04934418,0.00644596,0.03497712,0.0143555,-0.04999119,-0.00349104,0.14702196,-0.03719712,0.060118,-0.06870477,0.0902291,-0.00028462,-0.03327791,-0.02577246,-0.02413261,-0.20598789,0.00447846,0.00273368,0.05676364,-0.01629415,0.07824592,-0.07071996,-0.01675379,0.08140776,0.02771528,0.03867212,0.05432993,0.00139532,-0.02092622,0.01419835,0.01509117,-0.0349933,-0.0280222,-0.04095842,-0.008064,-0.03797534,-0.01893329,-0.02700992,-0.01877446,0.0152732,0.00296223,0.01479182,0.07801471,0.03075198,0.05708055,0.04590212,0.0044669,0.08019093,-0.21648265,0.04475157,0.01027112,-0.00710195,-0.01802399,-0.02287311,0.05065934,0.07117864,-0.00460038,-0.0248247,0.02321993,0.0830066,0.02981688,-0.04868405,0.0098967,0.01937413,-0.0171517,-0.01076403,-0.04179838,0.06129226,0.03976028,-0.01617822,0.05710324,0.01918373,0.05753262,-0.03781616,-0.00264937,-0.05793163,-0.03626546,-0.01940129,-0.00077152,0.00397759,-0.04467024,-0.04453695,0.0467093,0.02361423,-0.04112405,0.11199023,-0.04528392,0.04602927,-0.00154423,-0.00819241,0.06060309,0.05412715,-0.05046153,-0.03395381,-0.01295448,0.00729832,-0.02696598,-0.02476998,0.04867764,-0.03141351,-0.00180256,-0.00796312,0.05444854,0.05060861,-0.03571663,0.01660689,0.01377752,-0.00467342,0.04283596,-0.03104179,0.00185328,-0.03219029,-0.00592475,0.02814159,0.05850057,0.0336824,0.09243243,0.02968505,-0.09916469,-0.02369585,0.06230184,-0.00309785,-0.02010848,-0.02683949,-0.00092525,-0.0221019,-0.03289041,-0.06613637,-0.00255807,-0.04949409,-0.10774525,0.10499707,-0.07457685,-0.02125865,-0.00361359,-0.04760973,-0.01491735,0.0108627,-0.02768487,-0.05846032,0.05391692,-0.01173712,0.07138135,0.08767292,-0.06278402,-0.0462101,0.00635974,-0.06885792,-0.04989909,0.13078688,0.052416,-0.0431266,-0.02764324,0.04124029,-0.00248145,-0.04913253,0.06117898,-0.02590344,-0.02234727,-0.00544509,0.0216514,-0.01849887,-0.02578684,-0.0551498,0.00196083,0.0617964,0.0369331,-0.03436116,-0.04453187,0.06420353,0.01440232,-0.08726399,-0.00766307,-0.04605028,-0.01934532,-0.0055989,-0.0741581,0.01681966,-0.04330013,-0.05353447,-0.04132384,0.00566708,0.01782561,-0.04123341,0.00859457,-0.0560711,0.0739399,0.05474266,0.04324612,-0.02662172,-0.03098686,0.00846767,0.03356476,-0.06356675,0.00044604,0.0437158,-0.04689051,-0.00817647,-0.00447466,-0.01972223,-0.02426334,-0.00683092,-0.00738582,0.04934343,0.0071018,0.0687602,-0.02094022,0.00976052,-0.08860382,-0.23196384,-0.03067323,0.02920413,-0.07940242,0.0632646,-0.01023489,-0.00490231,0.00253575,0.03287501,0.07814658,0.06787873,0.01172229,-0.02339341,0.03294651,0.0061829,0.01799389,0.06942677,0.03536601,-0.01104519,0.02764233,-0.01235095,0.01240704,-0.03757198,-0.04943804,-0.01498853,0.03761076,0.10838638,-0.00647305,0.0971083,-0.00470545,0.02277667,0.02064812,0.02398814,-0.09320264,0.05201584,0.00444242,-0.03853834,-0.02296218,-0.02946042,-0.05407247,0.00996495,0.01699954,-0.01042176,-0.09600126,-0.05113463,0.00476991,-0.01917775,-0.00497688,-0.00090815,0.01399447,-0.03037883,-0.01830384,-0.02894477,0.07842865,-0.00206781,-0.02342959,-0.06935334,-0.02054618,-0.04908188,0.02302849,-0.02608636,-0.0394914,0.04663257,-0.06748071,-0.04263328,-0.00362559,-0.02624555,0.01085093,0.03063457,0.01421429,-0.1081063,0.1418927,0.03833839,0.02631998,0.00305639,0.06760882,-0.05119307,0.00445423,0.01697129,0.01700004,0.09105805,-0.00396513,0.07526521,0.07824962,-0.00127891,0.03059233,0.0189264,-0.05877985,0.03899995,-0.01664443,-0.04614865,-0.00890695,-0.06087719,-0.01839375,0.07397294,0.01368129,-0.26686266,0.01041642,0.01897181,-0.03073134,0.06068823,0.04829579,0.02708424,-0.0001938,-0.00996042,0.02044994,-0.0244197,0.02380615,0.00681556,-0.03076415,-0.06403708,-0.02307984,0.07400333,0.04897583,0.03014353,-0.03110112,-0.03005674,0.03425824,0.21804686,0.00262273,-0.0144766,-0.0136815,-0.00674408,0.02654395,0.02503621,0.02282849,-0.05241913,-0.03609749,0.0996213,0.06224987,0.03719829,0.09144016,-0.01107938,-0.01335184,0.0362399,0.02138935,-0.07168429,-0.02754302,-0.03204192,-0.02014122,0.09423494,-0.01580763,-0.04373732,-0.05673893,0.01178164,-0.01229434,-0.03217022,0.04939191,0.00611933,0.00348291,0.01414089,0.04019774,0.04251212,-0.02573692,-0.03639397,-0.08215109,0.03360409,0.01156334,0.00713686,0.05442206,0.01915786],"last_embed":{"hash":"997749ba213e4d9e746a374918ce13e628d6394b8e03b99b94faa3a9e611f69b","tokens":331}}},"text":null,"length":0,"last_read":{"hash":"997749ba213e4d9e746a374918ce13e628d6394b8e03b99b94faa3a9e611f69b","at":1757414183645},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[atan函数转换](http://blog.csdn.net/pipisorry/article/details/null)#{1}","lines":[68,73],"size":367,"outlinks":[{"title":"用反正切函数也可以实现数据的归一化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.2.png","line":3},{"title":"使用这个方法需要注意的是如果想映射的区间为0,1，则数据都应该大于等于0，小于0的数据将被映射到-1,0区间上，而并非所有数据标准化的结果都映射到0,1区间上。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.02683074,-0.01067429,-0.00535278,-0.02195041,0.03574144,0.00206264,-0.01191177,0.00227008,0.02854001,0.01683128,0.02942107,-0.0327974,0.05949106,0.05261718,0.01029102,-0.00589429,0.05523823,0.03498648,-0.04361924,0.03499375,0.12579642,-0.04170823,0.07134067,-0.05796096,0.10913332,0.02394433,-0.03216388,-0.07036432,-0.03382216,-0.22613692,0.01324879,-0.01596174,0.06581708,-0.00431104,0.05375719,-0.10154374,-0.01926404,0.08220036,0.02646985,0.02651045,0.05786533,-0.00655054,-0.00154032,-0.00577486,0.00738214,-0.0596079,-0.0406686,-0.0520402,-0.02308467,-0.00948186,-0.0167175,-0.04510938,-0.0129749,0.03048821,0.01125184,0.00897755,0.07632531,0.02777307,0.06129979,0.05288776,-0.01617667,0.06295983,-0.23179258,0.01907113,0.03022044,-0.0273098,-0.03805192,-0.03484211,0.00288419,0.06146736,-0.00095872,-0.03590727,0.02801927,0.07974845,-0.00850712,-0.02044855,0.03044725,-0.00695883,-0.01592509,0.00194509,-0.01565365,0.04022551,0.04301286,-0.02762321,0.06256117,0.02343076,0.03291244,-0.03229954,0.00404362,-0.04774144,-0.02638217,-0.00471768,-0.01873286,-0.00602882,-0.03474715,-0.05476274,0.03900529,0.00344635,-0.04834115,0.10352281,-0.0238138,0.04712448,-0.00589296,-0.00500178,0.06872047,0.04818599,-0.01476582,-0.05127503,0.00380113,-0.01262228,-0.01416975,-0.04052807,0.05321337,-0.06173942,-0.00297299,0.01251378,0.04595147,0.06246957,0.01477635,0.009371,0.0153766,0.00587846,0.03921227,-0.04840342,0.04053092,-0.01054276,-0.00948155,0.04482234,0.02033916,0.00516447,0.06820075,0.01836795,-0.1126204,0.00269782,0.02975218,0.01950351,-0.0214275,0.0109389,0.00642649,-0.04130979,-0.03503985,-0.05677258,0.00370875,-0.05235885,-0.11686482,0.09510697,-0.08507032,0.00017906,-0.00261641,-0.07254286,-0.01308365,0.02275601,-0.02854137,-0.07618514,0.05321038,0.00269447,0.04795547,0.06364268,-0.04444667,-0.0373931,-0.01019721,-0.08015582,-0.04212837,0.10624362,0.08419415,-0.01796973,0.02087924,0.04834216,-0.00894954,-0.04892604,0.05583275,-0.03377603,-0.04745023,0.01158174,0.01952177,-0.04423702,0.00019328,-0.0548599,0.01364373,0.06688429,0.03888387,-0.01758123,-0.05151476,0.06482843,0.0177625,-0.08143219,-0.0243178,-0.06564974,-0.01313229,-0.0140945,-0.07323951,-0.00039443,-0.02580146,-0.03536639,-0.03423884,0.00534492,0.01304597,-0.03387507,-0.00023104,-0.08325278,0.06816076,0.03454234,0.0360341,-0.04150378,-0.01314505,-0.00629152,0.04317044,-0.06886832,-0.01639929,0.06363168,-0.04702407,-0.02143663,0.00239695,-0.00313015,-0.00501334,0.00413159,-0.01204249,0.04157019,-0.01678033,0.06400398,-0.02073054,0.02635805,-0.11170578,-0.22358419,-0.03679699,0.04870414,-0.07527599,0.0646536,0.01402407,-0.02910961,-0.00167311,0.05632297,0.1239489,0.09142717,0.02405254,-0.00614119,0.04021311,0.02099418,0.01485657,0.03137746,0.0178915,-0.01880213,0.02477532,-0.01571802,0.01526656,-0.01080966,-0.03601893,-0.01690521,0.01722197,0.10521027,-0.01591963,0.0862723,0.00890751,0.02667501,0.03722121,0.04722137,-0.08004682,0.05702021,0.01516819,-0.02023372,-0.01334502,-0.0417368,-0.06318693,0.00153695,0.00409697,0.01192401,-0.08223299,-0.04886538,0.00697482,-0.01344078,-0.00096654,-0.01798652,0.05477904,-0.0257555,-0.02561358,-0.01834991,0.04831294,0.00983282,-0.0289029,-0.08773019,-0.01989932,-0.04039002,-0.01302458,-0.03490303,-0.03455339,0.05008209,-0.08160185,-0.04152549,-0.00974701,-0.01127846,0.01902864,0.02949726,0.01859149,-0.09561835,0.12979919,0.0134408,0.01038431,0.03814003,0.05762171,-0.05995174,-0.0188053,0.01105004,0.02191592,0.09654675,0.02437568,0.03304499,0.0623524,0.03112833,0.02250307,0.02875727,-0.0354569,0.04035725,-0.01780188,-0.0317749,-0.00199373,-0.05488137,-0.00215809,0.07222717,-0.0125619,-0.2665599,0.00659117,-0.02137777,-0.05353869,0.07002993,0.01588197,0.03395189,-0.01686317,-0.0284461,0.04891515,-0.02194985,0.00686411,0.0095867,-0.04890745,-0.06057639,-0.02076586,0.06818061,0.04652823,0.0546222,-0.03177846,-0.00631435,0.05064353,0.2254492,0.02092951,0.01410297,-0.03855328,-0.0021683,0.00053637,0.01761019,0.02215055,-0.03488675,0.00111706,0.10151594,0.06686472,0.01993192,0.10228736,-0.00352677,-0.00564115,0.02631301,0.00774226,-0.03923797,-0.03457674,-0.05514329,0.0072839,0.09813534,-0.01585483,-0.03735658,-0.06578821,0.02166744,0.00004321,-0.03499221,0.0437731,0.02045278,-0.01342075,-0.00970138,0.03852856,0.02827212,-0.03049992,-0.02250952,-0.05036915,-0.00664232,0.02327705,0.01568054,0.05775409,0.01986224],"last_embed":{"hash":"2043d076873394a56783fee79251874589708f6e651188065461289bb5672fc6","tokens":477}}},"text":null,"length":0,"last_read":{"hash":"2043d076873394a56783fee79251874589708f6e651188065461289bb5672fc6","at":1757414183678},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)","lines":[74,107],"size":1432,"outlinks":[{"title":"z-score 标准化(zero-mean normalization)","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"最常见的标准化方法就是Z标准化，也是SPSS中最为常用的标准化方法，spss默认的标准化方法就是z-score标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"也叫标准差标准化，这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5},{"title":"经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":7},{"title":"_x_∗=_x_−_μ__σ_\n\n其中μ为所有样本数据的均值，σ为所有样本数据的标准差。\n\nz-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。\n\n**标准化的公式很简单，步骤如下**\n\n　　1.求出各变量（指标）的算术平均值（数学期望）xi和标准差si ；　　2.进行标准化处理：　　zij=（xij－xi）/si　　其中：zij为标准化后的变量值；xij为实际变量值。　　3.将逆指标前的正负号对调。　　标准化后的变量值围绕0上下波动，大于0说明高于平均水平，小于0说明低于平均水平。\n\ndef z_score(x):    x_mean=np.mean(x)    s2=sum((i-np.mean(x))*(i-np.mean(x)) for i in x)/len(x)    return (i-x_mean)/s2 for i in x\n\n**为什么z-score 标准化后的数据标准差为1?**","target":"http://blog.csdn.net/pipisorry/article/details/null","line":11},{"title":"x-μ只改变均值，标准差不变，所以均值变为0","target":"http://blog.csdn.net/pipisorry/article/details/null","line":27},{"title":"(x-μ)/σ只会使标准差除以σ倍，所以标准差变为1","target":"http://blog.csdn.net/pipisorry/article/details/null","line":29},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","line":31},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":33}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.02705406,-0.00977691,-0.00778411,-0.01908889,0.03586458,0.00313046,-0.00694261,0.00253295,0.02288823,0.0204029,0.03224112,-0.03036295,0.06184639,0.05380524,0.00871069,-0.00513812,0.05719232,0.03442777,-0.04210035,0.03839936,0.12647194,-0.0425362,0.075684,-0.05856953,0.11298419,0.02268413,-0.03302448,-0.06304362,-0.03535555,-0.22549884,0.01186414,-0.01592945,0.06212225,-0.00374613,0.05839795,-0.10402341,-0.01875014,0.08613733,0.02697773,0.02824062,0.05962913,-0.00515297,-0.00768007,-0.00804973,0.0070848,-0.05993363,-0.04031812,-0.04679229,-0.02351477,-0.01035428,-0.01550697,-0.04428738,-0.01372694,0.02929176,0.01322499,0.0109095,0.07289089,0.02966623,0.06065434,0.05152476,-0.017404,0.06003482,-0.23173289,0.02189069,0.02968015,-0.02304216,-0.03735295,-0.03393111,0.00377699,0.06079328,0.00374869,-0.03697312,0.02610434,0.07917988,-0.01371598,-0.02376714,0.02938861,-0.00266775,-0.0128407,0.00254315,-0.01302102,0.03657869,0.04399325,-0.02785779,0.06120359,0.02206661,0.03715871,-0.03204113,0.00552234,-0.04783856,-0.02583391,-0.00404753,-0.0127201,-0.00689662,-0.03722376,-0.05208345,0.03851948,0.00369252,-0.04554873,0.10391869,-0.02245814,0.04375974,-0.00546299,-0.00514657,0.06952433,0.05461529,-0.01522339,-0.04819869,0.00579603,-0.00770412,-0.01476175,-0.04288742,0.05111275,-0.0573568,-0.0037831,0.01132379,0.04676566,0.0685305,0.01246697,0.0103934,0.01366148,0.00291183,0.04622245,-0.04377346,0.03737966,-0.00852491,-0.01055751,0.04836827,0.02305424,0.00511217,0.06907645,0.01882631,-0.11216521,0.00385815,0.03159025,0.01915491,-0.01757162,0.0121137,0.00158787,-0.04187941,-0.03114959,-0.05900368,0.00111506,-0.05041716,-0.1214866,0.09404346,-0.083132,0.00233495,-0.00310728,-0.07251849,-0.01745699,0.0222555,-0.02585069,-0.07638323,0.05229752,0.00511429,0.04979413,0.05984869,-0.04407701,-0.03765064,-0.01247384,-0.07766426,-0.04451594,0.10322869,0.08698808,-0.01845013,0.02115732,0.04570606,-0.01102976,-0.04990554,0.05835062,-0.03101286,-0.05129153,0.01036034,0.01658835,-0.04492456,-0.00063136,-0.0531275,0.01540352,0.06668542,0.03645653,-0.01290634,-0.0517336,0.06842431,0.01704496,-0.07908659,-0.02493903,-0.06465731,-0.01361901,-0.01719869,-0.07518737,-0.0029059,-0.02382206,-0.03813569,-0.03629142,0.00231483,0.00966215,-0.03589818,-0.00030066,-0.08666124,0.0680933,0.03286535,0.03404513,-0.0389761,-0.01452345,-0.00676217,0.04155524,-0.06975361,-0.01417146,0.06391842,-0.05300933,-0.02134854,0.00121489,-0.00568304,-0.00578611,0.00530397,-0.01190996,0.03792761,-0.01697442,0.06222107,-0.02056524,0.02687163,-0.10990448,-0.22576462,-0.03227642,0.04747983,-0.07762229,0.07033014,0.01696118,-0.03189002,-0.0004871,0.05318751,0.12380221,0.0897586,0.02297406,-0.00421592,0.03862887,0.02257757,0.01503763,0.02813558,0.01775398,-0.01967782,0.0246648,-0.0191246,0.00861051,-0.01503069,-0.03598866,-0.01536684,0.01913276,0.10404987,-0.01475716,0.08823158,0.00622581,0.02810592,0.03574138,0.04827405,-0.07925203,0.0555711,0.01493366,-0.01591586,-0.0130513,-0.04130682,-0.06544464,-0.00128438,0.00421354,0.01661132,-0.08349843,-0.0439121,0.01152193,-0.0132117,0.00248442,-0.01760154,0.05374885,-0.02947511,-0.02205368,-0.01912938,0.05172938,0.00755384,-0.03102154,-0.08750535,-0.01741606,-0.03860198,-0.01287202,-0.03320946,-0.03058583,0.04870225,-0.08566026,-0.04365594,-0.01196451,-0.01165984,0.01750027,0.02951936,0.02116319,-0.09340695,0.12632369,0.00957554,0.01462588,0.03283983,0.05845727,-0.05970971,-0.02181181,0.01067796,0.02150277,0.09214883,0.01991666,0.03225325,0.06196837,0.03687632,0.0219116,0.02556233,-0.03779785,0.04336971,-0.01609441,-0.033871,-0.00776796,-0.05662876,-0.00488065,0.07583791,-0.01511358,-0.26611459,0.00846895,-0.02317264,-0.05300748,0.06856165,0.0203427,0.03684748,-0.01324571,-0.02987521,0.04617982,-0.0225946,0.0048739,0.00989181,-0.04872227,-0.06229519,-0.01834689,0.06855979,0.04732544,0.05293894,-0.03216659,-0.00374319,0.0507221,0.22611098,0.01725981,0.01658157,-0.03508009,0.0023093,-0.00043602,0.01671248,0.02574642,-0.03422055,0.00123693,0.09999993,0.06675267,0.0233197,0.09589378,-0.0025361,-0.00699038,0.02382116,0.0047228,-0.04306205,-0.03923278,-0.0501937,0.0084166,0.09922381,-0.0179201,-0.04092038,-0.06468787,0.02509521,-0.0010233,-0.03881591,0.04328015,0.0189274,-0.01225873,-0.00987032,0.03927353,0.02926117,-0.03276006,-0.02607889,-0.05013023,-0.00612444,0.01915419,0.01428339,0.05482477,0.01847459],"last_embed":{"hash":"8ff1188aaf648afd61efd9909c43fb76c278500d31fb6620f6d947047b42c395","tokens":444}}},"text":null,"length":0,"last_read":{"hash":"8ff1188aaf648afd61efd9909c43fb76c278500d31fb6620f6d947047b42c395","at":1757414183727},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#{1}","lines":[76,97],"size":866,"outlinks":[{"title":"最常见的标准化方法就是Z标准化，也是SPSS中最为常用的标准化方法，spss默认的标准化方法就是z-score标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"也叫标准差标准化，这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5},{"title":"_x_∗=_x_−_μ__σ_\n\n其中μ为所有样本数据的均值，σ为所有样本数据的标准差。\n\nz-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。\n\n**标准化的公式很简单，步骤如下**\n\n　　1.求出各变量（指标）的算术平均值（数学期望）xi和标准差si ；　　2.进行标准化处理：　　zij=（xij－xi）/si　　其中：zij为标准化后的变量值；xij为实际变量值。　　3.将逆指标前的正负号对调。　　标准化后的变量值围绕0上下波动，大于0说明高于平均水平，小于0说明低于平均水平。\n\ndef z_score(x):    x_mean=np.mean(x)    s2=sum((i-np.mean(x))*(i-np.mean(x)) for i in x)/len(x)    return (i-x_mean)/s2 for i in x\n\n**为什么z-score 标准化后的数据标准差为1?**","target":"http://blog.csdn.net/pipisorry/article/details/null","line":9}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null><http://blog.csdn.net/pipisorry/article/details/null>": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.0394372,-0.01889746,0.00015106,-0.02059269,0.04515021,0.01100909,-0.01604761,-0.00245651,0.00806607,0.01021322,0.03910783,-0.03671865,0.05854242,0.05889615,0.01590973,-0.00023027,0.0369488,0.0284783,-0.03941403,0.02464032,0.13221624,-0.03863216,0.08360405,-0.05359422,0.11952789,0.01964418,-0.03437389,-0.05320805,-0.03663964,-0.22592719,0.00070881,-0.00761918,0.0668436,-0.0120692,0.06782467,-0.09582473,-0.02555679,0.09215229,0.02517096,0.02641979,0.07878552,0.00247949,-0.01455194,0.00375488,0.0144923,-0.06276593,-0.04080315,-0.04820058,-0.02323518,-0.01116314,-0.01421115,-0.04005281,-0.01859264,0.02332054,0.01956264,0.01546624,0.08257238,0.03282449,0.06838629,0.05874551,-0.01257156,0.07203534,-0.22463438,0.02440481,0.02520228,-0.02374964,-0.04443115,-0.01751809,0.00950689,0.07181741,0.01541168,-0.03695564,0.03312722,0.08321641,-0.00782081,-0.03585236,0.03130084,0.0028746,-0.01762253,-0.00360841,-0.01776687,0.04781535,0.03277075,-0.0273251,0.06301071,0.0117143,0.04174826,-0.0363017,0.00934274,-0.05145228,-0.03447256,-0.00583638,-0.01221171,0.00610232,-0.04192913,-0.05127326,0.03389408,0.00457852,-0.04101076,0.10291198,-0.03202062,0.04028768,-0.00168151,-0.0114791,0.06984688,0.05432253,-0.01694941,-0.0450078,-0.00156665,-0.00463579,-0.01953666,-0.03304244,0.05212488,-0.06103703,0.01263497,0.01613586,0.0444019,0.06118818,0.00348873,0.01451224,0.01114904,-0.00396283,0.04118822,-0.03827133,0.0208424,-0.01952077,-0.01195768,0.03659011,0.03456078,0.01205405,0.07288206,0.01577214,-0.11785378,0.00081319,0.02768071,0.02031511,-0.0160531,-0.01090456,0.0017214,-0.03482249,-0.02835557,-0.05878611,-0.00755443,-0.04981171,-0.12055408,0.10314146,-0.07341121,-0.01126008,0.00379281,-0.05958688,-0.01048638,0.02294693,-0.03149771,-0.07064796,0.05718091,-0.00258498,0.04966587,0.05175868,-0.03924487,-0.04992439,0.0016077,-0.08195526,-0.04653792,0.11556647,0.07546901,-0.02053102,-0.00392386,0.04723063,-0.0013264,-0.04612944,0.05969889,-0.03183995,-0.03988167,0.01031139,0.02824121,-0.03833661,-0.00836988,-0.06061029,0.00981244,0.06768134,0.03742526,-0.0269525,-0.05706011,0.06002228,0.01688135,-0.06783928,-0.01385164,-0.06723173,-0.00757343,-0.00897279,-0.0729921,-0.00176751,-0.02400895,-0.04359702,-0.03220525,0.00440509,0.01477915,-0.05414228,-0.00176416,-0.09434274,0.06826201,0.03310431,0.03997937,-0.04139791,-0.02405021,-0.01694201,0.03737952,-0.07513916,-0.01667826,0.06627417,-0.05715152,-0.02670486,0.00318184,0.00550254,-0.00872771,0.00016264,-0.00329301,0.03347995,-0.00565007,0.05686006,-0.01687758,0.0309453,-0.10702292,-0.22300601,-0.02652036,0.03873197,-0.07335128,0.06815038,0.00310789,-0.01518381,0.00892937,0.03982302,0.10827054,0.08532131,0.00442522,-0.00445365,0.04461907,0.02573671,0.02431924,0.03513126,0.01842505,-0.0221706,0.02240172,-0.01272895,0.01201681,-0.04142633,-0.04348183,-0.00758494,0.01605322,0.10438363,0.00473343,0.09340022,0.00056206,0.03794201,0.03074763,0.04435477,-0.08113457,0.05642088,0.0020417,-0.02505364,-0.01409851,-0.04351507,-0.06476265,0.01440996,0.0039992,0.01087817,-0.08393764,-0.03713937,0.00787847,-0.02503543,-0.00166175,-0.00839503,0.04989193,-0.01924185,-0.02097072,-0.02540624,0.06341787,0.00124054,-0.03566289,-0.09779939,-0.00879218,-0.0433913,0.00809577,-0.02773285,-0.0383098,0.04778492,-0.08531735,-0.03728071,-0.00756182,-0.00817187,0.02045487,0.05116784,0.02277536,-0.08218554,0.11666745,0.00827728,0.01704672,0.03068866,0.05359432,-0.06330036,-0.02006087,0.02004652,0.01086762,0.07789131,0.00445376,0.03309732,0.06328625,0.02912766,0.02603874,0.02579065,-0.04926765,0.04756466,-0.01165803,-0.03667291,-0.00952785,-0.05289175,-0.00872094,0.07640599,-0.01858764,-0.26392391,0.0108263,0.00175188,-0.04908543,0.07664341,0.03671161,0.02518411,-0.01626553,-0.03488533,0.04162519,-0.03692105,0.00704312,0.00514356,-0.05123345,-0.05912482,-0.02170057,0.07151633,0.04787423,0.0492528,-0.03222731,-0.01509595,0.04927707,0.22359934,0.00659291,0.00225819,-0.02801519,-0.00320488,0.00021446,0.0232363,0.02332586,-0.04110316,0.0103691,0.10770631,0.07190932,0.02904745,0.08794648,0.01548687,-0.00910545,0.03331999,0.01425005,-0.05021102,-0.03353726,-0.04646203,-0.01120658,0.08600989,-0.01182939,-0.03897176,-0.05015486,0.01839285,-0.00564526,-0.03182694,0.05039231,0.01793961,-0.00616924,-0.00296276,0.0346139,0.0397434,-0.03912274,-0.0288681,-0.05542666,-0.00311609,0.02488087,0.01983494,0.04515118,0.0040685],"last_embed":{"hash":"a6b65d06a90f374375fa747228cd3294980bab88d6a5168fb3b94ce3b6444ecf","tokens":357}}},"text":null,"length":0,"last_read":{"hash":"a6b65d06a90f374375fa747228cd3294980bab88d6a5168fb3b94ce3b6444ecf","at":1757414183772},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null><http://blog.csdn.net/pipisorry/article/details/null>","lines":[98,107],"size":469,"outlinks":[{"title":"x-μ只改变均值，标准差不变，所以均值变为0","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"(x-μ)/σ只会使标准差除以σ倍，所以标准差变为1","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","line":7},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":9}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null><http://blog.csdn.net/pipisorry/article/details/null>#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04164665,-0.02053643,0.00006133,-0.02006638,0.04680157,0.01102181,-0.01781386,-0.00577523,0.00950562,0.00896664,0.03948048,-0.03644835,0.06064325,0.06121057,0.01770699,-0.00018039,0.03636815,0.02385085,-0.03794751,0.02477561,0.13471258,-0.03657896,0.08210149,-0.05387531,0.11618591,0.01889644,-0.03326096,-0.04804815,-0.03683614,-0.22270048,-0.00026644,-0.00812024,0.06445993,-0.01087923,0.06858891,-0.09412782,-0.02194279,0.09039591,0.02466783,0.02750539,0.07834015,0.00424813,-0.01452767,0.00455783,0.01256292,-0.06343705,-0.039174,-0.04677008,-0.02348535,-0.01008452,-0.01592647,-0.04072503,-0.01943566,0.02424271,0.01676993,0.01590068,0.08096746,0.03306631,0.06782471,0.06043409,-0.01166219,0.07037707,-0.22495206,0.02498912,0.02263325,-0.02216058,-0.04649116,-0.01939975,0.01038445,0.07121512,0.013239,-0.03605675,0.03300378,0.08471999,-0.00484577,-0.03629371,0.03152001,0.00174943,-0.0178431,-0.00382182,-0.01971964,0.04940866,0.03220522,-0.02712361,0.06299451,0.01310431,0.04395599,-0.03900426,0.00940477,-0.05165691,-0.03375666,-0.00560673,-0.01129447,0.00664085,-0.041697,-0.05046171,0.03277723,0.00312795,-0.03890818,0.10206299,-0.03412576,0.03909423,0.00162122,-0.01259292,0.06881026,0.05524318,-0.01830194,-0.04461306,-0.00251464,-0.00352717,-0.02065648,-0.03399637,0.04955659,-0.0588397,0.01225137,0.01584443,0.04358864,0.06019295,-0.00132225,0.01478758,0.01174185,-0.0059383,0.04320075,-0.03730332,0.02062945,-0.01946002,-0.0115875,0.03750046,0.03428013,0.01394322,0.07389886,0.01480874,-0.11908812,-0.00138597,0.02805975,0.01809422,-0.01651065,-0.01137999,-0.00030073,-0.0345038,-0.02819042,-0.05785898,-0.00778601,-0.05253303,-0.11996292,0.10573915,-0.0693615,-0.01052663,0.00503615,-0.06055281,-0.01159094,0.02242783,-0.03177977,-0.07085367,0.05705845,-0.00335197,0.05028704,0.05282543,-0.03904063,-0.05039862,0.00033007,-0.08361489,-0.04626649,0.11773763,0.07620109,-0.02160954,-0.00450538,0.04578037,0.00011424,-0.0457257,0.05925367,-0.03047254,-0.03776251,0.00833284,0.02955052,-0.03817917,-0.00691296,-0.0592067,0.00976549,0.06888942,0.04017681,-0.02778067,-0.05628192,0.06180603,0.01580277,-0.06730834,-0.01481388,-0.06348173,-0.00574732,-0.01101874,-0.07533924,0.00067589,-0.02311916,-0.04514779,-0.03255714,0.00400859,0.01477205,-0.05323597,-0.00084248,-0.09434261,0.07046471,0.03310295,0.03775885,-0.04101502,-0.02224867,-0.0159421,0.03626636,-0.07499912,-0.01486162,0.06421353,-0.05647949,-0.02579946,0.00230676,0.00694384,-0.00900763,-0.00188524,-0.00218391,0.0344168,-0.00404108,0.05635138,-0.0161139,0.03038728,-0.1050107,-0.22344057,-0.02842639,0.03876956,-0.0751644,0.06603015,0.00099218,-0.01382993,0.00878495,0.039551,0.1067453,0.08529457,0.00479524,-0.00526957,0.04334253,0.02732419,0.02328592,0.03713148,0.01932008,-0.02243394,0.02400267,-0.01208985,0.01158759,-0.03892808,-0.04379217,-0.00808268,0.01699356,0.10451943,0.00315546,0.09422494,-0.00147708,0.03653222,0.03096704,0.04354769,-0.08612866,0.05529058,0.00183667,-0.02637462,-0.01496137,-0.04176698,-0.06619162,0.01246662,0.00747922,0.00925567,-0.0864143,-0.03670937,0.00861247,-0.02476923,-0.00232008,-0.00973204,0.04807385,-0.02059906,-0.01968176,-0.02496889,0.064607,0.00083333,-0.03702842,-0.0956097,-0.0053638,-0.04449039,0.0074564,-0.02724361,-0.04021091,0.04831241,-0.08592603,-0.03697485,-0.00970958,-0.01058712,0.02263425,0.0501835,0.02353913,-0.08183337,0.11901031,0.00917922,0.0173047,0.02987921,0.05211005,-0.06389512,-0.02034721,0.02098661,0.0100851,0.07823312,0.00557665,0.03284075,0.06452043,0.02888012,0.02807326,0.02597342,-0.05092825,0.04695752,-0.01143524,-0.0378304,-0.01184637,-0.0523107,-0.009867,0.07830019,-0.01845787,-0.26538876,0.01063769,0.00065495,-0.04690765,0.07631967,0.03583964,0.0261848,-0.01634816,-0.0332423,0.04181391,-0.03622659,0.00655725,0.00617456,-0.04962979,-0.05738248,-0.02208533,0.07293946,0.04866428,0.04795372,-0.02928145,-0.01760413,0.04831729,0.22430737,0.00866499,0.00274733,-0.02845891,-0.00220532,0.00343691,0.02527505,0.02409551,-0.0394913,0.00819273,0.10762634,0.07236703,0.02492749,0.08699976,0.01633622,-0.01249114,0.03236374,0.0151846,-0.05399494,-0.03369965,-0.04389188,-0.01130979,0.08843847,-0.01105785,-0.03881059,-0.05032458,0.0151171,-0.00571469,-0.03293517,0.05085459,0.01631465,-0.00578419,-0.00237608,0.03422389,0.04176267,-0.03802238,-0.03100105,-0.05622083,-0.00141097,0.02709165,0.01796051,0.04505586,0.00280831],"last_embed":{"hash":"2a8a19b565a0f0fd396d5b3b6c1e59569762fb61c649bd205169e9bcc8e89fd3","tokens":355}}},"text":null,"length":0,"last_read":{"hash":"2a8a19b565a0f0fd396d5b3b6c1e59569762fb61c649bd205169e9bcc8e89fd3","at":1757414183812},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[z-score 标准化(zero-mean normalization)](http://blog.csdn.net/pipisorry/article/details/null)#<http://blog.csdn.net/pipisorry/article/details/null><http://blog.csdn.net/pipisorry/article/details/null>#{1}","lines":[100,107],"size":357,"outlinks":[{"title":"x-μ只改变均值，标准差不变，所以均值变为0","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"(x-μ)/σ只会使标准差除以σ倍，所以标准差变为1","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","target":"./_resources/数据标准化归一化normalization_-_皮皮blog_-_博客频道_-_CSDN.NET.resources/unknown_filename.png","line":5},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":7}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[Decimal scaling小数定标标准化](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.05753755,-0.03084929,0.00588142,-0.01973143,0.00908473,0.00318515,-0.02535935,-0.01208809,0.03537112,0.01175627,0.01754997,-0.03733393,0.06503198,0.05805745,0.05231934,0.00916652,0.02733658,0.02966117,-0.06349626,0.01223186,0.16903262,-0.05725712,0.03053449,-0.0535585,0.10826285,0.01650049,-0.02792773,-0.06451695,-0.02693467,-0.23172285,0.02012914,0.03089178,0.09026673,-0.03298134,0.04921068,-0.06310263,-0.01599782,0.05245686,0.0350875,0.01954517,0.03471998,0.02808868,0.00123405,-0.00991069,-0.01016786,-0.02245509,-0.03536729,-0.01699542,-0.01799422,-0.0115067,-0.0188868,-0.03142035,-0.01452674,0.04858244,-0.0162669,0.02210956,0.07619191,0.00797865,0.08218677,0.02994435,-0.00516752,0.062353,-0.20154443,0.05134251,0.02123543,-0.01938997,0.00773436,-0.05358778,0.02484967,0.07772975,-0.01858655,-0.03563606,0.02227445,0.06485968,0.0304644,-0.069356,-0.02808776,0.01025474,-0.01152152,0.00668404,-0.04078121,0.05196106,0.02902821,-0.01995301,0.04675724,0.00813297,0.0510125,-0.02132092,-0.00015907,-0.06469376,-0.00851906,-0.0082609,-0.02216714,-0.01724335,-0.03162576,-0.05589078,0.06093311,0.03125129,-0.06036069,0.11869193,-0.00389654,0.04218139,-0.0298656,-0.00082069,0.0720159,0.04019317,-0.04067372,-0.04011677,-0.0135208,-0.03014694,-0.02617964,-0.01424143,0.02189165,-0.04163834,-0.00807952,-0.00368038,0.03834243,0.03517692,0.0017275,0.02233714,-0.00440919,0.01421778,0.03624123,-0.04237701,0.01643623,-0.00539026,-0.02767382,0.04374967,0.01178978,0.02930995,0.06295563,0.00245145,-0.10864682,-0.02542969,0.04254062,0.01622298,-0.01619104,-0.03342667,0.00019541,-0.02271209,-0.06571501,-0.04604593,-0.01964705,-0.05801885,-0.09058721,0.12806553,-0.072802,-0.01378453,0.02545691,-0.06777421,-0.00902498,0.00903118,-0.03477946,-0.05500551,0.03701435,0.00965454,0.06498401,0.06298117,-0.05752328,-0.05948398,-0.00643707,-0.05486658,-0.06254507,0.11927041,0.05075946,-0.042844,-0.03232989,0.05193954,-0.01915632,-0.0267998,0.07278375,-0.02428485,-0.0491647,-0.01070297,0.05495726,-0.03577512,-0.0258654,-0.05075578,0.00871691,0.05998757,0.06171155,-0.03109603,-0.02595966,0.05319221,0.00794098,-0.10215757,0.00987818,-0.04233191,0.00269475,-0.02365414,-0.05929927,0.0224819,-0.05944259,-0.0424659,-0.04404905,0.01319728,0.0290468,-0.00911566,0.01519742,-0.05529502,0.10380486,0.03154579,0.05454529,-0.01779715,-0.01765303,0.00865576,0.04716104,-0.06549598,0.02609306,0.06863578,-0.03296145,-0.01257424,0.0067356,-0.04152553,-0.00813603,0.01855892,-0.01067477,0.06215817,-0.02583398,0.07768138,-0.00100828,0.00032746,-0.12752745,-0.2233966,-0.02946048,0.02923324,-0.09489898,0.10326102,-0.00542616,-0.00853966,0.01034621,0.03011721,0.07883749,0.06735495,0.01828665,-0.04098753,0.00509132,-0.00372863,-0.00652341,0.05553495,0.01584273,-0.01113226,0.00238804,0.00665987,0.02212652,-0.07379556,-0.03281059,-0.00709594,0.02420935,0.09519417,-0.0380404,0.06841055,-0.03034934,0.00761702,-0.00273575,0.0017697,-0.04406228,0.03872032,0.01181039,-0.02737474,-0.01977905,-0.03404042,-0.03260246,-0.00005199,0.01244066,-0.00508687,-0.09212539,-0.03524766,0.01032357,-0.01518091,0.01790837,0.02406649,0.02726307,-0.01594014,-0.02140337,-0.02885146,0.07123447,0.00559136,-0.04768335,-0.05628447,-0.01918984,-0.03546688,0.0019273,-0.04357526,-0.064455,0.06315722,-0.04512594,-0.05702909,0.00368582,-0.02939324,0.01128295,0.0312773,0.01660135,-0.08488052,0.12450591,0.01886681,0.04776398,0.01003806,0.05988611,-0.05349203,0.01936262,0.02834844,0.04585877,0.11040691,0.02393737,0.06618705,0.05879591,0.01517527,0.01837095,0.02626996,0.00016902,0.04418349,0.00183262,-0.03238529,-0.02048668,-0.05339523,-0.01074121,0.03513463,-0.00648227,-0.2742891,0.02017604,-0.01537803,-0.02492429,0.03984293,0.03699923,0.04955908,0.01352598,-0.0210991,0.00504172,-0.02388914,0.02919274,0.01497248,-0.02162913,-0.02967085,-0.0490094,0.04645492,0.05803296,0.04946164,-0.01613359,-0.00496212,0.03561169,0.24228817,0.00242038,0.00596669,-0.00793085,-0.00223238,0.02945898,0.0577742,0.02094094,-0.04858468,-0.02694261,0.09423442,0.03556066,0.02407269,0.07797717,-0.01502349,-0.0040578,0.04710946,0.03804851,-0.03359309,-0.0134565,-0.04424564,-0.01486022,0.09506329,-0.02481337,-0.03841744,-0.0583202,0.02006599,-0.01750653,-0.03676168,0.03828603,0.04181991,-0.01331105,0.02238206,0.05032084,0.00597231,-0.00970663,-0.02920219,-0.09528847,0.00938255,0.01159201,-0.01191568,0.03377961,0.03126222],"last_embed":{"hash":"60d1d1e58f3f1ccb7ec46806ca85bd9ae53d650aaa7ec24a2ad30f0ebf856d41","tokens":382}}},"text":null,"length":0,"last_read":{"hash":"60d1d1e58f3f1ccb7ec46806ca85bd9ae53d650aaa7ec24a2ad30f0ebf856d41","at":1757414183849},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[Decimal scaling小数定标标准化](http://blog.csdn.net/pipisorry/article/details/null)","lines":[108,113],"size":441,"outlinks":[{"title":"Decimal scaling小数定标标准化","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"这种方法通过移动数据的小数点位置来进行标准化。小数点移动多少位取决于属性A的取值中的最大绝对值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"将属性A的原始值x使用decimal scaling标准化到x'的计算方法是：x'=x/(10^j)其中，j是满足条件的最小整数。例如 假定A的值由-986到917，A的最大绝对值为986，为使用小数定标标准化，我们用每个值除以1000（即，j=3），这样，-986被规范化为-0.986。注意，标准化会对原始数据做出改变，因此需要保存所使用的标准化方法的参数，以便对后续的数据进行统一的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":5}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[Decimal scaling小数定标标准化](http://blog.csdn.net/pipisorry/article/details/null)#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.05794615,-0.03123666,0.0039628,-0.0199303,0.01209583,0.00281826,-0.02597653,-0.01140581,0.03494038,0.01167984,0.01802456,-0.03466918,0.0656444,0.0599695,0.05298368,0.01066311,0.02870868,0.02745447,-0.06049668,0.01410571,0.17258802,-0.0571217,0.03168808,-0.05344851,0.10787379,0.01610329,-0.0273668,-0.06245984,-0.02875353,-0.23073287,0.02024299,0.03080087,0.090151,-0.03319092,0.05007739,-0.06455575,-0.0144413,0.05593218,0.03693479,0.02071114,0.0344172,0.02855914,-0.00060344,-0.0100959,-0.01016271,-0.02219359,-0.03497441,-0.01561054,-0.01724924,-0.01079584,-0.01772237,-0.0305187,-0.01462733,0.04791902,-0.01659033,0.0204666,0.07511711,0.00981837,0.0808062,0.03000894,-0.00483252,0.06100229,-0.2016011,0.05071174,0.02018013,-0.01725008,0.00807129,-0.05389476,0.02426624,0.07700061,-0.01634125,-0.03561938,0.02224199,0.06359263,0.03124407,-0.06978163,-0.02892431,0.01225022,-0.00795756,0.0072035,-0.04139826,0.05164121,0.02913954,-0.02066915,0.04579122,0.00545748,0.05334276,-0.02141918,-0.00013747,-0.06320833,-0.0064812,-0.0092012,-0.0204587,-0.0180545,-0.03319896,-0.05473659,0.06004449,0.03073636,-0.05903295,0.117833,-0.00150957,0.03995514,-0.03010197,-0.00300553,0.07037034,0.04226963,-0.04142949,-0.03923741,-0.01182133,-0.03145275,-0.02628927,-0.01298411,0.02061068,-0.03900097,-0.0082703,-0.00126326,0.0372325,0.03332702,-0.00198287,0.02104015,-0.00700285,0.01299404,0.0368062,-0.04187533,0.01527575,-0.00354634,-0.02745416,0.04365886,0.01163827,0.03210562,0.0648137,0.00141794,-0.10820536,-0.02859836,0.04286559,0.01588512,-0.01628271,-0.03359845,-0.00281291,-0.02503582,-0.06576189,-0.04939911,-0.02052742,-0.05854377,-0.09252805,0.12960382,-0.06821093,-0.01227014,0.02603816,-0.06782487,-0.01194158,0.00925518,-0.03555322,-0.05711913,0.03538267,0.01183908,0.06440272,0.06301352,-0.05804329,-0.05913775,-0.00711601,-0.05524474,-0.06416242,0.11874259,0.05075146,-0.0415714,-0.03279275,0.05092084,-0.01913495,-0.02657262,0.07141514,-0.02479831,-0.05253901,-0.00931629,0.05271645,-0.03761462,-0.02372627,-0.0503772,0.00845199,0.06046612,0.06202986,-0.03108325,-0.02425752,0.05516471,0.00658941,-0.1008641,0.00767348,-0.0403653,-0.00021381,-0.02333976,-0.06281548,0.02303768,-0.06060935,-0.04205381,-0.04520096,0.01193228,0.02857292,-0.00999179,0.01390079,-0.05540178,0.10708506,0.03085082,0.05273097,-0.01785244,-0.01676394,0.00631765,0.04845213,-0.06474184,0.02524211,0.0685032,-0.03294175,-0.01277461,0.00760239,-0.04109956,-0.00910592,0.01917742,-0.01126141,0.06246649,-0.02596553,0.07664119,-0.00107777,0.00039785,-0.12565479,-0.22356042,-0.02999711,0.02916311,-0.09677785,0.10431589,-0.00626163,-0.00778141,0.0095636,0.02764012,0.08055165,0.0664454,0.01770097,-0.04002854,0.00318788,-0.00335278,-0.00892775,0.05449348,0.01568346,-0.01024068,0.00355135,0.00743483,0.01731011,-0.0711927,-0.03216876,-0.00581996,0.02480655,0.0946914,-0.03946503,0.06982692,-0.02997149,0.00515751,-0.00174507,0.00109959,-0.04532747,0.03798391,0.01217691,-0.02578796,-0.01851846,-0.03499829,-0.03341824,-0.00212061,0.01428467,-0.00397263,-0.09251999,-0.03308619,0.01091514,-0.01625833,0.01957518,0.02349709,0.02867819,-0.01553248,-0.01880216,-0.02749946,0.07186503,0.00636777,-0.04700403,-0.05518224,-0.01839626,-0.03586895,0.00137774,-0.04231261,-0.0634475,0.06203492,-0.04454945,-0.0560141,0.0000613,-0.03038527,0.01348851,0.03126338,0.01883344,-0.08355605,0.12287358,0.01850934,0.0474567,0.00890684,0.05892054,-0.055162,0.0183641,0.02878656,0.04540651,0.10842761,0.02213816,0.06598755,0.05889247,0.01719079,0.02000527,0.0251832,0.00114774,0.0442495,0.00117065,-0.03183702,-0.01998639,-0.05285726,-0.00920532,0.03793541,-0.00697517,-0.27551296,0.02069603,-0.01424714,-0.02463276,0.04112497,0.03811131,0.05132566,0.01460161,-0.02310909,0.00404137,-0.02354141,0.02791361,0.01523164,-0.01868167,-0.03085714,-0.04923743,0.04541573,0.05984116,0.04867025,-0.01431843,-0.00509167,0.03596603,0.240688,0.00163333,0.00698803,-0.00677536,-0.00025707,0.03171461,0.06013149,0.0228966,-0.04833215,-0.02900527,0.0964488,0.03638175,0.02290172,0.07617662,-0.01705805,-0.00440833,0.04532896,0.03685252,-0.03706837,-0.01484079,-0.04303373,-0.01510501,0.09550059,-0.02550237,-0.03878428,-0.05854047,0.02041667,-0.01963992,-0.03867115,0.03978372,0.0426957,-0.01164665,0.02256259,0.04851245,0.00524488,-0.0131317,-0.03162146,-0.09516372,0.00975705,0.01173693,-0.01067939,0.03479256,0.03137362],"last_embed":{"hash":"28c9914e77b2048616c5ef426ddc0dd08a8c5539fbc554636e2ac9086ddf026f","tokens":381}}},"text":null,"length":0,"last_read":{"hash":"28c9914e77b2048616c5ef426ddc0dd08a8c5539fbc554636e2ac9086ddf026f","at":1757414183890},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[Decimal scaling小数定标标准化](http://blog.csdn.net/pipisorry/article/details/null)#{1}","lines":[110,113],"size":359,"outlinks":[{"title":"这种方法通过移动数据的小数点位置来进行标准化。小数点移动多少位取决于属性A的取值中的最大绝对值。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"将属性A的原始值x使用decimal scaling标准化到x'的计算方法是：x'=x/(10^j)其中，j是满足条件的最小整数。例如 假定A的值由-986到917，A的最大绝对值为986，为使用小数定标标准化，我们用每个值除以1000（即，j=3），这样，-986被规范化为-0.986。注意，标准化会对原始数据做出改变，因此需要保存所使用的标准化方法的参数，以便对后续的数据进行统一的标准化。","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[对数Logistic模式](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.05266073,-0.02514635,0.00092338,-0.03659249,0.06091287,0.00970239,-0.02657605,-0.02072619,0.01043117,0.0155531,-0.00283484,-0.04315525,0.07162537,0.05508161,0.05337093,0.00259162,0.01748127,0.00123302,-0.05325855,-0.01631635,0.14266638,-0.04412461,0.04414927,-0.06888868,0.09277292,-0.00375717,-0.0321559,-0.01927193,-0.02407814,-0.20574266,-0.00571212,-0.00295199,0.05867336,-0.03402033,0.06966116,-0.06957018,-0.01992091,0.07483445,0.03191343,0.05381069,0.03210216,0.00129115,-0.02678883,0.01302836,0.02361702,-0.0598299,-0.02151431,-0.02362975,-0.00199506,-0.058661,-0.00774765,-0.02455722,-0.0324323,0.00210003,0.01281195,0.00621995,0.07579795,0.00966671,0.07445319,0.04266435,0.01732572,0.08093562,-0.20720437,0.03732968,-0.00231065,-0.02236053,-0.02244937,-0.03324192,0.06216123,0.0814494,0.00275589,-0.02487062,0.02031938,0.09964783,0.02191378,-0.03004833,0.00448313,0.01632231,-0.01865867,-0.01092343,-0.03032265,0.04916987,0.03643113,-0.01818608,0.05520661,0.01796347,0.06693406,-0.0436898,0.00878496,-0.05780573,-0.02711784,-0.01353027,-0.00501047,0.00542674,-0.05583724,-0.05367628,0.03445003,0.02003046,-0.03415382,0.12329797,-0.03420345,0.05373985,0.00148574,0.00556972,0.04343978,0.04497045,-0.04159561,-0.04008732,-0.02082208,0.00916699,-0.01979455,-0.01314464,0.05185495,-0.02870872,-0.00649146,0.0105699,0.04644331,0.0554734,-0.02469801,0.02297044,0.02409693,-0.00124862,0.04606345,-0.04024099,-0.00145351,-0.02892555,-0.00649068,0.02610818,0.04618161,0.03465778,0.10094388,0.02643081,-0.10164389,-0.02812275,0.05630688,-0.01096342,-0.00832096,-0.01939953,-0.01303498,0.00018789,-0.03215275,-0.05942927,-0.01875834,-0.06676509,-0.09913047,0.1131821,-0.05981912,-0.03280498,-0.00427258,-0.0516028,-0.03432348,0.02127502,-0.01853984,-0.07171281,0.05804808,-0.01080079,0.07364994,0.0983713,-0.05475037,-0.05502996,0.00430025,-0.07607049,-0.05104448,0.13566528,0.05266478,-0.05384566,-0.00913013,0.04786885,0.01407125,-0.06632368,0.05816087,-0.01189476,-0.02499659,-0.00959603,0.02127378,-0.01048936,-0.00467867,-0.04815809,-0.01113886,0.06427096,0.04303868,-0.04333219,-0.06980006,0.07197094,0.01409939,-0.08849691,-0.02211752,-0.05356217,-0.01146958,0.0158575,-0.05140777,0.00227974,-0.01552889,-0.04714917,-0.03793994,0.00579348,0.01898767,-0.04388561,0.00998008,-0.05388334,0.0669993,0.05410603,0.04594231,-0.03150836,-0.03341283,0.00362469,0.03987736,-0.07287531,-0.00397436,0.05217298,-0.04711861,-0.00115444,0.01560646,-0.00943637,-0.02608402,0.00046239,-0.02332029,0.06264174,0.01964412,0.05575182,-0.01731885,0.02807963,-0.07870065,-0.22948933,-0.04697246,0.04499025,-0.08743986,0.07041023,-0.00383768,-0.00439955,0.00450393,0.03132849,0.07450125,0.07625324,0.01194398,-0.00602724,0.02847479,0.00990831,0.0122545,0.04887294,0.01743104,-0.01244475,0.02443754,-0.00712845,0.01711643,-0.03565065,-0.06156574,0.00019394,0.02875659,0.0988324,-0.00420844,0.10740313,-0.00155828,-0.00639181,0.01430225,0.0370836,-0.07871817,0.06161172,0.01493758,-0.03618106,-0.04400739,-0.03318696,-0.06917642,0.00867537,0.02157725,-0.00315659,-0.10414442,-0.05302021,-0.01351472,-0.02535788,0.00115945,-0.00929513,0.03022728,-0.01683301,-0.00964783,-0.02920874,0.07503659,0.00484443,-0.02417597,-0.0692036,-0.03023234,-0.05023167,0.01526064,-0.03904487,-0.03342165,0.03603444,-0.04882431,-0.05113696,-0.01395972,-0.01931064,-0.00871096,0.03489447,0.00015175,-0.08333164,0.14029016,0.03954406,0.02246119,0.00339615,0.0614933,-0.05124695,0.01677098,0.02470211,0.02158394,0.10000361,-0.00331025,0.07363036,0.07257069,0.00255493,0.02579972,0.00583768,-0.05157385,0.03297118,-0.0007189,-0.06708581,-0.01641874,-0.04419083,-0.03594288,0.0598348,0.01194309,-0.27330089,0.00636649,0.00419204,-0.01037131,0.06726078,0.05862463,0.02180988,0.00100896,-0.0201173,0.01461677,-0.02676958,0.02681618,0.01234151,-0.01189391,-0.043986,-0.02533101,0.05448988,0.03959899,0.03981465,-0.0214996,-0.02340297,0.03216967,0.21362029,0.02069707,0.00675246,-0.01124029,-0.00718957,0.0132746,0.03428715,0.0076542,-0.03524985,-0.03742635,0.09818035,0.05636747,0.04507451,0.06615008,0.01193408,-0.00602505,0.023491,0.00846133,-0.08146341,-0.00854053,-0.02018834,-0.01588009,0.11646028,-0.00440341,-0.06122271,-0.06499022,0.00641154,-0.00114893,-0.0255967,0.03860293,0.00906696,0.00812071,0.00178897,0.04383629,0.03954819,-0.03181788,-0.03988125,-0.07956572,0.03208596,0.00791075,0.00433623,0.04656639,0.01292559],"last_embed":{"hash":"3e5d0c6ae46ebb7a9ad33634555d584c3acf613c7762b94a6a1ceb8dff8cb657","tokens":149}}},"text":null,"length":0,"last_read":{"hash":"3e5d0c6ae46ebb7a9ad33634555d584c3acf613c7762b94a6a1ceb8dff8cb657","at":1757414183928},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[对数Logistic模式](http://blog.csdn.net/pipisorry/article/details/null)","lines":[114,117],"size":146,"outlinks":[{"title":"对数Logistic模式","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"新数据=1/（1+e^(-原数据)）","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[对数Logistic模式](http://blog.csdn.net/pipisorry/article/details/null)#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.0524069,-0.02219853,0.00041866,-0.03603509,0.06225628,0.01146653,-0.02730998,-0.0223725,0.0115057,0.01466857,-0.00331075,-0.04256341,0.07264487,0.05574113,0.05293044,0.00261407,0.01880867,-0.00106646,-0.05018244,-0.0140629,0.14339198,-0.04206564,0.04651837,-0.0682881,0.09155563,-0.00408374,-0.03191257,-0.01670305,-0.02567415,-0.20409802,-0.00693637,-0.00595182,0.05262894,-0.03172065,0.0702998,-0.07040349,-0.01431106,0.07393317,0.03333876,0.053429,0.03472948,0.00542182,-0.02587956,0.01409655,0.02108943,-0.06026645,-0.0212018,-0.0221143,-0.00246168,-0.05708986,-0.00702733,-0.02541627,-0.03259355,0.00421015,0.00931477,0.00386609,0.07515027,0.00894631,0.07287458,0.04109303,0.01899627,0.07913113,-0.20677786,0.03741538,-0.0023717,-0.02210165,-0.02161954,-0.03734141,0.0622807,0.08186772,0.00265503,-0.02492416,0.01964354,0.10092998,0.0243125,-0.02894274,0.00323124,0.01482185,-0.01732926,-0.00837901,-0.02832096,0.04758402,0.03683556,-0.02107451,0.05483188,0.01899516,0.06894608,-0.04167965,0.00781243,-0.05538984,-0.02537745,-0.01465351,-0.00274823,0.00490739,-0.05667644,-0.05107014,0.03129102,0.01978346,-0.03288155,0.12191494,-0.03487905,0.05376802,0.00035461,0.00377939,0.04310316,0.04489293,-0.0409463,-0.041353,-0.01927852,0.00988114,-0.02016175,-0.01312376,0.05178557,-0.0253621,-0.00862633,0.01044498,0.04734023,0.05517403,-0.0291275,0.02165866,0.02240778,-0.00330623,0.04855905,-0.0409488,0.00093658,-0.02742386,-0.00742426,0.03057092,0.04527833,0.03486842,0.1001979,0.02173681,-0.10390674,-0.0306587,0.05746363,-0.01456878,-0.00656439,-0.01742272,-0.01512919,-0.00278532,-0.03314248,-0.05806276,-0.01811223,-0.06788731,-0.10032888,0.11350708,-0.05523819,-0.03247669,-0.00353669,-0.05262949,-0.03403521,0.02165017,-0.02042737,-0.0719807,0.05602162,-0.00915239,0.07527903,0.09935861,-0.05522672,-0.05424172,0.0035474,-0.07654265,-0.05022713,0.13462569,0.05393135,-0.05329833,-0.01152711,0.04556767,0.01340977,-0.06482974,0.05960006,-0.01341484,-0.02569274,-0.01600456,0.02143053,-0.00963804,-0.00091757,-0.04774403,-0.01378512,0.06582043,0.04178405,-0.04226954,-0.06909864,0.07269613,0.01426876,-0.09093477,-0.02447265,-0.05155434,-0.01140265,0.01330035,-0.05805213,0.0030441,-0.01572484,-0.04668843,-0.03695672,0.0044031,0.01854795,-0.04330461,0.00953265,-0.05095881,0.06894173,0.05398044,0.04693277,-0.03141936,-0.03011921,0.00160585,0.03797879,-0.07279789,-0.00039106,0.04953099,-0.04611421,0.00072287,0.01476223,-0.00831774,-0.02469577,-0.00201639,-0.0211379,0.06121204,0.0201031,0.05596627,-0.01937122,0.02757054,-0.07795687,-0.23105153,-0.04689708,0.04392452,-0.09181479,0.07101747,-0.0067672,-0.00297499,0.0069475,0.03012651,0.07304119,0.07435945,0.0145903,-0.00759471,0.03064858,0.01114663,0.01210422,0.0483438,0.01689722,-0.00980904,0.02539505,-0.00771375,0.01386129,-0.03142178,-0.06160755,-0.00083364,0.03028546,0.0992661,-0.0019435,0.11135337,-0.00245414,-0.00567014,0.01683411,0.03736004,-0.08210093,0.05999116,0.01568654,-0.03669258,-0.04459135,-0.03361,-0.07084791,0.00418532,0.02082055,-0.00514144,-0.10690419,-0.05434672,-0.01363337,-0.02391584,0.00226866,-0.01250613,0.02964503,-0.01629508,-0.0095144,-0.02698088,0.07594747,0.00391491,-0.02659528,-0.06728618,-0.02666736,-0.04977673,0.01446246,-0.0368227,-0.03239891,0.03398662,-0.05088924,-0.04910847,-0.01670301,-0.01921326,-0.00840313,0.03723542,0.00003674,-0.08216041,0.14131048,0.03573881,0.02191514,-0.00068265,0.06091915,-0.05266495,0.01600548,0.02090025,0.023621,0.095749,-0.00194653,0.07169905,0.07143917,0.0015468,0.02946854,0.00715988,-0.0489678,0.03364687,0.00042695,-0.06848792,-0.01953663,-0.04066503,-0.0354021,0.05980596,0.01143825,-0.27417761,0.00773555,0.00329007,-0.01031355,0.06846414,0.05899298,0.02392548,0.00190421,-0.01950224,0.01350105,-0.02159316,0.02662774,0.01287534,-0.01105035,-0.04163619,-0.02430877,0.05303464,0.0398709,0.0397283,-0.02278926,-0.02499862,0.03184143,0.21380141,0.02312972,0.00557076,-0.01054716,-0.00282122,0.01479182,0.03392151,0.00761405,-0.03455963,-0.04012503,0.10232361,0.05403164,0.04568226,0.06686725,0.01090629,-0.00683823,0.02250743,0.00643271,-0.08362916,-0.00781507,-0.01784162,-0.01268536,0.11958279,-0.00803329,-0.06007072,-0.06376483,0.00457601,-0.00284277,-0.02758146,0.04077191,0.01136625,0.00998593,0.00098264,0.04102751,0.03957179,-0.03166419,-0.04304458,-0.0782625,0.03487707,0.01024931,0.00147352,0.04563034,0.01204829],"last_embed":{"hash":"bb437638fc916c277bcd1ac567b356adca55eae2c78aa8b48c90793e4cccba40","tokens":148}}},"text":null,"length":0,"last_read":{"hash":"bb437638fc916c277bcd1ac567b356adca55eae2c78aa8b48c90793e4cccba40","at":1757414183945},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[对数Logistic模式](http://blog.csdn.net/pipisorry/article/details/null)#{1}","lines":[116,117],"size":74,"outlinks":[{"title":"新数据=1/（1+e^(-原数据)）","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[模糊量化模式](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.06000182,-0.03828701,0.01306543,-0.00898574,0.04599381,0.00710703,-0.02362927,-0.0207441,0.02302181,0.01058897,0.00341231,-0.040981,0.08109325,0.04796296,0.04866203,0.00000805,0.02416555,0.01229089,-0.06012389,-0.0173562,0.15819418,-0.03934726,0.05609236,-0.06777872,0.09758452,0.00291716,-0.02972934,-0.03195682,-0.01207547,-0.20544134,-0.00627439,0.01386157,0.05833926,-0.01524835,0.0666836,-0.07895961,-0.02677835,0.0685069,0.02702181,0.04617961,0.04549642,0.00283026,-0.01964841,0.0110008,0.01956642,-0.036701,-0.02860907,-0.03593969,-0.006953,-0.04495248,-0.00936582,-0.02243063,-0.01895802,0.01731463,0.0144157,0.01732277,0.07904723,0.02168812,0.05976909,0.04375853,0.01473476,0.08076294,-0.2139363,0.04272215,0.00184736,-0.00307052,-0.02036713,-0.04007867,0.05468385,0.08309115,0.00439513,-0.02130524,0.01402218,0.07791229,0.02681982,-0.04747507,0.00554399,0.01139659,-0.01194285,-0.00838569,-0.03522906,0.05792742,0.04398713,-0.015512,0.05016149,0.01092164,0.05486685,-0.03553905,0.00134825,-0.06350517,-0.03163083,-0.0169936,-0.00196785,-0.00424583,-0.04130258,-0.03655072,0.04068457,0.02331338,-0.03874321,0.11721221,-0.03927943,0.04838004,-0.00581383,-0.00145137,0.05778524,0.05850152,-0.04321285,-0.03499538,-0.02809789,0.00100449,-0.02959287,-0.02184676,0.04910384,-0.03275206,-0.00484489,0.00276427,0.05599547,0.05200752,-0.02165182,0.02026736,0.01097775,-0.00650448,0.04266723,-0.03279144,0.003351,-0.03153417,-0.00134908,0.02376389,0.04323866,0.03956007,0.09578708,0.03438049,-0.09361792,-0.02490558,0.06066792,-0.00360387,-0.01517653,-0.02356517,0.00237188,-0.01614949,-0.03699256,-0.06042352,-0.00836731,-0.05468518,-0.0935481,0.10950384,-0.07310291,-0.02892025,0.00520337,-0.04212094,-0.01889179,0.01667906,-0.04268787,-0.06874955,0.05834101,-0.0225734,0.0675821,0.08580721,-0.0575261,-0.0557476,0.01362736,-0.06622992,-0.06216965,0.1307361,0.05946897,-0.03717863,-0.02043574,0.04078214,0.00984351,-0.0526263,0.0594861,-0.01834208,-0.02966792,-0.00793201,0.02612503,-0.01297036,-0.01577082,-0.05637631,-0.00668888,0.05225208,0.04563439,-0.04157485,-0.05002368,0.06715936,0.00279711,-0.09489232,-0.00693548,-0.04703059,-0.02117241,-0.00075495,-0.06179703,0.01879689,-0.04273853,-0.04493795,-0.0396543,0.00110356,0.01481053,-0.0450828,0.00773612,-0.06287852,0.06918143,0.04681089,0.04310247,-0.04503974,-0.03700188,-0.00564387,0.0438599,-0.07130452,0.00011623,0.05213182,-0.04551573,-0.00411322,0.00285878,-0.02135248,-0.01834451,0.00203533,-0.01331582,0.05313797,0.00422139,0.06094735,-0.0160671,0.02691102,-0.08793934,-0.22219521,-0.03469125,0.0298344,-0.08813509,0.07522876,-0.00869612,-0.00041901,0.00785548,0.04778766,0.08255441,0.0678245,0.0091855,-0.01445944,0.03117061,0.00766799,0.01987554,0.06647027,0.02753181,-0.00158291,0.02457262,-0.01074765,0.01587121,-0.03227811,-0.05794228,-0.01139216,0.03061173,0.10835498,0.00251228,0.10581367,-0.0059688,0.0163456,0.00953769,0.03096966,-0.08975707,0.04915074,0.01897787,-0.04320957,-0.02862794,-0.04091299,-0.05936424,0.01931057,0.01552844,-0.00742144,-0.09442383,-0.04370191,-0.00743514,-0.02374411,-0.00985228,0.00466501,0.02529237,-0.02929197,-0.0165355,-0.03385364,0.07072759,-0.00270845,-0.02510153,-0.07922853,-0.02728706,-0.04743516,0.02379207,-0.03527987,-0.0423006,0.03585929,-0.06719776,-0.04648903,-0.00273689,-0.01683373,0.00187325,0.03857499,0.01590316,-0.10251363,0.14216624,0.0395191,0.0222015,-0.00007819,0.06709744,-0.04686373,0.00632014,0.01889558,0.01898642,0.09901067,-0.00918221,0.07152477,0.07533333,0.01196603,0.02816947,0.01648235,-0.04710317,0.03718414,-0.02229076,-0.05243781,-0.01668386,-0.05904585,-0.03045885,0.05610418,0.0036452,-0.26918495,0.00857878,0.0186829,-0.02567765,0.05827633,0.05202834,0.02505114,-0.00942359,-0.02503229,0.02306602,-0.03572766,0.02564882,0.01264994,-0.02253813,-0.05434802,-0.02736109,0.0729569,0.04316621,0.02711819,-0.02348108,-0.02366073,0.03965207,0.21705468,0.01032184,0.00275258,-0.01969251,-0.01072982,0.02208436,0.01729164,0.01286296,-0.05296455,-0.03250369,0.09934817,0.05905522,0.04189214,0.08141387,-0.00995754,-0.00999617,0.03724959,0.03009723,-0.07498933,-0.01905105,-0.04065288,-0.01578786,0.10027561,-0.00698752,-0.04855104,-0.0541664,0.00911995,-0.01200248,-0.0261163,0.04817318,0.01016369,0.00464911,0.02072544,0.04411463,0.03179216,-0.02859137,-0.03489273,-0.07201112,0.03384709,0.00921138,0.01049124,0.05704584,0.02247013],"last_embed":{"hash":"8eca7816213f679483709ad64dbd1fb54e8ec6ae3d7eec640d9e09549c75ee36","tokens":201}}},"text":null,"length":0,"last_read":{"hash":"8eca7816213f679483709ad64dbd1fb54e8ec6ae3d7eec640d9e09549c75ee36","at":1757414183964},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[模糊量化模式](http://blog.csdn.net/pipisorry/article/details/null)","lines":[118,123],"size":217,"outlinks":[{"title":"模糊量化模式","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"新数据=1/2+1/2sin派3.1415/（极大值-极小值）*（X-（极大值-极小值）/2） X为原数据","target":"http://blog.csdn.net/pipisorry/article/details/null","line":3},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":5}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[模糊量化模式](http://blog.csdn.net/pipisorry/article/details/null)#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.06076565,-0.03835769,0.0146755,-0.00780389,0.04724744,0.00667143,-0.02356964,-0.02136863,0.02282389,0.01107995,0.00375067,-0.04012215,0.0831557,0.04856828,0.04800015,0.00041198,0.02515746,0.00847762,-0.05839526,-0.01625022,0.15783192,-0.03792249,0.05656394,-0.0667633,0.09689414,0.00345691,-0.02759033,-0.02921331,-0.01327577,-0.20430417,-0.00676615,0.01360312,0.05362276,-0.01375745,0.06645294,-0.08100086,-0.02260748,0.06720671,0.02853004,0.04765309,0.04627257,0.00479297,-0.01946942,0.01190914,0.01651138,-0.03729192,-0.02757797,-0.03395152,-0.00690597,-0.04397426,-0.009623,-0.02297984,-0.01876969,0.0178672,0.0120511,0.01596257,0.07734634,0.02151312,0.05833071,0.04298662,0.01465801,0.07810454,-0.21464497,0.0436097,0.00035436,-0.00133925,-0.02025516,-0.04364315,0.05383922,0.08508357,0.00489248,-0.02171054,0.01459442,0.07945061,0.02766244,-0.04759043,0.00514377,0.01017197,-0.0098746,-0.00580863,-0.03423998,0.05701153,0.04311613,-0.01706471,0.05047698,0.01021056,0.05716348,-0.03440474,0.001743,-0.06104447,-0.03048254,-0.01727569,-0.00040448,-0.00567285,-0.04229238,-0.03604654,0.04077595,0.02157466,-0.03806064,0.11510933,-0.03820644,0.04603619,-0.00706091,-0.00140103,0.05718453,0.05890758,-0.04275596,-0.03756094,-0.02767775,-0.00047511,-0.03076041,-0.02285619,0.05006735,-0.0311878,-0.00722137,0.00344295,0.0546469,0.05173297,-0.02514652,0.01830122,0.01128349,-0.00790892,0.04470798,-0.03207295,0.00579442,-0.02995562,-0.00209639,0.02660717,0.03934862,0.04010905,0.09606189,0.03290966,-0.09413404,-0.02731096,0.06288915,-0.00597931,-0.01302859,-0.02306809,0.00127626,-0.01936375,-0.03642373,-0.06088664,-0.00803677,-0.0550269,-0.09308149,0.11093252,-0.06808664,-0.0268835,0.0054713,-0.04217807,-0.01910991,0.01677411,-0.043973,-0.0702021,0.05721347,-0.02017916,0.06934943,0.08513414,-0.05778002,-0.05497923,0.01268233,-0.06796096,-0.06252478,0.12845528,0.06045689,-0.03699023,-0.02219968,0.03856004,0.00863396,-0.05146766,0.06054518,-0.01995188,-0.03137311,-0.01195308,0.02707217,-0.0135156,-0.01413963,-0.0553982,-0.00640163,0.05318131,0.04676193,-0.04168127,-0.04909585,0.06817465,0.00357495,-0.09573041,-0.00951662,-0.04552309,-0.02288349,-0.00265502,-0.06590891,0.02177273,-0.04332294,-0.04458321,-0.03936155,0.00052844,0.01450347,-0.04327869,0.00876019,-0.06279947,0.06958336,0.04649841,0.04458713,-0.04635017,-0.0353939,-0.0083181,0.04388009,-0.07092176,0.00273784,0.05030352,-0.04406054,-0.00334182,0.00289899,-0.02198408,-0.01747472,0.0001546,-0.01377365,0.05234278,0.00399741,0.06150983,-0.01834579,0.02738119,-0.08625711,-0.22319067,-0.03395794,0.03119763,-0.09054831,0.07616515,-0.00913928,0.00017119,0.00822255,0.04627328,0.08105316,0.06850155,0.00977132,-0.01356177,0.03116938,0.01027366,0.01889548,0.06634033,0.0270594,0.00019086,0.02464125,-0.01056521,0.01411744,-0.02902619,-0.05789245,-0.01223979,0.03169812,0.10814098,0.00236427,0.1088433,-0.00701899,0.01589019,0.01285741,0.02935038,-0.09361193,0.04641071,0.02092779,-0.04178055,-0.02914645,-0.04009129,-0.06027018,0.01760844,0.01654293,-0.00783376,-0.09512107,-0.04337009,-0.00745753,-0.02297375,-0.00666493,0.00181853,0.02437475,-0.03033175,-0.01555634,-0.0330871,0.07112068,-0.00521197,-0.02719328,-0.0765003,-0.02425056,-0.0483387,0.02411027,-0.0343719,-0.04121234,0.03366959,-0.06790781,-0.0455862,-0.00483529,-0.01701682,0.00286967,0.04074494,0.01565281,-0.10202183,0.14174379,0.03737507,0.0223306,-0.00080118,0.06667005,-0.04803152,0.00760167,0.01637807,0.02029196,0.09867644,-0.00852301,0.07003664,0.07439719,0.01248714,0.02943049,0.01796249,-0.04464396,0.03775775,-0.02149512,-0.05458888,-0.01945831,-0.0571552,-0.02999697,0.05683233,0.00383847,-0.27019516,0.00835597,0.01883605,-0.02579362,0.05848656,0.05148546,0.02799057,-0.00936731,-0.02429073,0.02155386,-0.03364346,0.02426805,0.01340446,-0.02051265,-0.05392779,-0.02845931,0.07427755,0.04432566,0.02717147,-0.02341254,-0.02551269,0.04004089,0.21687928,0.01074051,0.00472603,-0.0210636,-0.0096499,0.02316253,0.01644583,0.01330792,-0.05413544,-0.03409799,0.10200727,0.05606495,0.0413124,0.08167189,-0.00976031,-0.01068636,0.03711689,0.02870938,-0.07620911,-0.01931693,-0.03932799,-0.01209056,0.1019081,-0.00802538,-0.047741,-0.05371616,0.00695458,-0.01255746,-0.02879865,0.05023377,0.01112175,0.00597422,0.0206674,0.04129642,0.03064355,-0.02810103,-0.03761311,-0.07236782,0.03622036,0.01024817,0.01005725,0.05731346,0.02179088],"last_embed":{"hash":"dd5fbc5015774206e14823314a983124a988b20efa143d8c693ea2e8e13d4078","tokens":200}}},"text":null,"length":0,"last_read":{"hash":"dd5fbc5015774206e14823314a983124a988b20efa143d8c693ea2e8e13d4078","at":1757414183987},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[常见的数据归一化方法](http://blog.csdn.net/pipisorry/article/details/null)#[模糊量化模式](http://blog.csdn.net/pipisorry/article/details/null)#{1}","lines":[120,123],"size":151,"outlinks":[{"title":"新数据=1/2+1/2sin派3.1415/（极大值-极小值）*（X-（极大值-极小值）/2） X为原数据","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"皮皮blog","target":"http://blog.csdn.net/pipisorry","line":3}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[数据标准化/归一化的编程实现](http://blog.csdn.net/pipisorry/article/details/null)": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04619463,-0.0231501,0.01323354,-0.02322158,0.04669117,-0.00866935,-0.01771647,-0.01033912,0.01485473,0.01443241,-0.00030079,-0.06568725,0.0774374,0.0370053,0.06731039,-0.00256253,0.00104206,0.01294116,-0.05992027,-0.00451176,0.13502571,-0.02298961,0.04881216,-0.07814471,0.10094893,0.00540575,-0.02747315,-0.03324853,-0.00344239,-0.21976592,0.00930825,-0.00709388,0.07191472,-0.03285969,0.09044472,-0.05654594,-0.02374456,0.05023956,0.00726463,0.04686119,0.05190421,-0.00532531,-0.03605708,0.0212421,0.0129454,-0.05075095,-0.02159167,-0.05055011,-0.00842399,-0.03430965,-0.01061619,-0.04147889,-0.01487741,0.01472537,0.03597787,0.015286,0.06946839,0.01405799,0.06870175,0.06422125,0.00243885,0.0788929,-0.20344114,0.05183914,0.00359576,0.00953608,-0.02601859,-0.02633576,0.04969799,0.05165232,-0.02112984,-0.01168686,0.01190933,0.06895311,0.0239504,-0.04825848,0.01401492,0.01405348,-0.00234553,-0.02142967,-0.01330342,0.07342149,0.05681236,-0.03106075,0.05609815,-0.00258402,0.04978116,-0.02830865,0.00087401,-0.07208987,-0.03233808,-0.01889217,-0.00613914,0.01705679,-0.03574326,-0.03856247,0.05994252,0.02425994,-0.03508113,0.10711797,-0.04935417,0.05272757,0.00593046,0.02526245,0.05884285,0.03352554,-0.05533369,-0.01337402,-0.01267217,-0.01377605,-0.01800537,-0.0197486,0.03899533,-0.0453434,0.02008895,-0.02076429,0.04658854,0.06080187,-0.01132403,0.02660053,0.01867702,-0.01450842,0.04896825,-0.03013198,0.00608417,-0.01556929,-0.00891532,0.03855029,0.0258986,0.03766755,0.09398948,0.01876147,-0.08769804,-0.01550146,0.068976,0.00154062,-0.02325706,-0.02665917,0.00745979,0.00136077,-0.04297223,-0.0585741,-0.00923929,-0.03257026,-0.1104019,0.09485072,-0.07755659,-0.02119439,-0.00434707,-0.04168218,-0.02300296,0.0180098,-0.03233139,-0.06700569,0.06324437,-0.00038345,0.07773763,0.06975751,-0.06377798,-0.06289104,-0.00969655,-0.08259127,-0.06352397,0.13671651,0.05397807,-0.0586162,-0.02688113,0.03975796,-0.01202254,-0.05242994,0.05938901,-0.00940115,-0.02605619,0.0100462,0.02316479,-0.02490107,-0.06197908,-0.03878123,0.03577992,0.05009095,0.0323665,-0.03011311,-0.03749623,0.05157058,0.02577102,-0.09436676,0.00483281,-0.03877525,-0.02018719,-0.01106006,-0.03913199,0.00820466,-0.04086523,-0.06914113,-0.01945672,0.0254273,0.02558585,-0.03944606,0.0219653,-0.05048591,0.0448271,0.0432814,0.02912277,-0.02592915,-0.05105207,0.00423543,0.04021334,-0.05327199,0.0111451,0.0394015,-0.04873562,-0.01493675,0.02702066,-0.03403941,-0.02898641,-0.0083271,-0.02162116,0.06836368,0.01579172,0.05649999,-0.02354202,0.01004938,-0.10020953,-0.22913143,-0.01791368,0.04455843,-0.07409785,0.07184437,-0.02847432,0.00346293,0.00258498,0.04346938,0.081921,0.06974645,0.01268434,0.0132755,0.02648883,-0.00382671,0.01952642,0.06070772,0.03230484,-0.02195,0.0399625,-0.02204618,-0.0013822,-0.03788865,-0.06853013,-0.01436981,0.00777445,0.11763278,-0.00904142,0.11061988,-0.01643194,0.01755534,0.01872033,0.01974982,-0.10095609,0.03817855,0.01555448,-0.0095601,-0.00803284,-0.03581386,-0.05053311,0.03069873,0.01561596,0.00852831,-0.09752463,-0.03967518,0.00175572,-0.04084939,-0.00390983,0.0119983,0.03555122,-0.02583866,0.00811266,-0.01646358,0.06297013,-0.00884251,-0.02905019,-0.09105267,-0.02044286,-0.06826487,0.0217717,-0.03467587,-0.06959774,0.01363439,-0.06432932,-0.03390091,-0.00104257,-0.00230008,-0.0013495,0.04102109,0.00807693,-0.09744016,0.12561016,0.0358355,0.03349584,-0.01077621,0.06243869,-0.03813074,0.00620433,0.02656825,0.01078746,0.11429079,0.00679057,0.08190242,0.07207599,0.0156826,0.00456774,0.02985479,-0.03126277,0.04290101,-0.01473615,-0.04601452,-0.01971438,-0.06248323,-0.00246636,0.07136746,0.01972912,-0.26414546,-0.00159474,0.01766045,-0.01938768,0.07114204,0.05136806,0.01970074,-0.01664803,-0.02611435,0.02843631,-0.05133383,0.01761473,0.02214411,-0.02376091,-0.05550181,-0.01296406,0.07462217,0.04415338,0.02397732,-0.02900326,-0.01455679,0.04039348,0.21401486,0.00806983,-0.01458349,-0.01237628,-0.01942185,0.02249262,0.03773337,0.01225499,-0.04143544,-0.01651151,0.06598034,0.06075859,0.0323444,0.10752525,-0.00309245,-0.01068643,0.04813089,0.02939123,-0.06613162,-0.02257721,-0.04655042,-0.04272195,0.08660266,-0.02540262,-0.04085086,-0.074885,0.00480103,-0.02547731,-0.02220084,0.03097564,-0.00736272,-0.00415399,0.01145984,0.05011196,0.05097372,-0.02055226,-0.03592661,-0.07094333,0.01630673,-0.01081681,0.02245551,0.04231633,-0.00615731],"last_embed":{"hash":"1819fb78e2a39ab2f9804682894de905e022fbece4d1a28ce21cea6ec54297ac","tokens":184}}},"text":null,"length":0,"last_read":{"hash":"1819fb78e2a39ab2f9804682894de905e022fbece4d1a28ce21cea6ec54297ac","at":1757414184009},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[数据标准化/归一化的编程实现](http://blog.csdn.net/pipisorry/article/details/null)","lines":[124,133],"size":294,"outlinks":[{"title":"数据标准化/归一化的编程实现","target":"http://blog.csdn.net/pipisorry/article/details/null","line":1},{"title":"Python","target":"http://lib.csdn.net/base/python","line":3},{"title":"[Scikit-learn：数据预处理Preprocessing data","target":"http://blog.csdn.net/pipisorry/article/details/52247679","line":5}],"class_name":"SmartBlock"},
"smart_blocks:印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[数据标准化/归一化的编程实现](http://blog.csdn.net/pipisorry/article/details/null)#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04311039,-0.01953553,0.01214495,-0.02308362,0.04902822,-0.0074847,-0.01278179,-0.0113137,0.01410164,0.01589392,0.00520025,-0.06877612,0.08234824,0.04193537,0.06598143,-0.00412103,0.00575814,0.00941373,-0.05169817,0.00023137,0.1348419,-0.02576065,0.04801522,-0.07679402,0.09708395,0.00588821,-0.02833333,-0.02341517,-0.00638948,-0.21713708,-0.00190442,-0.01306783,0.06972504,-0.03242104,0.09187625,-0.05699382,-0.01626812,0.0488504,0.01053159,0.04534271,0.05356073,-0.00657318,-0.03853035,0.01908765,0.00831389,-0.05296002,-0.0240723,-0.04828224,-0.00788832,-0.03821519,-0.00964699,-0.03726023,-0.01662844,0.01308561,0.03159712,0.01534582,0.06593139,0.01896747,0.06638242,0.06148745,0.00242443,0.07556866,-0.20319945,0.05089809,0.0064109,0.013022,-0.02509351,-0.03072874,0.05222689,0.05163022,-0.01775412,-0.01215669,0.01413066,0.07641463,0.0284579,-0.04856772,0.0105332,0.01576242,-0.00086532,-0.02181157,-0.01433831,0.06870428,0.04577086,-0.02950713,0.05426314,0.00239706,0.05547243,-0.02195966,0.00024129,-0.06908867,-0.0277512,-0.02187379,-0.00233258,0.01759827,-0.04442799,-0.03594371,0.06027472,0.02256303,-0.03493989,0.10701218,-0.04816708,0.05341925,0.00632543,0.02157597,0.05852295,0.03258,-0.05540363,-0.02003857,-0.01127709,-0.01615077,-0.0134269,-0.02416023,0.04262861,-0.04091319,0.01816822,-0.02177842,0.04785714,0.05981247,-0.02081314,0.02489284,0.01395794,-0.01165416,0.05596481,-0.02963683,0.00529497,-0.01305554,-0.01450115,0.0429914,0.0289165,0.03395746,0.09828927,0.01315548,-0.08877014,-0.01798389,0.07026991,-0.00428671,-0.02409581,-0.01834307,0.00869552,-0.0125906,-0.04092333,-0.06335282,-0.00433633,-0.02970519,-0.11462386,0.09366357,-0.07397513,-0.01912484,-0.00382269,-0.04221137,-0.02506353,0.01342858,-0.0376338,-0.0706322,0.05938728,0.0075748,0.07871464,0.07749321,-0.0671408,-0.05901034,-0.01324437,-0.08330934,-0.06516809,0.13645636,0.05163974,-0.05943608,-0.0306783,0.03877382,-0.02032471,-0.05029924,0.06423596,-0.01527981,-0.02504529,0.00150478,0.02203583,-0.02570175,-0.05341758,-0.03492046,0.03669991,0.05275209,0.02814701,-0.02942682,-0.03573586,0.0542599,0.02555668,-0.09439299,-0.00291123,-0.03480123,-0.02085372,-0.01872258,-0.04812067,0.00245172,-0.0399726,-0.07313729,-0.02364995,0.02548569,0.02979518,-0.03986048,0.01964729,-0.04623424,0.0538446,0.04063024,0.03060978,-0.02001784,-0.0430271,-0.00325447,0.0384059,-0.05264692,0.01313418,0.03982525,-0.04515398,-0.01575778,0.0234517,-0.03349932,-0.02661167,-0.01452098,-0.0238318,0.06313664,0.01551141,0.05751939,-0.02997505,0.01190231,-0.09529532,-0.2347623,-0.0151316,0.03924185,-0.07903039,0.07153103,-0.03164199,0.00336647,0.00294584,0.0427836,0.07799023,0.0729361,0.0173338,0.00953836,0.02632683,-0.00495525,0.01870888,0.05913774,0.03274188,-0.02075248,0.04014601,-0.01976739,-0.00894114,-0.03695348,-0.06802453,-0.01705389,0.0103472,0.11836439,-0.00503034,0.11947261,-0.01531703,0.01828294,0.02502907,0.01923144,-0.09802848,0.03444985,0.01865292,-0.01107709,-0.01061271,-0.03701778,-0.05018581,0.02350815,0.01827437,0.01777477,-0.09477765,-0.03939041,0.00030107,-0.04122253,0.00365314,0.00881941,0.03808938,-0.02182054,0.01141253,-0.01685912,0.06651793,-0.01196902,-0.02799413,-0.08623819,-0.01795303,-0.06552764,0.02687765,-0.03037569,-0.06248659,0.01336889,-0.06051894,-0.03726512,-0.0066376,-0.0057262,0.0006008,0.03728003,0.00661902,-0.09317528,0.13262148,0.03720532,0.02992529,-0.02231071,0.06434713,-0.03301578,0.00519019,0.02584989,0.01178278,0.10748117,0.01036954,0.08273712,0.0685319,0.01640885,0.0134991,0.03227038,-0.03541443,0.0454794,-0.01170708,-0.04979251,-0.02149565,-0.05919353,0.00233004,0.07585681,0.02254866,-0.26276264,-0.00446798,0.01417753,-0.02315819,0.07669409,0.0508815,0.02607706,-0.01693331,-0.03010699,0.02700848,-0.04633742,0.01271421,0.01960131,-0.02226357,-0.05452309,-0.01303366,0.07303571,0.04952632,0.02087062,-0.03254227,-0.0173532,0.04324711,0.21191871,0.00569974,-0.01406557,-0.01412814,-0.01246225,0.0260287,0.03704748,0.01650335,-0.04612734,-0.02739357,0.06938511,0.06036819,0.02681628,0.10325168,-0.00852743,-0.00820513,0.04722288,0.02320764,-0.06873599,-0.02070766,-0.03886131,-0.03825399,0.08698221,-0.0286744,-0.04165174,-0.07403144,0.00692803,-0.02721995,-0.02174352,0.03940259,-0.00294049,0.00256215,0.01189876,0.04746348,0.04822914,-0.02158462,-0.04014741,-0.07605093,0.01920071,-0.0114676,0.01965458,0.04176529,-0.00057548],"last_embed":{"hash":"9753409b101b0722e4e946a5ad86051213eb5e09e5e17717e8ce4f5761d28f23","tokens":184}}},"text":null,"length":0,"last_read":{"hash":"9753409b101b0722e4e946a5ad86051213eb5e09e5e17717e8ce4f5761d28f23","at":1757414184031},"key":"印象笔记/DeepLearning/数据标准化归一化normalization - 皮皮blog - 博客频道 - CSDN.NET.md#[数据标准化/归一化的编程实现](http://blog.csdn.net/pipisorry/article/details/null)#{1}","lines":[126,133],"size":221,"outlinks":[{"title":"Python","target":"http://lib.csdn.net/base/python","line":1},{"title":"[Scikit-learn：数据预处理Preprocessing data","target":"http://blog.csdn.net/pipisorry/article/details/52247679","line":3}],"class_name":"SmartBlock"},
